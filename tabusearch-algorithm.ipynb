{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":88046,"databundleVersionId":10229277,"sourceType":"competition"},{"sourceId":104492,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":72255,"modelId":76277}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gc\nimport os\nfrom math import exp\nfrom collections import Counter\nfrom typing import List, Optional, Union\n\nimport numpy as np\nimport pandas as pd\nimport transformers\nimport torch\n\nos.environ['OMP_NUM_THREADS'] = '1'\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\nPAD_TOKEN_LABEL_ID = torch.nn.CrossEntropyLoss().ignore_index\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\nclass ParticipantVisibleError(Exception):\n    pass\n\n\ndef score(\n    solution: pd.DataFrame,\n    submission: pd.DataFrame,\n    row_id_column_name: str,\n    model_path: str = '/kaggle/input/gemma-2/transformers/gemma-2-9b/2',\n    load_in_8bit: bool = False,\n    clear_mem: bool = False,\n) -> float:\n    # Check that each submitted string is a permutation of the solution string\n    sol_counts = solution.loc[:, 'text'].str.split().apply(Counter)\n    sub_counts = submission.loc[:, 'text'].str.split().apply(Counter)\n    invalid_mask = sol_counts != sub_counts\n    if invalid_mask.any():\n        raise ParticipantVisibleError(\n            'At least one submitted string is not a valid permutation of the solution string.'\n        )\n\n    # Calculate perplexity for the submitted strings\n    sub_strings = [\n        ' '.join(s.split()) for s in submission['text'].tolist()\n    ]  # Split and rejoin to normalize whitespace\n    scorer = PerplexityCalculator(\n        model_path=model_path,\n        load_in_8bit=load_in_8bit,\n    )  # Initialize the perplexity calculator with a pre-trained model\n    perplexities = scorer.get_perplexity(\n        sub_strings\n    )  # Calculate perplexity for each submitted string\n\n    if clear_mem:\n        # Just move on if it fails. Not essential if we have the score.\n        try:\n            scorer.clear_gpu_memory()\n        except:\n            print('GPU memory clearing failed.')\n\n    return float(np.mean(perplexities))\n\n\nclass PerplexityCalculator:\n    def __init__(\n        self,\n        model_path: str,\n        load_in_8bit: bool = False,\n        device_map: str = 'auto',\n    ):\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)\n        # Configure model loading based on quantization setting and device availability\n        if load_in_8bit:\n            if DEVICE.type != 'cuda':\n                raise ValueError('8-bit quantization requires CUDA device')\n            quantization_config = transformers.BitsAndBytesConfig(load_in_8bit=True)\n            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n                model_path,\n                quantization_config=quantization_config,\n                device_map=device_map,\n            )\n        else:\n            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n                model_path,\n                torch_dtype=torch.float16 if DEVICE.type == 'cuda' else torch.float32,\n                device_map=device_map,\n            )\n\n        self.loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n\n        self.model.eval()\n\n    def get_perplexity(\n        self, input_texts: Union[str, List[str]], debug=False\n    ) -> Union[float, List[float]]:\n        single_input = isinstance(input_texts, str)\n        input_texts = [input_texts] if single_input else input_texts\n\n        loss_list = []\n        with torch.no_grad():\n            # Process each sequence independently\n            for text in input_texts:\n                # Explicitly add sequence boundary tokens to the text\n                text_with_special = f\"{self.tokenizer.bos_token}{text}{self.tokenizer.eos_token}\"\n\n                # Tokenize\n                model_inputs = self.tokenizer(\n                    text_with_special,\n                    return_tensors='pt',\n                    add_special_tokens=False,\n                )\n\n                if 'token_type_ids' in model_inputs:\n                    model_inputs.pop('token_type_ids')\n\n                model_inputs = {k: v.to(DEVICE) for k, v in model_inputs.items()}\n\n                # Get model output\n                output = self.model(**model_inputs, use_cache=False)\n                logits = output['logits']\n\n                # Shift logits and labels for calculating loss\n                shift_logits = logits[..., :-1, :].contiguous()  # Drop last prediction\n                shift_labels = model_inputs['input_ids'][..., 1:].contiguous()  # Drop first input\n\n                # Calculate token-wise loss\n                loss = self.loss_fct(\n                    shift_logits.view(-1, shift_logits.size(-1)),\n                    shift_labels.view(-1)\n                )\n\n                # Calculate average loss\n                sequence_loss = loss.sum() / len(loss)\n                loss_list.append(sequence_loss.cpu().item())\n\n                # Debug output\n                if debug:\n                    print(f\"\\nProcessing: '{text}'\")\n                    print(f\"With special tokens: '{text_with_special}'\")\n                    print(f\"Input tokens: {model_inputs['input_ids'][0].tolist()}\")\n                    print(f\"Target tokens: {shift_labels[0].tolist()}\")\n                    print(f\"Input decoded: {self.tokenizer.decode(model_inputs['input_ids'][0])}\")\n                    print(f\"Target decoded: {self.tokenizer.decode(shift_labels[0])}\")\n                    print(f\"Individual losses: {loss.tolist()}\")\n                    print(f\"Average loss: {sequence_loss.item():.4f}\")\n\n        ppl = [exp(i) for i in loss_list]\n\n        if debug:\n            print(\"\\nFinal perplexities:\")\n            for text, perp in zip(input_texts, ppl):\n                print(f\"Text: '{text}'\")\n                print(f\"Perplexity: {perp:.2f}\")\n\n        return ppl[0] if single_input else ppl\n\n    def clear_gpu_memory(self) -> None:\n        if not torch.cuda.is_available():\n            return\n\n        # Delete model and tokenizer if they exist\n        if hasattr(self, 'model'):\n            del self.model\n        if hasattr(self, 'tokenizer'):\n            del self.tokenizer\n\n        # Run garbage collection\n        gc.collect()\n\n        # Clear CUDA cache and reset memory stats\n        with DEVICE:\n            torch.cuda.empty_cache()\n            torch.cuda.ipc_collect()\n            torch.cuda.reset_peak_memory_stats()","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-12-19T19:56:37.515012Z","iopub.execute_input":"2024-12-19T19:56:37.515336Z","iopub.status.idle":"2024-12-19T19:56:42.021787Z","shell.execute_reply.started":"2024-12-19T19:56:37.515311Z","shell.execute_reply":"2024-12-19T19:56:42.021075Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nmodel_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\nscorer = PerplexityCalculator(model_path=model_path)","metadata":{"execution":{"iopub.status.busy":"2024-12-19T19:56:42.022906Z","iopub.execute_input":"2024-12-19T19:56:42.023559Z","iopub.status.idle":"2024-12-19T19:59:15.621934Z","shell.execute_reply.started":"2024-12-19T19:56:42.023528Z","shell.execute_reply":"2024-12-19T19:59:15.621276Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53ae4b1d8b894196b5931fb7823b0471"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"submission = pd.read_csv(\"/kaggle/input/santa-2024/sample_submission.csv\")\n#perplexities = scorer.get_perplexity(submission[\"text\"].tolist())\n#perplexities","metadata":{"execution":{"iopub.status.busy":"2024-12-19T19:59:15.623727Z","iopub.execute_input":"2024-12-19T19:59:15.624058Z","iopub.status.idle":"2024-12-19T19:59:15.645865Z","shell.execute_reply.started":"2024-12-19T19:59:15.624039Z","shell.execute_reply":"2024-12-19T19:59:15.645375Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import numpy as np\nfrom collections import deque\nfrom typing import Tuple\nfrom random import randint\n\nclass TabuSearch:\n    def __init__(self, tabu_tenure=5, stop_on_non_improvement=20, max_iter=100):\n        self.tabu_tenure = tabu_tenure\n        self.stop_on_non_improvement = stop_on_non_improvement\n        self.max_iter = max_iter\n        # self.calculate_cost = calculate_cost\n        self.tabu_list = deque(maxlen=self.tabu_tenure)\n    \n    def calculate_perplexity(self, sentence) -> float:\n        sentence = \" \".join(sentence)\n        submission = pd.DataFrame({'id': [0], 'text': [sentence] })\n        perplexities = scorer.get_perplexity(submission[\"text\"].tolist())\n        # print(f\"{perplexities[0]} || {sentence}\")\n        return perplexities[0]\n    \n    def is_sentence_visited(self, sentence):\n        return self.to_text(sentence) in self.tabu_list\n    \n    def to_text(self, sentence):\n        return \" \".join(sentence) \n\n    def get_candidates(self, sentence): # two-opt swap\n        neighbors = []\n        \n        for _ in range(len(sentence)):\n            node1 = 0\n            node2 = 0\n            \n            while node1 == node2:\n                node1 = randint(1, len(sentence)-1)\n                node2 = randint(1, len(sentence)-1)\n                \n            if node1 > node2:\n                swap = node1\n                node1 = node2\n                node2 = swap\n                \n            tmp = sentence[node1:node2]\n            tmp_route = sentence[:node1] + tmp[::-1] +sentence[node2:]\n            neighbors.append(tmp_route)\n            \n        return neighbors\n\n    def optimize(self, current_sentence) -> Tuple[str, float]:\n        current_perplexity = self.calculate_perplexity(current_sentence)\n\n        best_sentence = current_sentence\n        best_perplexity = current_perplexity\n\n        no_improvement = 0\n        itr = 1\n        \n        while True and itr <= self.max_iter:\n            print(f\"[ITER] [{itr}] --- {best_perplexity}\")\n            \n            candidates = self.get_candidates(current_sentence)\n\n            best_candidate = None\n            best_candidate_perplexity = None\n\n            for candidate in candidates:\n                candidate_perplexity = self.calculate_perplexity(candidate)\n                if self.is_sentence_visited(candidate) or best_candidate_perplexity is None or candidate_perplexity < best_candidate_perplexity:\n                        best_candidate = candidate\n                        best_candidate_perplexity = candidate_perplexity\n            # print(f\"[{best_candidate_perplexity}] {self.to_text(best_candidate)}\")\n\n            if best_candidate_perplexity < best_perplexity:\n                best_sentence = best_candidate\n                best_perplexity = best_candidate_perplexity\n                no_improvement = 0\n                print(f\"[IM] [{best_perplexity}]\")\n            else:\n                no_improvement += 1\n                print(f\"[NO IM] __{no_improvement}__\")\n\n            if no_improvement == self.stop_on_non_improvement:\n                break\n\n            current_sentence = best_candidate\n            current_perplexity = best_candidate_perplexity\n\n            self.tabu_list.append(best_sentence)\n            itr+=1\n\n        return self.to_text(best_sentence), best_perplexity\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/santa-2024/sample_submission.csv\").sample(1)\n\nts = TabuSearch(tabu_tenure=10, stop_on_non_improvement=70, max_iter=1000)\n\nsentence = [word for sublist in data['text'].str.split(' ') for word in sublist]\nts.optimize(sentence)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from random import randint\n\ndef get_candidates(sentence):\n        neighbors = []\n        \n        for _ in range(len(sentence)):\n            node1 = 0\n            node2 = 0\n            \n            while node1 == node2:\n                node1 = randint(1, len(sentence)-1)\n                node2 = randint(1, len(sentence)-1)\n                \n            if node1 > node2:\n                swap = node1\n                node1 = node2\n                node2 = swap\n                \n            tmp = sentence[node1:node2]\n            tmp_route = sentence[:node1] + tmp[::-1] +sentence[node2:]\n            neighbors.append(tmp_route)\n            \n        return neighbors\n\n#sen = [\"H\", \"A\", \"M\", \"Z\", \"A\"]\n#ng = get_candidates(sen)\n#ng","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:59:25.399074Z","iopub.status.idle":"2024-12-19T19:59:25.399384Z","shell.execute_reply":"2024-12-19T19:59:25.399267Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}