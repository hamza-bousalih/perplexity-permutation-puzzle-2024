{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":88046,"databundleVersionId":10229277,"sourceType":"competition"},{"sourceId":104492,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":72255,"modelId":76277}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gc\nimport os\nfrom math import exp\nfrom collections import Counter\nfrom typing import List, Optional, Union\n\nimport numpy as np\nimport pandas as pd\nimport transformers\nimport torch\n\nos.environ['OMP_NUM_THREADS'] = '1'\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\nPAD_TOKEN_LABEL_ID = torch.nn.CrossEntropyLoss().ignore_index\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\nclass ParticipantVisibleError(Exception):\n    pass\n\n\ndef score(\n    solution: pd.DataFrame,\n    submission: pd.DataFrame,\n    row_id_column_name: str,\n    model_path: str = '/kaggle/input/gemma-2/transformers/gemma-2-9b/2',\n    load_in_8bit: bool = False,\n    clear_mem: bool = False,\n) -> float:\n    # Check that each submitted string is a permutation of the solution string\n    sol_counts = solution.loc[:, 'text'].str.split().apply(Counter)\n    sub_counts = submission.loc[:, 'text'].str.split().apply(Counter)\n    invalid_mask = sol_counts != sub_counts\n    if invalid_mask.any():\n        raise ParticipantVisibleError(\n            'At least one submitted string is not a valid permutation of the solution string.'\n        )\n\n    # Calculate perplexity for the submitted strings\n    sub_strings = [\n        ' '.join(s.split()) for s in submission['text'].tolist()\n    ]  # Split and rejoin to normalize whitespace\n    scorer = PerplexityCalculator(\n        model_path=model_path,\n        load_in_8bit=load_in_8bit,\n    )  # Initialize the perplexity calculator with a pre-trained model\n    perplexities = scorer.get_perplexity(\n        sub_strings\n    )  # Calculate perplexity for each submitted string\n\n    if clear_mem:\n        # Just move on if it fails. Not essential if we have the score.\n        try:\n            scorer.clear_gpu_memory()\n        except:\n            print('GPU memory clearing failed.')\n\n    return float(np.mean(perplexities))\n\n\nclass PerplexityCalculator:\n    def __init__(\n        self,\n        model_path: str,\n        load_in_8bit: bool = False,\n        device_map: str = 'auto',\n    ):\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)\n        # Configure model loading based on quantization setting and device availability\n        if load_in_8bit:\n            if DEVICE.type != 'cuda':\n                raise ValueError('8-bit quantization requires CUDA device')\n            quantization_config = transformers.BitsAndBytesConfig(load_in_8bit=True)\n            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n                model_path,\n                quantization_config=quantization_config,\n                device_map=device_map,\n            )\n        else:\n            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n                model_path,\n                torch_dtype=torch.float16 if DEVICE.type == 'cuda' else torch.float32,\n                device_map=device_map,\n            )\n\n        self.loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n\n        self.model.eval()\n\n    def get_perplexity(\n        self, input_texts: Union[str, List[str]], debug=False\n    ) -> Union[float, List[float]]:\n        single_input = isinstance(input_texts, str)\n        input_texts = [input_texts] if single_input else input_texts\n\n        loss_list = []\n        with torch.no_grad():\n            # Process each sequence independently\n            for text in input_texts:\n                # Explicitly add sequence boundary tokens to the text\n                text_with_special = f\"{self.tokenizer.bos_token}{text}{self.tokenizer.eos_token}\"\n\n                # Tokenize\n                model_inputs = self.tokenizer(\n                    text_with_special,\n                    return_tensors='pt',\n                    add_special_tokens=False,\n                )\n\n                if 'token_type_ids' in model_inputs:\n                    model_inputs.pop('token_type_ids')\n\n                model_inputs = {k: v.to(DEVICE) for k, v in model_inputs.items()}\n\n                # Get model output\n                output = self.model(**model_inputs, use_cache=False)\n                logits = output['logits']\n\n                # Shift logits and labels for calculating loss\n                shift_logits = logits[..., :-1, :].contiguous()  # Drop last prediction\n                shift_labels = model_inputs['input_ids'][..., 1:].contiguous()  # Drop first input\n\n                # Calculate token-wise loss\n                loss = self.loss_fct(\n                    shift_logits.view(-1, shift_logits.size(-1)),\n                    shift_labels.view(-1)\n                )\n\n                # Calculate average loss\n                sequence_loss = loss.sum() / len(loss)\n                loss_list.append(sequence_loss.cpu().item())\n\n                # Debug output\n                if debug:\n                    print(f\"\\nProcessing: '{text}'\")\n                    print(f\"With special tokens: '{text_with_special}'\")\n                    print(f\"Input tokens: {model_inputs['input_ids'][0].tolist()}\")\n                    print(f\"Target tokens: {shift_labels[0].tolist()}\")\n                    print(f\"Input decoded: {self.tokenizer.decode(model_inputs['input_ids'][0])}\")\n                    print(f\"Target decoded: {self.tokenizer.decode(shift_labels[0])}\")\n                    print(f\"Individual losses: {loss.tolist()}\")\n                    print(f\"Average loss: {sequence_loss.item():.4f}\")\n\n        ppl = [exp(i) for i in loss_list]\n\n        if debug:\n            print(\"\\nFinal perplexities:\")\n            for text, perp in zip(input_texts, ppl):\n                print(f\"Text: '{text}'\")\n                print(f\"Perplexity: {perp:.2f}\")\n\n        return ppl[0] if single_input else ppl\n\n    def clear_gpu_memory(self) -> None:\n        if not torch.cuda.is_available():\n            return\n\n        # Delete model and tokenizer if they exist\n        if hasattr(self, 'model'):\n            del self.model\n        if hasattr(self, 'tokenizer'):\n            del self.tokenizer\n\n        # Run garbage collection\n        gc.collect()\n\n        # Clear CUDA cache and reset memory stats\n        with DEVICE:\n            torch.cuda.empty_cache()\n            torch.cuda.ipc_collect()\n            torch.cuda.reset_peak_memory_stats()","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-12-21T16:47:01.331507Z","iopub.status.busy":"2024-12-21T16:47:01.331132Z","iopub.status.idle":"2024-12-21T16:47:05.823572Z","shell.execute_reply":"2024-12-21T16:47:05.822623Z","shell.execute_reply.started":"2024-12-21T16:47:01.331479Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nmodel_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\nscorer = PerplexityCalculator(model_path=model_path)","metadata":{"execution":{"iopub.execute_input":"2024-12-21T16:47:56.638406Z","iopub.status.busy":"2024-12-21T16:47:56.638031Z","iopub.status.idle":"2024-12-21T16:50:06.314639Z","shell.execute_reply":"2024-12-21T16:50:06.313925Z","shell.execute_reply.started":"2024-12-21T16:47:56.638379Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ff2e8c68446b4b5a8dcd51d615153690","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"execution_count":4},{"cell_type":"code","source":"submission = pd.read_csv(\"/kaggle/input/santa-2024/sample_submission.csv\")\n#perplexities = scorer.get_perplexity(submission[\"text\"].tolist())\n#perplexities","metadata":{"execution":{"iopub.execute_input":"2024-12-21T16:50:06.315952Z","iopub.status.busy":"2024-12-21T16:50:06.315710Z","iopub.status.idle":"2024-12-21T16:50:06.324218Z","shell.execute_reply":"2024-12-21T16:50:06.323602Z","shell.execute_reply.started":"2024-12-21T16:50:06.315930Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from collections import deque\nfrom typing import Tuple\nimport random\nimport pandas as pd\n\nclass TabuSearch:\n    def __init__(self, sentence: str, tabu_tenure=5, update_ani=20, max_iter=100):\n        \"\"\"\n        update_ani: incress tabu_tenure after no improvment\n        \"\"\"\n        self.words = sentence.split()\n        self.n = len(self.words)\n        self.tabu_tenure = tabu_tenure\n        self.update_ani = update_ani\n        self.max_iter = max_iter\n        self.tabu_list = deque(maxlen=self.tabu_tenure)\n        \n        self.cache = {}\n        self.attempt = 0\n        self.calc = 0\n        self.cached = 0\n        \n    def logcaching(self):\n        print(f\"[CACHING] attempt: {self.attempt}, calc: {self.calc}, cached: {self.cached}\")\n    \n    def calculate_perplexity(self, sol) -> float:\n        self.attempt+=1\n        perplexity = 0\n        tsol = tuple(sol)\n        \n        if tsol in self.cache:\n            self.cached+=1\n            perplexity = self.cache[tsol]\n        else:\n            snetence = \" \".join([self.words[i] for i in sol])\n            submission = pd.DataFrame({'id': [0], 'text': [snetence] })\n            perplexity = scorer.get_perplexity(submission[\"text\"].tolist())[0]\n            self.cache[tsol] = perplexity\n            self.calc+=1\n        \n        return perplexity\n    \n    def init(self):\n        r = list(range(self.n))\n        random.shuffle(r)  \n        return r\n    \n    def is_sentence_visited(self, sentence):\n        return tuple(sentence) in self.tabu_list\n    \n    def to_text(self, sentence):\n        return \" \".join([self.words[i] for i in sentence])\n\n    def get_candidates(self, sentence: list):\n        n = len(sentence)\n        neighbors = []\n        \n        for i in range(n):\n            for j in range(i + 1, n):\n                neighbor = sentence.copy()\n                neighbor[i], neighbor[j] = neighbor[j], neighbor[i]\n                neighbors.append(neighbor)\n        \n        return neighbors\n\n    def optimize(self, current_sentence) -> Tuple[str, float]:\n        if current_sentence is None:\n            current_sentence = self.init()\n        \n        current_perplexity = self.calculate_perplexity(current_sentence)\n\n        best_sentence = current_sentence\n        best_perplexity = current_perplexity\n        print(f\"[START] {best_sentence} --- {best_perplexity}\")\n        \n        no_improvement = 0\n        itr = 1\n        \n        while True and itr <= self.max_iter:\n            # print(f\"[ITER] [{itr}/{self.max_iter}] --- {best_perplexity}\")\n            \n            candidates = self.get_candidates(current_sentence)\n\n            best_candidate = None\n            best_candidate_perplexity = None\n            \n            for candidate in candidates:\n                candidate_perplexity = self.calculate_perplexity(candidate)\n                if self.is_sentence_visited(candidate) or best_candidate_perplexity is None or candidate_perplexity < best_candidate_perplexity:\n                        best_candidate = candidate\n                        best_candidate_perplexity = candidate_perplexity\n            # print(f\"[{best_candidate_perplexity}] {self.to_text(best_candidate)}\")\n\n            if best_candidate_perplexity < best_perplexity:\n                best_sentence = best_candidate\n                best_perplexity = best_candidate_perplexity\n                no_improvement = 0\n                print(f\"[IM] [{itr}/{self.max_iter}] [{best_perplexity}]\")\n            else:\n                no_improvement += 1\n                # print(f\"[NO IM] __{no_improvement}__\")\n\n            if no_improvement == self.update_ani:\n                if self.tabu_tenure > 50:\n                    print(f\"[NO IM] [{itr}/{self.max_iter}] MAX [{self.tabu_tenure}] | break;\")\n                    break\n                no_improvement = 0\n                old = self.tabu_list\n                self.tabu_tenure += 5\n                self.tabu_list = deque(maxlen=self.tabu_tenure)\n                self.tabu_list.extend(old)\n                print(\"tabu_list\", end=\" || \")\n                print(f\"[NO IM] [{itr}/{self.max_iter}] INCRESS TABU_LIST [{self.tabu_tenure}]\")\n            \n            current_sentence = best_candidate\n            current_perplexity = best_candidate_perplexity\n\n            self.tabu_list.append(tuple(best_sentence))\n            itr+=1\n                \n        self.logcaching()\n        return self.to_text(best_sentence), best_sentence, best_perplexity","metadata":{"execution":{"iopub.status.busy":"2024-12-21T19:31:10.529936Z","iopub.execute_input":"2024-12-21T19:31:10.530223Z","iopub.status.idle":"2024-12-21T19:31:10.542516Z","shell.execute_reply.started":"2024-12-21T19:31:10.530199Z","shell.execute_reply":"2024-12-21T19:31:10.541697Z"},"trusted":true},"outputs":[],"execution_count":35},{"cell_type":"code","source":"import random\n\ndata = pd.read_csv(\"/kaggle/input/santa-2024/sample_submission.csv\")\n\nts = TabuSearch(data['text'][0], tabu_tenure=10, update_ani=50, max_iter=1000)\nts.optimize(ts.init())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T19:31:13.040501Z","iopub.execute_input":"2024-12-21T19:31:13.040817Z","iopub.status.idle":"2024-12-21T19:31:52.065441Z","shell.execute_reply.started":"2024-12-21T19:31:13.040791Z","shell.execute_reply":"2024-12-21T19:31:52.064678Z"}},"outputs":[{"name":"stdout","text":"[START] [1, 2, 8, 7, 4, 3, 9, 0, 5, 6] --- 2900.9736045356544\n[IM] [1/1000] [1641.5146668588116]\n[IM] [2/1000] [1322.6646815536633]\n[IM] [3/1000] [926.119419511793]\n[IM] [4/1000] [768.6019472806792]\n[IM] [15/1000] [688.6363021417106]\n[IM] [16/1000] [584.088676418536]\ntabu_list || [NO IM] [66/1000] INCRESS TABU_LIST [15]\n[CACHING] attempt: 4456, calc: 384, cached: 4072\ntabu_list || [NO IM] [116/1000] INCRESS TABU_LIST [20]\ntabu_list || [NO IM] [166/1000] INCRESS TABU_LIST [25]\n[CACHING] attempt: 8956, calc: 384, cached: 8572\ntabu_list || [NO IM] [216/1000] INCRESS TABU_LIST [30]\ntabu_list || [NO IM] [266/1000] INCRESS TABU_LIST [35]\n[CACHING] attempt: 13456, calc: 384, cached: 13072\ntabu_list || [NO IM] [316/1000] INCRESS TABU_LIST [40]\ntabu_list || [NO IM] [366/1000] INCRESS TABU_LIST [45]\n[CACHING] attempt: 17956, calc: 384, cached: 17572\ntabu_list || [NO IM] [416/1000] INCRESS TABU_LIST [50]\ntabu_list || [NO IM] [466/1000] INCRESS TABU_LIST [55]\n[CACHING] attempt: 22456, calc: 384, cached: 22072\n[NO IM] [516/1000] MAX [55] | break;\n","output_type":"stream"},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"('reindeer mistletoe gingerbread chimney fireplace advent scrooge elf ornament family',\n [8, 6, 5, 1, 4, 0, 9, 2, 7, 3],\n 584.088676418536)"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"import random\n\ndata = pd.read_csv(\"/kaggle/input/santa-2024/sample_submission.csv\")\n\nts = TabuSearch(data['text'][0], tabu_tenure=10, update_ani=50, max_iter=1000)\nts.optimize([8, 6, 5, 1, 4, 0, 9, 2, 7, 3])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T19:33:00.687060Z","iopub.execute_input":"2024-12-21T19:33:00.687393Z","iopub.status.idle":"2024-12-21T19:33:09.868937Z","shell.execute_reply.started":"2024-12-21T19:33:00.687327Z","shell.execute_reply":"2024-12-21T19:33:09.868229Z"}},"outputs":[{"name":"stdout","text":"[START] [8, 6, 5, 1, 4, 0, 9, 2, 7, 3] --- 584.088676418536\ntabu_list || [NO IM] [50/1000] INCRESS TABU_LIST [15]\n[CACHING] attempt: 4456, calc: 90, cached: 4366\ntabu_list || [NO IM] [100/1000] INCRESS TABU_LIST [20]\ntabu_list || [NO IM] [150/1000] INCRESS TABU_LIST [25]\n[CACHING] attempt: 8956, calc: 90, cached: 8866\ntabu_list || [NO IM] [200/1000] INCRESS TABU_LIST [30]\ntabu_list || [NO IM] [250/1000] INCRESS TABU_LIST [35]\n[CACHING] attempt: 13456, calc: 90, cached: 13366\ntabu_list || [NO IM] [300/1000] INCRESS TABU_LIST [40]\ntabu_list || [NO IM] [350/1000] INCRESS TABU_LIST [45]\n[CACHING] attempt: 17956, calc: 90, cached: 17866\ntabu_list || [NO IM] [400/1000] INCRESS TABU_LIST [50]\ntabu_list || [NO IM] [450/1000] INCRESS TABU_LIST [55]\n[CACHING] attempt: 22456, calc: 90, cached: 22366\n[NO IM] [500/1000] MAX [55] | break;\n","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"('reindeer mistletoe gingerbread chimney fireplace advent scrooge elf ornament family',\n [8, 6, 5, 1, 4, 0, 9, 2, 7, 3],\n 584.088676418536)"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"import random\n\ndata = pd.read_csv(\"/kaggle/input/santa-2024/sample_submission.csv\")\n\nres = []\n\nfor text in data['text']:\n    ts = TabuSearch(text, tabu_tenure=10, update_ani=50, max_iter=1000)\n    sen, sol, perplexity = ts.optimize(ts.init())\n    res.append((sen, sol, perplexity))\n\nres","metadata":{"execution":{"iopub.status.busy":"2024-12-21T19:38:59.368138Z","iopub.execute_input":"2024-12-21T19:38:59.368497Z","execution_failed":"2024-12-21T20:37:55.564Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[START] [2, 3, 6, 7, 1, 8, 5, 0, 4, 9] --- 10541.869315984432\n[IM] [1/1000] [2549.5211378804725]\n[IM] [2/1000] [1067.78985306711]\n[IM] [3/1000] [1037.2223535190114]\n[IM] [14/1000] [720.8217523105569]\n[IM] [15/1000] [653.1138361007249]\n[IM] [16/1000] [593.2027189881885]\n[IM] [27/1000] [552.8875041123517]\ntabu_list || [NO IM] [77/1000] INCRESS TABU_LIST [15]\n[CACHING] attempt: 4456, calc: 469, cached: 3987\ntabu_list || [NO IM] [127/1000] INCRESS TABU_LIST [20]\ntabu_list || [NO IM] [177/1000] INCRESS TABU_LIST [25]\n[CACHING] attempt: 8956, calc: 469, cached: 8487\ntabu_list || [NO IM] [227/1000] INCRESS TABU_LIST [30]\ntabu_list || [NO IM] [277/1000] INCRESS TABU_LIST [35]\n[CACHING] attempt: 13456, calc: 469, cached: 12987\ntabu_list || [NO IM] [327/1000] INCRESS TABU_LIST [40]\ntabu_list || [NO IM] [377/1000] INCRESS TABU_LIST [45]\n[CACHING] attempt: 17956, calc: 469, cached: 17487\ntabu_list || [NO IM] [427/1000] INCRESS TABU_LIST [50]\ntabu_list || [NO IM] [477/1000] INCRESS TABU_LIST [55]\n[CACHING] attempt: 22456, calc: 469, cached: 21987\n[NO IM] [527/1000] MAX [55] | break;\n[START] [1, 9, 2, 18, 15, 19, 7, 13, 10, 14, 4, 3, 12, 8, 0, 17, 6, 5, 11, 16] --- 4593.146567089566\n[IM] [1/1000] [2514.9447713454197]\n[IM] [2/1000] [2059.0551646411463]\n[IM] [3/1000] [1793.8566370791784]\n[IM] [4/1000] [1497.7252383644222]\n[IM] [5/1000] [1468.4008713083465]\n[IM] [16/1000] [1263.7394780628906]\n[IM] [17/1000] [935.374656514985]\n[IM] [18/1000] [841.5939724131025]\n[IM] [19/1000] [836.5674700057602]\n[IM] [30/1000] [808.6073554371927]\ntabu_list || [NO IM] [80/1000] INCRESS TABU_LIST [15]\n[CACHING] attempt: 18811, calc: 2257, cached: 16554\ntabu_list || [NO IM] [130/1000] INCRESS TABU_LIST [20]\ntabu_list || [NO IM] [180/1000] INCRESS TABU_LIST [25]\n[CACHING] attempt: 37811, calc: 2257, cached: 35554\ntabu_list || [NO IM] [230/1000] INCRESS TABU_LIST [30]\ntabu_list || [NO IM] [280/1000] INCRESS TABU_LIST [35]\n[CACHING] attempt: 56811, calc: 2257, cached: 54554\ntabu_list || [NO IM] [330/1000] INCRESS TABU_LIST [40]\ntabu_list || [NO IM] [380/1000] INCRESS TABU_LIST [45]\n[CACHING] attempt: 75811, calc: 2257, cached: 73554\ntabu_list || [NO IM] [430/1000] INCRESS TABU_LIST [50]\ntabu_list || [NO IM] [480/1000] INCRESS TABU_LIST [55]\n[CACHING] attempt: 94811, calc: 2257, cached: 92554\n[NO IM] [530/1000] MAX [55] | break;\n[START] [8, 18, 15, 12, 2, 5, 11, 3, 14, 16, 0, 6, 19, 9, 13, 1, 7, 10, 4, 17] --- 11748.725143481153\n[IM] [1/1000] [2345.438554422693]\n[IM] [2/1000] [1084.7271578557315]\n[IM] [3/1000] [771.5185605184013]\n[IM] [4/1000] [708.6549929453342]\n[IM] [5/1000] [642.6887654931609]\n[IM] [6/1000] [572.8479190893522]\n[IM] [7/1000] [536.4808364317028]\n[IM] [18/1000] [509.6441174359445]\n[IM] [19/1000] [502.6331368227454]\n[IM] [20/1000] [476.25827989193596]\n[IM] [21/1000] [475.8192755036899]\n[IM] [32/1000] [422.96598228241453]\n[IM] [33/1000] [419.914443019113]\n[IM] [34/1000] [396.8468938737865]\n[IM] [35/1000] [377.08776209139035]\n[IM] [36/1000] [374.3807839725067]\n[IM] [37/1000] [373.99199380944737]\ntabu_list || [NO IM] [87/1000] INCRESS TABU_LIST [15]\n[CACHING] attempt: 18811, calc: 3382, cached: 15429\ntabu_list || [NO IM] [137/1000] INCRESS TABU_LIST [20]\ntabu_list || [NO IM] [187/1000] INCRESS TABU_LIST [25]\n[CACHING] attempt: 37811, calc: 3382, cached: 34429\ntabu_list || [NO IM] [237/1000] INCRESS TABU_LIST [30]\ntabu_list || [NO IM] [287/1000] INCRESS TABU_LIST [35]\n[CACHING] attempt: 56811, calc: 3382, cached: 53429\ntabu_list || [NO IM] [337/1000] INCRESS TABU_LIST [40]\ntabu_list || [NO IM] [387/1000] INCRESS TABU_LIST [45]\n[CACHING] attempt: 75811, calc: 3382, cached: 72429\ntabu_list || [NO IM] [437/1000] INCRESS TABU_LIST [50]\ntabu_list || [NO IM] [487/1000] INCRESS TABU_LIST [55]\n[CACHING] attempt: 94811, calc: 3382, cached: 91429\n[NO IM] [537/1000] MAX [55] | break;\n[START] [3, 25, 15, 24, 1, 20, 22, 10, 21, 9, 18, 17, 12, 11, 2, 8, 16, 4, 27, 29, 26, 28, 14, 0, 6, 5, 19, 13, 7, 23] --- 3905.379069079091\n[IM] [1/1000] [2142.7413427530983]\n[IM] [2/1000] [1415.471583372897]\n[IM] [3/1000] [1163.344557421129]\n[IM] [4/1000] [1026.4810476641194]\n[IM] [5/1000] [924.9940030789302]\n[IM] [7/1000] [922.5726078140035]\n[IM] [8/1000] [791.286639570288]\n[IM] [9/1000] [716.2125593907841]\n[IM] [10/1000] [644.9436450602495]\n[IM] [11/1000] [605.3977599173]\n[IM] [13/1000] [583.7091835569987]\n[IM] [14/1000] [549.9350322184602]\n[IM] [15/1000] [531.5183714391629]\n[IM] [26/1000] [499.54797141934415]\n[IM] [27/1000] [439.1689393469256]\n[IM] [28/1000] [415.4608673267663]\n[IM] [39/1000] [395.6606758963954]\n[IM] [40/1000] [392.99273372010896]\n[IM] [51/1000] [380.13972526780344]\n[IM] [62/1000] [371.49956855185013]\n[IM] [63/1000] [356.0788200759916]\n[IM] [74/1000] [352.9434110502995]\n[CACHING] attempt: 43066, calc: 13413, cached: 29653\ntabu_list || [NO IM] [124/1000] INCRESS TABU_LIST [15]\ntabu_list || [NO IM] [174/1000] INCRESS TABU_LIST [20]\n[CACHING] attempt: 86566, calc: 13413, cached: 73153\ntabu_list || [NO IM] [224/1000] INCRESS TABU_LIST [25]\ntabu_list || [NO IM] [274/1000] INCRESS TABU_LIST [30]\n[CACHING] attempt: 130066, calc: 13413, cached: 116653\ntabu_list || [NO IM] [324/1000] INCRESS TABU_LIST [35]\ntabu_list || [NO IM] [374/1000] INCRESS TABU_LIST [40]\n[CACHING] attempt: 173566, calc: 13413, cached: 160153\ntabu_list || [NO IM] [424/1000] INCRESS TABU_LIST [45]\ntabu_list || [NO IM] [474/1000] INCRESS TABU_LIST [50]\n[CACHING] attempt: 217066, calc: 13413, cached: 203653\ntabu_list || [NO IM] [524/1000] INCRESS TABU_LIST [55]\n[NO IM] [574/1000] MAX [55] | break;\n[START] [17, 37, 43, 24, 25, 34, 12, 4, 29, 2, 38, 27, 15, 30, 19, 22, 16, 40, 5, 45, 42, 35, 33, 9, 47, 41, 13, 0, 39, 3, 46, 44, 21, 18, 1, 36, 23, 7, 49, 20, 14, 11, 10, 32, 8, 31, 6, 26, 48, 28] --- 1551.4470863122472\n[IM] [1/1000] [1294.6690476231993]\n[IM] [2/1000] [1058.6192313261124]\n[IM] [3/1000] [947.809595181662]\n[IM] [4/1000] [828.464094338354]\n[IM] [5/1000] [763.0593225026845]\n[IM] [6/1000] [707.5731462133728]\n[IM] [7/1000] [668.8341214655125]\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}