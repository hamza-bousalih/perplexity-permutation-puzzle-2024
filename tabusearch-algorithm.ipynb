{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-19T18:33:13.475255Z",
     "iopub.status.busy": "2024-12-19T18:33:13.474925Z",
     "iopub.status.idle": "2024-12-19T18:33:17.058755Z",
     "shell.execute_reply": "2024-12-19T18:33:17.058058Z",
     "shell.execute_reply.started": "2024-12-19T18:33:13.475215Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "from math import exp\n",
    "from collections import Counter\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "PAD_TOKEN_LABEL_ID = torch.nn.CrossEntropyLoss().ignore_index\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def score(\n",
    "    solution: pd.DataFrame,\n",
    "    submission: pd.DataFrame,\n",
    "    row_id_column_name: str,\n",
    "    model_path: str = '/kaggle/input/gemma-2/transformers/gemma-2-9b/2',\n",
    "    load_in_8bit: bool = False,\n",
    "    clear_mem: bool = False,\n",
    ") -> float:\n",
    "    # Check that each submitted string is a permutation of the solution string\n",
    "    sol_counts = solution.loc[:, 'text'].str.split().apply(Counter)\n",
    "    sub_counts = submission.loc[:, 'text'].str.split().apply(Counter)\n",
    "    invalid_mask = sol_counts != sub_counts\n",
    "    if invalid_mask.any():\n",
    "        raise ParticipantVisibleError(\n",
    "            'At least one submitted string is not a valid permutation of the solution string.'\n",
    "        )\n",
    "\n",
    "    # Calculate perplexity for the submitted strings\n",
    "    sub_strings = [\n",
    "        ' '.join(s.split()) for s in submission['text'].tolist()\n",
    "    ]  # Split and rejoin to normalize whitespace\n",
    "    scorer = PerplexityCalculator(\n",
    "        model_path=model_path,\n",
    "        load_in_8bit=load_in_8bit,\n",
    "    )  # Initialize the perplexity calculator with a pre-trained model\n",
    "    perplexities = scorer.get_perplexity(\n",
    "        sub_strings\n",
    "    )  # Calculate perplexity for each submitted string\n",
    "\n",
    "    if clear_mem:\n",
    "        # Just move on if it fails. Not essential if we have the score.\n",
    "        try:\n",
    "            scorer.clear_gpu_memory()\n",
    "        except:\n",
    "            print('GPU memory clearing failed.')\n",
    "\n",
    "    return float(np.mean(perplexities))\n",
    "\n",
    "\n",
    "class PerplexityCalculator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: str,\n",
    "        load_in_8bit: bool = False,\n",
    "        device_map: str = 'auto',\n",
    "    ):\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)\n",
    "        # Configure model loading based on quantization setting and device availability\n",
    "        if load_in_8bit:\n",
    "            if DEVICE.type != 'cuda':\n",
    "                raise ValueError('8-bit quantization requires CUDA device')\n",
    "            quantization_config = transformers.BitsAndBytesConfig(load_in_8bit=True)\n",
    "            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                quantization_config=quantization_config,\n",
    "                device_map=device_map,\n",
    "            )\n",
    "        else:\n",
    "            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                torch_dtype=torch.float16 if DEVICE.type == 'cuda' else torch.float32,\n",
    "                device_map=device_map,\n",
    "            )\n",
    "\n",
    "        self.loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "    def get_perplexity(\n",
    "        self, input_texts: Union[str, List[str]], debug=False\n",
    "    ) -> Union[float, List[float]]:\n",
    "        single_input = isinstance(input_texts, str)\n",
    "        input_texts = [input_texts] if single_input else input_texts\n",
    "\n",
    "        loss_list = []\n",
    "        with torch.no_grad():\n",
    "            # Process each sequence independently\n",
    "            for text in input_texts:\n",
    "                # Explicitly add sequence boundary tokens to the text\n",
    "                text_with_special = f\"{self.tokenizer.bos_token}{text}{self.tokenizer.eos_token}\"\n",
    "\n",
    "                # Tokenize\n",
    "                model_inputs = self.tokenizer(\n",
    "                    text_with_special,\n",
    "                    return_tensors='pt',\n",
    "                    add_special_tokens=False,\n",
    "                )\n",
    "\n",
    "                if 'token_type_ids' in model_inputs:\n",
    "                    model_inputs.pop('token_type_ids')\n",
    "\n",
    "                model_inputs = {k: v.to(DEVICE) for k, v in model_inputs.items()}\n",
    "\n",
    "                # Get model output\n",
    "                output = self.model(**model_inputs, use_cache=False)\n",
    "                logits = output['logits']\n",
    "\n",
    "                # Shift logits and labels for calculating loss\n",
    "                shift_logits = logits[..., :-1, :].contiguous()  # Drop last prediction\n",
    "                shift_labels = model_inputs['input_ids'][..., 1:].contiguous()  # Drop first input\n",
    "\n",
    "                # Calculate token-wise loss\n",
    "                loss = self.loss_fct(\n",
    "                    shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                    shift_labels.view(-1)\n",
    "                )\n",
    "\n",
    "                # Calculate average loss\n",
    "                sequence_loss = loss.sum() / len(loss)\n",
    "                loss_list.append(sequence_loss.cpu().item())\n",
    "\n",
    "                # Debug output\n",
    "                if debug:\n",
    "                    print(f\"\\nProcessing: '{text}'\")\n",
    "                    print(f\"With special tokens: '{text_with_special}'\")\n",
    "                    print(f\"Input tokens: {model_inputs['input_ids'][0].tolist()}\")\n",
    "                    print(f\"Target tokens: {shift_labels[0].tolist()}\")\n",
    "                    print(f\"Input decoded: {self.tokenizer.decode(model_inputs['input_ids'][0])}\")\n",
    "                    print(f\"Target decoded: {self.tokenizer.decode(shift_labels[0])}\")\n",
    "                    print(f\"Individual losses: {loss.tolist()}\")\n",
    "                    print(f\"Average loss: {sequence_loss.item():.4f}\")\n",
    "\n",
    "        ppl = [exp(i) for i in loss_list]\n",
    "\n",
    "        if debug:\n",
    "            print(\"\\nFinal perplexities:\")\n",
    "            for text, perp in zip(input_texts, ppl):\n",
    "                print(f\"Text: '{text}'\")\n",
    "                print(f\"Perplexity: {perp:.2f}\")\n",
    "\n",
    "        return ppl[0] if single_input else ppl\n",
    "\n",
    "    def clear_gpu_memory(self) -> None:\n",
    "        if not torch.cuda.is_available():\n",
    "            return\n",
    "\n",
    "        # Delete model and tokenizer if they exist\n",
    "        if hasattr(self, 'model'):\n",
    "            del self.model\n",
    "        if hasattr(self, 'tokenizer'):\n",
    "            del self.tokenizer\n",
    "\n",
    "        # Run garbage collection\n",
    "        gc.collect()\n",
    "\n",
    "        # Clear CUDA cache and reset memory stats\n",
    "        with DEVICE:\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "            torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T18:33:17.847035Z",
     "iopub.status.busy": "2024-12-19T18:33:17.846636Z",
     "iopub.status.idle": "2024-12-19T18:35:55.485446Z",
     "shell.execute_reply": "2024-12-19T18:35:55.484491Z",
     "shell.execute_reply.started": "2024-12-19T18:33:17.847012Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6372014eead94368a5ed6a9935a27206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n",
    "scorer = PerplexityCalculator(model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T18:35:55.487076Z",
     "iopub.status.busy": "2024-12-19T18:35:55.486578Z",
     "iopub.status.idle": "2024-12-19T18:35:55.507032Z",
     "shell.execute_reply": "2024-12-19T18:35:55.506137Z",
     "shell.execute_reply.started": "2024-12-19T18:35:55.487044Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"/kaggle/input/santa-2024/sample_submission.csv\")\n",
    "#perplexities = scorer.get_perplexity(submission[\"text\"].tolist())\n",
    "#perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T18:53:51.478034Z",
     "iopub.status.busy": "2024-12-19T18:53:51.477743Z",
     "iopub.status.idle": "2024-12-19T18:53:51.487051Z",
     "shell.execute_reply": "2024-12-19T18:53:51.486151Z",
     "shell.execute_reply.started": "2024-12-19T18:53:51.478010Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "from typing import Tuple, List\n",
    "\n",
    "class TabuSearch:\n",
    "    def __init__(self, tabu_tenure=5, stop_on_non_improvement=20, max_ter=100):\n",
    "        self.tabu_tenure = tabu_tenure\n",
    "        self.stop_on_non_improvement = stop_on_non_improvement\n",
    "        self.max_ter = max_ter\n",
    "        # self.calculate_cost = calculate_cost\n",
    "        self.tabu_list = deque(maxlen=self.tabu_tenure)\n",
    "    \n",
    "    def calculate_perplexity(self, sentence) -> float:\n",
    "        sentence = \" \".join(sentence)\n",
    "        submission = pd.DataFrame({'id': [0], 'text': [sentence] })\n",
    "        perplexities = scorer.get_perplexity(submission[\"text\"].tolist())\n",
    "        # print(f\"{perplexities[0]} || {sentence}\")\n",
    "        return perplexities[0]\n",
    "    \n",
    "    def is_sentence_visited(self, sentence):\n",
    "        return self.to_text(sentence) in self.tabu_list\n",
    "    \n",
    "    def to_text(self, sentence):\n",
    "        return \" \".join(sentence) \n",
    "\n",
    "    def get_candidates(self, solution):\n",
    "        n = len(solution)\n",
    "        neighbors = []\n",
    "\n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, n):\n",
    "                neighbor = solution.copy()\n",
    "                neighbor[i], neighbor[j] = neighbor[j], neighbor[i]\n",
    "                neighbors.append(neighbor)\n",
    "\n",
    "        return neighbors\n",
    "\n",
    "    def optimize(self, current_sentence) -> Tuple[str, float]:\n",
    "        current_perplexity = self.calculate_perplexity(current_sentence)\n",
    "\n",
    "        best_sentence = current_sentence\n",
    "        best_perplexity = current_perplexity\n",
    "\n",
    "        no_improvement = 0\n",
    "        itr = 1\n",
    "        \n",
    "        while True and itr <= self.max_ter:\n",
    "            itr+=1\n",
    "            print(f\"[ITER] [{itr}] --- {best_perplexity}\")\n",
    "            \n",
    "            candidates = self.get_candidates(current_sentence)\n",
    "\n",
    "            best_candidate = None\n",
    "            best_candidate_perplexity = None\n",
    "\n",
    "            for candidate in candidates:\n",
    "                candidate_perplexity = self.calculate_perplexity(candidate)\n",
    "                if self.is_sentence_visited(candidate) or best_candidate_perplexity is None or candidate_perplexity < best_candidate_perplexity:\n",
    "                        best_candidate = candidate\n",
    "                        best_candidate_perplexity = candidate_perplexity\n",
    "            print(f\"[{best_candidate_perplexity}] {self.to_text(best_candidate)}\")\n",
    "\n",
    "            if best_candidate_perplexity < best_perplexity:\n",
    "                best_sentence = best_candidate\n",
    "                best_perplexity = best_candidate_perplexity\n",
    "                no_improvement = 0\n",
    "                print(f\"[IM] [{best_perplexity}]\")\n",
    "            else:\n",
    "                no_improvement += 1\n",
    "                print(f\"[NO IM] __{no_improvement}__\")\n",
    "\n",
    "            if no_improvement == self.stop_on_non_improvement:\n",
    "                break\n",
    "\n",
    "            current_sentence = best_candidate\n",
    "            current_perplexity = best_candidate_perplexity\n",
    "\n",
    "            self.tabu_list.append(best_sentence)\n",
    "\n",
    "        return self.to_text(best_sentence), best_perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T18:53:56.568775Z",
     "iopub.status.busy": "2024-12-19T18:53:56.568491Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[304.5799402664961] peppermint candle poinsettia snowglobe hohoho eggnog fruitcake chocolate candy puzzle game doll toy workshop wonder believe dream hope peace joy merry season greeting card wrapping paper bow fireplace night cookie milk star wish wreath angel the to of and in that have it not with as you from we kaggle\n",
      "[IM] [304.5799402664961]\n",
      "[278.9364085500469] peppermint candle poinsettia fruitcake hohoho eggnog snowglobe chocolate candy puzzle game doll toy workshop wonder believe dream hope peace joy merry season greeting card wrapping paper bow fireplace night cookie milk star wish wreath angel the to of and in that have it not with as you from we kaggle\n",
      "[IM] [278.9364085500469]\n",
      "[259.9079500103052] peppermint candle poinsettia fruitcake hohoho eggnog snowglobe chocolate candy puzzle game toy doll workshop wonder believe dream hope peace joy merry season greeting card wrapping paper bow fireplace night cookie milk star wish wreath angel the to of and in that have it not with as you from we kaggle\n",
      "[IM] [259.9079500103052]\n",
      "[246.4850470446721] peppermint candle hohoho fruitcake poinsettia eggnog snowglobe chocolate candy puzzle game toy doll workshop wonder believe dream hope peace joy merry season greeting card wrapping paper bow fireplace night cookie milk star wish wreath angel the to of and in that have it not with as you from we kaggle\n",
      "[IM] [246.4850470446721]\n",
      "[235.9407250339335] peppermint candle hohoho fruitcake poinsettia eggnog snowglobe chocolate candy puzzle game toy doll workshop wonder believe dream hope peace joy merry season greeting card wrapping paper bow fireplace night cookie milk star wish wreath angel the to of and in that have not it with as you from we kaggle\n",
      "[IM] [235.9407250339335]\n",
      "[214.1058110351341] peppermint candle night fruitcake poinsettia eggnog snowglobe chocolate candy puzzle game toy doll workshop dream believe wonder hope peace joy merry season greeting card wrapping paper bow fireplace hohoho cookie milk star wish wreath angel the to of and in that have not it with as you from we kaggle\n",
      "[IM] [214.1058110351341]\n",
      "[204.26076747641602] peppermint candle night fruitcake eggnog poinsettia snowglobe chocolate candy puzzle game toy doll workshop dream believe wonder hope peace joy merry season greeting card wrapping paper bow fireplace hohoho cookie milk star wish wreath angel the to of and in that have not it with as you from we kaggle\n",
      "[IM] [204.26076747641602]\n",
      "[198.55197625193443] peppermint candle night fruitcake eggnog poinsettia snowglobe chocolate candy puzzle game toy doll workshop dream believe wonder hope peace joy merry season greeting card wrapping paper bow fireplace angel cookie milk star wish wreath hohoho the to of and in that have not it with as you from we kaggle\n",
      "[IM] [198.55197625193443]\n",
      "[192.371428039738] peppermint candle night fruitcake eggnog poinsettia snowglobe chocolate candy puzzle game toy doll workshop dream believe wonder hope peace joy merry season greeting card wrapping paper bow star angel cookie milk fireplace wish wreath hohoho the to of and in that have not it with as you from we kaggle\n",
      "[IM] [192.371428039738]\n",
      "[187.3088762488356] peppermint candle night fruitcake eggnog poinsettia snowglobe chocolate candy puzzle game toy doll workshop dream believe wonder hope peace joy merry season greeting card wrapping paper bow star angel cookie milk fireplace wish wreath kaggle the to of and in that have not it with as you from we hohoho\n",
      "[IM] [187.3088762488356]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"/kaggle/input/santa-2024/sample_submission.csv\").sample(1)\n",
    "\n",
    "ts = TabuSearch(tabu_tenure=20, stop_on_non_improvement=20)\n",
    "\n",
    "sentence = [word for sublist in data['text'].str.split(' ') for word in sublist]\n",
    "ts.optimize(sentence)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 10229277,
     "sourceId": 88046,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 76277,
     "modelInstanceId": 72255,
     "sourceId": 104492,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
