{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e05df299",
      "metadata": {},
      "source": [
        "# WOA + TabuSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d97bccb6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-31T20:03:01.281597Z",
          "iopub.status.busy": "2024-12-31T20:03:01.281306Z",
          "iopub.status.idle": "2024-12-31T20:03:04.926601Z",
          "shell.execute_reply": "2024-12-31T20:03:04.925646Z",
          "shell.execute_reply.started": "2024-12-31T20:03:01.281572Z"
        },
        "id": "d97bccb6",
        "papermill": {
          "duration": 4.184328,
          "end_time": "2024-12-29T17:53:21.724893",
          "exception": false,
          "start_time": "2024-12-29T17:53:17.540565",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import os\n",
        "from math import exp\n",
        "from collections import Counter\n",
        "from typing import List, Optional, Union\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "from collections import deque\n",
        "from typing import Tuple, List\n",
        "from collections import deque\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b50ebb67",
      "metadata": {
        "id": "b50ebb67"
      },
      "source": [
        "## GEMMA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05f83c54",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2024-12-31T20:03:04.928118Z",
          "iopub.status.busy": "2024-12-31T20:03:04.927686Z",
          "iopub.status.idle": "2024-12-31T20:03:05.013956Z",
          "shell.execute_reply": "2024-12-31T20:03:05.013034Z",
          "shell.execute_reply.started": "2024-12-31T20:03:04.928083Z"
        },
        "id": "05f83c54",
        "papermill": {
          "duration": 0.090415,
          "end_time": "2024-12-29T17:53:21.818503",
          "exception": false,
          "start_time": "2024-12-29T17:53:21.728088",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "os.environ['OMP_NUM_THREADS'] = '1'\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "PAD_TOKEN_LABEL_ID = torch.nn.CrossEntropyLoss().ignore_index\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class ParticipantVisibleError(Exception):\n",
        "    pass\n",
        "\n",
        "def score(\n",
        "    solution: pd.DataFrame,\n",
        "    submission: pd.DataFrame,\n",
        "    row_id_column_name: str,\n",
        "    model_path: str = '/kaggle/input/gemma-2/transformers/gemma-2-9b/2',\n",
        "    load_in_8bit: bool = False,\n",
        "    clear_mem: bool = False,\n",
        ") -> float:\n",
        "    # Check that each submitted string is a permutation of the solution string\n",
        "    sol_counts = solution.loc[:, 'text'].str.split().apply(Counter)\n",
        "    sub_counts = submission.loc[:, 'text'].str.split().apply(Counter)\n",
        "    invalid_mask = sol_counts != sub_counts\n",
        "    if invalid_mask.any():\n",
        "        raise ParticipantVisibleError(\n",
        "            'At least one submitted string is not a valid permutation of the solution string.'\n",
        "        )\n",
        "\n",
        "    # Calculate perplexity for the submitted strings\n",
        "    sub_strings = [\n",
        "        ' '.join(s.split()) for s in submission['text'].tolist()\n",
        "    ]  # Split and rejoin to normalize whitespace\n",
        "    scorer = PerplexityCalculator(\n",
        "        model_path=model_path,\n",
        "        load_in_8bit=load_in_8bit,\n",
        "    )  # Initialize the perplexity calculator with a pre-trained model\n",
        "    perplexities = scorer.get_perplexity(\n",
        "        sub_strings\n",
        "    )  # Calculate perplexity for each submitted string\n",
        "\n",
        "    if clear_mem:\n",
        "        # Just move on if it fails. Not essential if we have the score.\n",
        "        try:\n",
        "            scorer.clear_gpu_memory()\n",
        "        except:\n",
        "            print('GPU memory clearing failed.')\n",
        "\n",
        "    return float(np.mean(perplexities))\n",
        "\n",
        "class PerplexityCalculator:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_path: str,\n",
        "        load_in_8bit: bool = False,\n",
        "        device_map: str = 'auto',\n",
        "    ):\n",
        "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)\n",
        "        # Configure model loading based on quantization setting and device availability\n",
        "        if load_in_8bit:\n",
        "            if DEVICE.type != 'cuda':\n",
        "                raise ValueError('8-bit quantization requires CUDA device')\n",
        "            quantization_config = transformers.BitsAndBytesConfig(load_in_8bit=True)\n",
        "            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "                model_path,\n",
        "                quantization_config=quantization_config,\n",
        "                device_map=device_map,\n",
        "            )\n",
        "        else:\n",
        "            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "                model_path,\n",
        "                torch_dtype=torch.float16 if DEVICE.type == 'cuda' else torch.float32,\n",
        "                device_map=device_map,\n",
        "            )\n",
        "\n",
        "        self.loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "    def get_perplexity(\n",
        "        self, input_texts: Union[str, List[str]], debug=False\n",
        "    ) -> Union[float, List[float]]:\n",
        "        single_input = isinstance(input_texts, str)\n",
        "        input_texts = [input_texts] if single_input else input_texts\n",
        "\n",
        "        loss_list = []\n",
        "        with torch.no_grad():\n",
        "            # Process each sequence independently\n",
        "            for text in input_texts:\n",
        "                # Explicitly add sequence boundary tokens to the text\n",
        "                text_with_special = f\"{self.tokenizer.bos_token}{text}{self.tokenizer.eos_token}\"\n",
        "\n",
        "                # Tokenize\n",
        "                model_inputs = self.tokenizer(\n",
        "                    text_with_special,\n",
        "                    return_tensors='pt',\n",
        "                    add_special_tokens=False,\n",
        "                )\n",
        "\n",
        "                if 'token_type_ids' in model_inputs:\n",
        "                    model_inputs.pop('token_type_ids')\n",
        "\n",
        "                model_inputs = {k: v.to(DEVICE) for k, v in model_inputs.items()}\n",
        "\n",
        "                # Get model output\n",
        "                output = self.model(**model_inputs, use_cache=False)\n",
        "                logits = output['logits']\n",
        "\n",
        "                # Shift logits and labels for calculating loss\n",
        "                shift_logits = logits[..., :-1, :].contiguous()  # Drop last prediction\n",
        "                shift_labels = model_inputs['input_ids'][..., 1:].contiguous()  # Drop first input\n",
        "\n",
        "                # Calculate token-wise loss\n",
        "                loss = self.loss_fct(\n",
        "                    shift_logits.view(-1, shift_logits.size(-1)),\n",
        "                    shift_labels.view(-1)\n",
        "                )\n",
        "\n",
        "                # Calculate average loss\n",
        "                sequence_loss = loss.sum() / len(loss)\n",
        "                loss_list.append(sequence_loss.cpu().item())\n",
        "\n",
        "                # Debug output\n",
        "                if debug:\n",
        "                    print(f\"\\nProcessing: '{text}'\")\n",
        "                    print(f\"With special tokens: '{text_with_special}'\")\n",
        "                    print(f\"Input tokens: {model_inputs['input_ids'][0].tolist()}\")\n",
        "                    print(f\"Target tokens: {shift_labels[0].tolist()}\")\n",
        "                    print(f\"Input decoded: {self.tokenizer.decode(model_inputs['input_ids'][0])}\")\n",
        "                    print(f\"Target decoded: {self.tokenizer.decode(shift_labels[0])}\")\n",
        "                    print(f\"Individual losses: {loss.tolist()}\")\n",
        "                    print(f\"Average loss: {sequence_loss.item():.4f}\")\n",
        "\n",
        "        ppl = [exp(i) for i in loss_list]\n",
        "\n",
        "        if debug:\n",
        "            print(\"\\nFinal perplexities:\")\n",
        "            for text, perp in zip(input_texts, ppl):\n",
        "                print(f\"Text: '{text}'\")\n",
        "                print(f\"Perplexity: {perp:.2f}\")\n",
        "\n",
        "        return ppl[0] if single_input else ppl\n",
        "\n",
        "    def clear_gpu_memory(self) -> None:\n",
        "        if not torch.cuda.is_available():\n",
        "            return\n",
        "\n",
        "        # Delete model and tokenizer if they exist\n",
        "        if hasattr(self, 'model'):\n",
        "            del self.model\n",
        "        if hasattr(self, 'tokenizer'):\n",
        "            del self.tokenizer\n",
        "\n",
        "        # Run garbage collection\n",
        "        gc.collect()\n",
        "\n",
        "        # Clear CUDA cache and reset memory stats\n",
        "        with DEVICE:\n",
        "            torch.cuda.empty_cache()\n",
        "            torch.cuda.ipc_collect()\n",
        "            torch.cuda.reset_peak_memory_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "001c4b31",
      "metadata": {
        "id": "001c4b31"
      },
      "source": [
        "---\n",
        "# CACHING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df9165e8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-31T20:03:05.015910Z",
          "iopub.status.busy": "2024-12-31T20:03:05.015656Z",
          "iopub.status.idle": "2024-12-31T20:03:05.029331Z",
          "shell.execute_reply": "2024-12-31T20:03:05.028513Z",
          "shell.execute_reply.started": "2024-12-31T20:03:05.015885Z"
        },
        "id": "df9165e8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class Caching:\n",
        "    def __init__(self, max_size: int = 1000):\n",
        "        self.stores: List[dict] = []\n",
        "        self.idx = -1\n",
        "        self.max_size = max_size\n",
        "        self._state = {'len': 0, 'hit': 0, 'miss': 0, 'total': 0}\n",
        "\n",
        "    def create_store(self):\n",
        "        self.stores.append({})\n",
        "        self.idx += 1\n",
        "        self._state['len'] += 1\n",
        "\n",
        "    def add(self, key, value):\n",
        "        if self.idx == -1 or len(self.stores[self.idx]) >= self.max_size:\n",
        "            self.create_store()\n",
        "        self.stores[self.idx][key] = value\n",
        "\n",
        "    def get(self, key):\n",
        "        if self.idx == -1:\n",
        "            return None\n",
        "        self._state['total'] += 1\n",
        "        store_idx = random.choices(range(self.idx+1), k=self.idx+1)\n",
        "        for i in store_idx:\n",
        "            v = self.stores[i].get(key, None)\n",
        "            if v is not None:\n",
        "                self._state['hit'] += 1\n",
        "                return v\n",
        "        self._state['miss'] += 1\n",
        "        return v\n",
        "\n",
        "    def state(self):\n",
        "        return f\"total: {self._state['total']}, len: {self._state['len']}, stores: {len(self.stores)} \"\\\n",
        "              f\"hit: {self._state['hit']} [{self._state['hit']/self._state['total']*100}%], \"\\\n",
        "              f\"miss: {self._state['miss']} [{self._state['miss']/self._state['total']*100}%]\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a49681d0",
      "metadata": {
        "id": "a49681d0",
        "papermill": {
          "duration": 0.002792,
          "end_time": "2024-12-29T17:56:05.356078",
          "exception": false,
          "start_time": "2024-12-29T17:56:05.353286",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "---\n",
        "# TabuSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "808ef37d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-31T20:03:05.030630Z",
          "iopub.status.busy": "2024-12-31T20:03:05.030295Z",
          "iopub.status.idle": "2024-12-31T20:03:05.045410Z",
          "shell.execute_reply": "2024-12-31T20:03:05.044731Z",
          "shell.execute_reply.started": "2024-12-31T20:03:05.030605Z"
        },
        "id": "808ef37d",
        "papermill": {
          "duration": 0.016426,
          "end_time": "2024-12-29T17:56:05.375373",
          "exception": false,
          "start_time": "2024-12-29T17:56:05.358947",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class TabuSearch:\n",
        "    '''\n",
        "    Algorithme de recherche tabou\n",
        "\n",
        "    Cette classe implémente l'algorithme d'optimisation de recherche tabou, utilisé pour résoudre\n",
        "    des problèmes d'optimisation combinatoire. Il améliore itérativement les solutions en explorant\n",
        "    des candidats voisins et en utilisant une liste tabou pour éviter de revisiter des solutions récemment explorées.\n",
        "\n",
        "    Paramètres :\n",
        "        words (list[str]): La séquence de mots à optimiser.\n",
        "        calculate_perplexity (callable): Une méthode pour calculer la perplexité d'une phrase.\n",
        "        tabu_tenure (int, optionnel): La taille de la liste tabou. Par défaut, 5.\n",
        "        stop_on_non_improvement (int, optionnel): Le nombre d'itérations autorisées sans amélioration avant d'arrêter. Par défaut, 20.\n",
        "        max_iter (int, optionnel): Le nombre maximal d'itérations à exécuter. Par défaut, 100.\n",
        "        max_candidates (int, optionnel): Le nombre maximal de solutions candidates à générer par itération. Par défaut, 100.\n",
        "        debug (bool, optionnel): Si True, active la journalisation de débogage. Par défaut, False.\n",
        "        logger (callable, optionnel): Une fonction de journalisation. Si None, imprime dans la console. Par défaut, None.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, words: list, calculate_perplexity, tabu_tenure=5,\n",
        "                 stop_on_non_improvement=20, max_iter=100,\n",
        "                 max_condidates=100,\n",
        "                 debug=False, logger = None):\n",
        "        '''\n",
        "        Initialise l'instance de recherche tabou.\n",
        "\n",
        "        Paramètres :\n",
        "            words (list[str]): La séquence de mots à optimiser.\n",
        "            calculate_perplexity (callable): Une méthode pour calculer la perplexité d'une phrase.\n",
        "            tabu_tenure (int, optionnel): La taille de la liste tabou. Par défaut, 5.\n",
        "            stop_on_non_improvement (int, optionnel): Le nombre d'itérations autorisées sans amélioration. Par défaut, 20.\n",
        "            max_iter (int, optionnel): Le nombre maximal d'itérations. Par défaut, 100.\n",
        "            max_condidates (int, optionnel): Le nombre maximal de solutions candidates par itération. Par défaut, 100.\n",
        "            debug (bool, optionnel): Si True, active la journalisation de débogage. Par défaut, False.\n",
        "            logger (callable, optionnel): Une fonction de journalisation. Par défaut, None.\n",
        "        '''\n",
        "        self.words = words  # Liste des mots à optimiser\n",
        "        self.n = len(self.words)  # Nombre total de mots\n",
        "        self.tabu_tenure = tabu_tenure  # Taille de la liste tabou\n",
        "        self.stop_on_non_improvement = stop_on_non_improvement  # Condition d'arrêt en cas de non-amélioration\n",
        "        self.max_iter = max_iter  # Maximum d'itérations\n",
        "        self.calculate_perplexity = calculate_perplexity  # Méthode pour calculer la perplexité\n",
        "        self.tabu_list = deque(maxlen=self.tabu_tenure)  # Liste tabou pour suivre les mouvements récents\n",
        "        self.max_condidates = max_condidates  # Nombre maximal de solutions candidates à générer\n",
        "        self.debug = debug  # Indicateur pour activer le débogage\n",
        "        self.logger = logger  # Fonction de journalisation\n",
        "\n",
        "    def log(self, msg):\n",
        "        '''\n",
        "        Journalise un message si le débogage est activé.\n",
        "\n",
        "        Paramètres :\n",
        "            msg (str): Le message à journaliser.\n",
        "        '''\n",
        "        if self.debug:  # Vérifie si le débogage est activé\n",
        "            if self.logger is None:  # Si aucun logger fourni\n",
        "                print(f\"[LOCAL SEARCH] {msg}\")  # Imprime le message\n",
        "            else:\n",
        "                self.logger(f\"[LOCAL SEARCH] {msg}\")  # Utilise le logger fourni\n",
        "\n",
        "    def is_move_used(self, move: Tuple[int, int]):\n",
        "        '''\n",
        "        Vérifie si un mouvement est présent dans la liste tabou.\n",
        "        '''\n",
        "        return move in self.tabu_list  # Vérifie si le mouvement est dans la liste tabou\n",
        "\n",
        "    def __to_sentence_text(self, solution):\n",
        "        '''\n",
        "        Convertit une solution en phrase en mappant les indices aux mots.\n",
        "\n",
        "          '''\n",
        "        return \" \".join([self.words[i] for i in solution])  # Retourne la phrase correspondante\n",
        "\n",
        "    def to_tabulist(self, move):\n",
        "        '''\n",
        "        Ajoute un mouvement à la liste tabou.\n",
        "\n",
        "        Paramètres :\n",
        "            move (Tuple[int, int]): Le mouvement à ajouter à la liste tabou.\n",
        "        '''\n",
        "        self.tabu_list.append(tuple(move))  # Ajoute le mouvement à la liste tabou\n",
        "\n",
        "    def get_candidates(self, solution: np.ndarray) -> List[Tuple[list, Tuple[int, int]]]:\n",
        "        '''\n",
        "        Génère des solutions candidates en échangeant aléatoirement deux éléments.\n",
        "\n",
        "        Paramètres :\n",
        "            solution (np.ndarray): La solution actuelle sous forme d'un tableau d'indices.\n",
        "\n",
        "        Retourne :\n",
        "            list[Tuple[list, Tuple[int, int]]]: Une liste de solutions candidates et leurs mouvements correspondants.\n",
        "        '''\n",
        "        candidates = []  # Liste pour stocker les candidats\n",
        "\n",
        "        while len(candidates) < self.max_condidates:  # Tant que le nombre de candidats est inférieur au maximum\n",
        "            i, j = random.choices(solution, k=2)  # Sélectionne deux indices aléatoires\n",
        "            candidate = solution.copy()  # Copie de la solution actuelle\n",
        "            candidate[i], candidate[j] = candidate[j], candidate[i]  # Échange les deux éléments\n",
        "            candidates.append((candidate, (i, j)))  # Ajoute le candidat et le mouvement à la liste\n",
        "        return candidates  # Retourne la liste des candidats\n",
        "\n",
        "    def optimize(self, current_sentence: np.ndarray) -> Tuple[np.ndarray, float]:\n",
        "        '''\n",
        "        Exécute l'algorithme de recherche tabou.\n",
        "\n",
        "        Paramètres :\n",
        "            current_sentence (np.ndarray): La solution initiale sous forme d'un tableau d'indices.\n",
        "\n",
        "        Retourne :\n",
        "            Tuple[np.ndarray, float]: La meilleure solution et sa perplexité correspondante.\n",
        "        '''\n",
        "        self.log(f\"start {current_sentence}\")  # Journalise le début de l'optimisation\n",
        "\n",
        "        # Calcule la perplexité de la solution initiale\n",
        "        current_perplexity = self.calculate_perplexity(current_sentence)\n",
        "\n",
        "        # Initialise la meilleure solution et sa perplexité\n",
        "        best_sentence = current_sentence\n",
        "        best_perplexity = current_perplexity\n",
        "\n",
        "        # Suivre le nombre d'itérations sans amélioration et le nombre total d'itérations\n",
        "        no_improvement = 0  # Compteur d'itérations sans amélioration\n",
        "        itr = 1  # Compteur d'itérations\n",
        "\n",
        "        # Itérer jusqu'à ce que la condition d'arrêt soit atteinte\n",
        "        while no_improvement < self.stop_on_non_improvement and itr <= self.max_iter:\n",
        "            self.log(f\"[ITER] [{itr}] --- {best_perplexity}\")  # Journalise l'itération en cours\n",
        "\n",
        "            # Génère des solutions candidates\n",
        "            candidates = self.get_candidates(current_sentence)\n",
        "\n",
        "            # Initialise les variables pour la meilleure solution candidate\n",
        "            best_candidate = None\n",
        "            best_candidate_perplexity = None\n",
        "            used_move = None\n",
        "\n",
        "            # Évalue chaque solution candidate\n",
        "            for candidate, swap in candidates:\n",
        "                candidate_perplexity = self.calculate_perplexity(candidate)  # Calcule la perplexité de la candidate\n",
        "                # Met à jour la meilleure candidate si elle est meilleure et respecte les conditions tabou\n",
        "                if best_candidate_perplexity is None \\\n",
        "                    or (((not self.is_move_used(swap)) or (candidate_perplexity < best_perplexity)) \\\n",
        "                        and (candidate_perplexity < best_candidate_perplexity)):\n",
        "                        best_candidate = candidate  # Met à jour la meilleure candidate\n",
        "                        best_candidate_perplexity = candidate_perplexity  # Met à jour la perplexité de la meilleure candidate\n",
        "                        used_move = swap  # Enregistre le mouvement utilisé\n",
        "\n",
        "            self.log(f\"[{best_candidate_perplexity}] {best_candidate}\")\n",
        "\n",
        "            # Vérifie si la meilleure candidate est une amélioration\n",
        "            if best_candidate_perplexity < best_perplexity:\n",
        "                # Met à jour la meilleure solution\n",
        "                best_sentence = best_candidate\n",
        "                best_perplexity = best_candidate_perplexity\n",
        "                no_improvement = 0\n",
        "                self.log(f\"[IM] [{itr}/{self.max_iter}] [{best_perplexity}]\")\n",
        "            else:\n",
        "                # Incrémente le compteur d'itérations sans amélioration\n",
        "                no_improvement += 1\n",
        "                self.log(f\"[NO IM] __{no_improvement}__\")\n",
        "\n",
        "            # Considère la meilleure candidate comme la solution actuelle\n",
        "            current_sentence = best_candidate\n",
        "            current_perplexity = best_candidate_perplexity\n",
        "\n",
        "\n",
        "            self.to_tabulist(used_move)\n",
        "            itr += 1\n",
        "\n",
        "            # Sortie si la limite de non-amélioration est atteinte\n",
        "            if(no_improvement == self.stop_on_non_improvement):\n",
        "                self.log(f\"[NO IM] break; {best_sentence} | {best_perplexity}\")\n",
        "                break\n",
        "\n",
        "        # Retourne la meilleure solution et sa perplexité\n",
        "        return best_sentence, best_perplexity"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15651c7f",
      "metadata": {
        "id": "15651c7f",
        "papermill": {
          "duration": 0.002429,
          "end_time": "2024-12-29T17:56:05.390506",
          "exception": false,
          "start_time": "2024-12-29T17:56:05.388077",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "---\n",
        "# WAHLE OPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d437fd5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-31T20:03:09.670684Z",
          "iopub.status.busy": "2024-12-31T20:03:09.670364Z",
          "iopub.status.idle": "2024-12-31T20:03:09.692037Z",
          "shell.execute_reply": "2024-12-31T20:03:09.691194Z",
          "shell.execute_reply.started": "2024-12-31T20:03:09.670654Z"
        },
        "id": "8d437fd5",
        "papermill": {
          "duration": 0.022337,
          "end_time": "2024-12-29T17:56:05.415462",
          "exception": false,
          "start_time": "2024-12-29T17:56:05.393125",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from itertools import permutations\n",
        "\n",
        "class WhaleOptimization:\n",
        "    \"\"\"\n",
        "    Implementation de l'algorithme d'optimisation des baleines (WOA) pour optimiser l'ordre des mots\n",
        "    dans une phrase en fonction du score de perplexité.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, sentence: str, scorer,\n",
        "                 n_whales: int = 20, max_iter: int = 80,\n",
        "                 tabu_tenure=10, tabu_no_imp=10, tabu_max_iter=50,\n",
        "                 cache: Caching = Caching(), logger=None, debug=False):\n",
        "        \"\"\"\n",
        "        Initialise l'algorithme d'optimisation des baleines.\n",
        "\n",
        "        Args:\n",
        "            sentence (str): Phrase d'entrée à optimiser.\n",
        "            scorer (object): Scorer avec une méthode `get_perplexity` pour évaluer la qualité de la phrase.\n",
        "            n_whales (int, optionnel): Nombre de baleines dans la population. Par défaut, 20.\n",
        "            max_iter (int, optionnel): Itérations maximales pour WOA. Par défaut, 80.\n",
        "            tabu_tenure (int, optionnel): Taille de la liste tabou dans la recherche tabou. Par défaut, 10.\n",
        "            tabu_no_imp (int, optionnel): Itérations sans amélioration pour arrêter la recherche tabou. Par défaut, 10.\n",
        "            tabu_max_iter (int, optionnel): Itérations maximales dans la recherche tabou. Par défaut, 50.\n",
        "            logger (callable, optionnel): Fonction de journalisation. Par défaut, None.\n",
        "            debug (bool, optionnel): Activer les messages de débogage. Par défaut, False.\n",
        "        \"\"\"\n",
        "        self.words = sentence.split(' ')  # Sépare la phrase en mots\n",
        "        self.n = len(self.words)  # Nombre total de mots\n",
        "        self.scorer = scorer  # Instance du scoreur pour évaluer la perplexité\n",
        "        self.n_whales = n_whales  # Nombre de baleines\n",
        "        self.max_iter = max_iter  # Nombre maximal d'itérations\n",
        "        self.logger = logger  # Fonction de journalisation\n",
        "        self.debug = debug  # Indicateur pour activer le débogage\n",
        "\n",
        "        self.cwi = None  # Index de la baleine actuelle\n",
        "        self.itr = ''  # Itération actuelle\n",
        "\n",
        "        # Initialise la recherche tabou pour des améliorations locales\n",
        "        self.TabuSearch = TabuSearch(\n",
        "            self.words,\n",
        "            self.__calculate_perplexity,\n",
        "            tabu_tenure=tabu_tenure,\n",
        "            stop_on_non_improvement=tabu_no_imp,\n",
        "            max_iter=tabu_max_iter,\n",
        "            debug=debug,\n",
        "            logger=self.log\n",
        "        )\n",
        "\n",
        "        # Cache pour les calculs de perplexité\n",
        "        self.cache = cache\n",
        "\n",
        "    def __create_initial_sols(self) -> np.ndarray:\n",
        "        \"\"\"Génère une solution initiale aléatoire (permutation de mots).\"\"\"\n",
        "        return np.random.permutation(self.n)  # Retourne une permutation aléatoire des indices des mots\n",
        "\n",
        "    def log(self, msg):\n",
        "        \"\"\"\n",
        "        Journalise les messages pour le débogage et le suivi de l'exécution.\n",
        "\n",
        "        Args:\n",
        "            msg (str): Le message à journaliser.\n",
        "        \"\"\"\n",
        "        if self.debug:  # Vérifie si le débogage est activé\n",
        "            if self.logger is None:  # Si aucun logger fourni\n",
        "                if self.cwi is None:\n",
        "                    print(f\"[WOA] [{self.itr}] {msg}\")  # Imprime le message sans baleine actuelle\n",
        "                else:\n",
        "                    print(f\"[WOA] [{self.itr}] [WHALE {self.cwi + 1}] {msg}\")  # Imprime avec l'index de la baleine\n",
        "            else:\n",
        "                if self.cwi is None:\n",
        "                    self.logger(f\"[{self.itr}] {msg}\")  # Utilise le logger fourni\n",
        "                else:\n",
        "                    self.logger(f\"[{self.itr}] [WHALE {self.cwi + 1}] {msg}\")  # Journalise avec l'index de la baleine\n",
        "\n",
        "    def _compute_A(self, a: float):\n",
        "        \"\"\"\n",
        "        Calcule le coefficient A pour les équations de mouvement du WOA.\n",
        "\n",
        "        \"\"\"\n",
        "        r = np.random.uniform(0.0, 1.0, size=1)  # Génère un nombre aléatoire entre 0 et 1\n",
        "        return (2.0*np.multiply(a, r)) - a  # Calcule et retourne le coefficient A\n",
        "\n",
        "    def _compute_C(self):\n",
        "        \"\"\"\n",
        "        Calcule le coefficient C pour les équations de mouvement du WOA.\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        r = np.random.uniform(0.0, 1.0, size=1)  # Génère un nombre aléatoire entre 0 et 1\n",
        "        return 2.0 * r  # Calcule et retourne le coefficient C\n",
        "\n",
        "    def __to_sentence_text(self, sol):\n",
        "        \"\"\"\n",
        "        Convertit une solution (ordre des mots) en une phrase lisible.\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        return \" \".join([self.words[i] for i in sol])  # Crée et retourne la phrase à partir des indices\n",
        "\n",
        "    def __get_from_cache(self, solution: tuple) -> float:\n",
        "        \"\"\"Récupère la perplexité d'une solution à partir du cache.\"\"\"\n",
        "        return self.cache.get(solution)  # Retourne la valeur correspondante dans le cache\n",
        "\n",
        "    def __calculate_perplexity(self, solution: np.ndarray) -> float:\n",
        "        \"\"\"\n",
        "        Calcule la perplexité d'une solution donnée, avec mise en cache.\n",
        "\n",
        "          \"\"\"\n",
        "        solt = tuple(solution)\n",
        "        cached = self.__get_from_cache(solt)\n",
        "        if cached is not None:\n",
        "            return cached\n",
        "\n",
        "\n",
        "        sentence = self.__to_sentence_text(solution)\n",
        "        submission = pd.DataFrame({'id': [0], 'text': [sentence] })\n",
        "        perplexities = self.scorer.get_perplexity(submission[\"text\"].tolist())\n",
        "        self.cache.add(solt, perplexities[0])\n",
        "        return perplexities[0]\n",
        "\n",
        "    def __caching_state(self):\n",
        "        \"\"\"Affiche l'état du cache.\"\"\"\n",
        "        self.log(self.cache.state())\n",
        "\n",
        "    def __encircling_prey(self, current_pos: np.ndarray, best_pos: np.ndarray, A: float, C: float) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Méthode pour encercler une proie ( sur meilleure solution).\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        D = abs(C * best_pos - current_pos)  # Calcule la distance\n",
        "        new_pos = best_pos - A * D  # Calcule la nouvelle position\n",
        "        return np.argsort(new_pos)  # Retourne l'ordre des indices de la nouvelle position\n",
        "\n",
        "    def __search_for_prey(self, current_pos: np.ndarray, random_pos: np.ndarray, A: float, C: float) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Méthode pour rechercher une proie aléatoire.\n",
        "\n",
        "         \"\"\"\n",
        "        D = abs(C * random_pos - current_pos)  # Calcule la distance\n",
        "        new_pos = random_pos - A * D  # Calcule la nouvelle position\n",
        "        return np.argsort(new_pos)  # Retourne l'ordre des indices de la nouvelle position\n",
        "\n",
        "    def __bubble_net_attack(self, current_pos: np.ndarray, best_pos: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Méthode d'attaque par filet à bulles.\n",
        "\n",
        "          \"\"\"\n",
        "        D = abs(best_pos - current_pos)  # Calcule la distance\n",
        "        b = 1  # Paramètre pour le calcul\n",
        "        l = random.uniform(-1, 1)  # Génère un nombre aléatoire entre -1 et 1\n",
        "        new_pos = D * np.exp(l * b) * np.cos(2 * np.pi * l) + best_pos  # Calcule la nouvelle position\n",
        "        return np.argsort(new_pos)  # Retourne l'ordre des indices de la nouvelle position\n",
        "\n",
        "    def __amend_position(self, position: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Assure que la position est une permutation valide.\"\"\"\n",
        "        return np.argsort(position)\n",
        "\n",
        "    def __local_search(self, solution: np.ndarray) -> Tuple[np.ndarray, float]:\n",
        "        \"\"\"Applique la recherche locale pour améliorer la solution.\"\"\"\n",
        "        return self.TabuSearch.optimize(solution)\n",
        "    def optimize(self) -> Tuple[np.ndarray, float, List[float], List[list]]:\n",
        "        \"\"\"\n",
        "        Exécute l'algorithme d'optimisation des baleines pour trouver le meilleur ordre de mots.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[np.ndarray, float]: La meilleure solution et son score de perplexité.\n",
        "        \"\"\"\n",
        "\n",
        "        # Initialise la population de baleines avec des solutions aléatoires\n",
        "        population = []\n",
        "        perplexities_values = []\n",
        "        for w in range(self.n_whales):\n",
        "            self.cwi = w\n",
        "            solution = self.__create_initial_sols()  # Crée une solution initiale\n",
        "            self.log(f\"sol {solution} --> to improve\")\n",
        "            improved_solution, improved_perplexity = self.__local_search(solution)  # Améliore la solution localement\n",
        "            self.log(f\"{improved_solution} | {improved_perplexity}\")\n",
        "            population.append(improved_solution)  # Ajoute la solution améliorée à la population\n",
        "            perplexities_values.append(improved_perplexity)  # Ajoute la perplexité correspondante\n",
        "        self.cwi = None\n",
        "\n",
        "        self.log(\"------------------------------\")\n",
        "        self.log(f\"[WHALES]\")\n",
        "        self.log(population)\n",
        "        self.log(perplexities_values)\n",
        "        self.log(\"------------------------------\")\n",
        "\n",
        "        # Identifie la meilleure baleine initiale\n",
        "        best_idx = np.argmin(perplexities_values)\n",
        "        prey = population[best_idx].copy()\n",
        "        best_perplexity = perplexities_values[best_idx]  # Récupère la perplexité correspondante\n",
        "\n",
        "        self.log(f\"[PREY] : {prey} | {best_perplexity}\")\n",
        "\n",
        "        # Démarre la boucle principale de WOA\n",
        "        self.itr = 1\n",
        "        while self.itr <= self.max_iter:  # Boucle jusqu'à ce que le nombre maximal d'itérations soit atteint\n",
        "            self.log(f\"[START ITER] {prey} | {best_perplexity}\")\n",
        "\n",
        "\n",
        "            a = 2 - self.itr / self.max_iter * 2  # Calcule le paramètre a\n",
        "\n",
        "            # Met à jour chaque baleine dans la population\n",
        "            for self.cwi in range(self.n_whales):\n",
        "                cwhale = population[self.cwi]  # Récupère la baleine actuelle\n",
        "                self.log(f\"{cwhale} | {perplexities_values[self.cwi]}\")\n",
        "                A = self._compute_A(a)  # Calcule le coefficient A\n",
        "                C = self._compute_C()  # Calcule le coefficient C\n",
        "                p = random.random()  # Génère un nombre aléatoire pour déterminer le mouvement\n",
        "                self.log(f\"a={a}, A={A}, C={C}, p={p}\")\n",
        "\n",
        "                if p < 0.5:\n",
        "                    if abs(A) < 1:\n",
        "                        # Encercle la mielleure sulution afin de le  rapprocher\n",
        "                        new_pos = self.__encircling_prey(cwhale, prey, A, C)  # Calcule la nouvelle position\n",
        "                        self.log(f\"[MOVE] encircling prey --> {new_pos}\")\n",
        "                    else:\n",
        "                        # Recherche une autre solution(explorer autre zone  de recherche)\n",
        "                        rand_idx = random.randint(0, self.n_whales - 1)  # Choisit une whale aléatoire\n",
        "                        random_pos = population[rand_idx]  # Récupère sa position\n",
        "                        new_pos = self.__search_for_prey(cwhale, random_pos, A, C)  # Calcule la nouvelle position\n",
        "                        self.log(f\"[MOVE] search for prey --> {new_pos}\")\n",
        "                else:\n",
        "                    new_pos = self.__bubble_net_attack(cwhale, prey)  # Effectue une attaque par filet à bulles(exploitation)\n",
        "                    self.log(f\"[MOVE] bubble net attack --> {new_pos}\")\n",
        "\n",
        "\n",
        "                # apres la recuperation de la nouvelle position\n",
        "                # Applique une recherche locale pour améliorer la nouvelle position\n",
        "                improved_pos, improved_perplexity = self.__local_search(new_pos)\n",
        "                self.log(f\"[IMPROVE POSITION] {new_pos}, [{improved_perplexity}]\")\n",
        "\n",
        "\n",
        "                population[self.cwi] = improved_pos  # Met à jour la population avec la solution améliorée\n",
        "                perplexities_values[self.cwi] = improved_perplexity  # Met à jour la valeur de perplexité\n",
        "\n",
        "            # Met à jour la mieulleur solution  si une autre meilleure solution est trouvée dans la  nouvelle population\n",
        "            best_idx = np.argmin(perplexities_values)\n",
        "            if perplexities_values[best_idx] < best_perplexity:  # Vérifie si une meilleure solution a été trouvée\n",
        "                prey = population[best_idx]  # Met à jour la mieulleur solution\n",
        "                best_perplexity = perplexities_values[best_idx]  # Met à jour la perplexité\n",
        "                self.log(f\"[RES] [IMP] Prey: {prey} with perplexity {best_perplexity}\")\n",
        "                self.log(f\"[RES] [NO IMP] Prey: {prey} with perplexity {best_perplexity}\")\n",
        "\n",
        "            self.cwi = None\n",
        "            self.itr += 1\n",
        "\n",
        "        self.__caching_state()\n",
        "        return prey, best_perplexity, self.__to_sentence_text(prey)  # Retourne la meilleure solution, sa perplexité et la phrase reconstruite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8a95137-50d9-4c85-85ea-4019ee66934a",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "4b4a50ae8e39479e84fd859df5068849"
          ]
        },
        "execution": {
          "iopub.execute_input": "2024-12-31T20:03:09.693604Z",
          "iopub.status.busy": "2024-12-31T20:03:09.693371Z",
          "iopub.status.idle": "2024-12-31T20:05:55.095517Z",
          "shell.execute_reply": "2024-12-31T20:05:55.094716Z",
          "shell.execute_reply.started": "2024-12-31T20:03:09.693584Z"
        },
        "id": "a8a95137-50d9-4c85-85ea-4019ee66934a",
        "outputId": "24981e5e-5669-4578-dd78-dd34ac653ea0",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b4a50ae8e39479e84fd859df5068849",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n",
        "scorer = PerplexityCalculator(model_path=model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dbcf993-4ffa-43d6-96ce-af367778cec8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-31T20:05:55.097349Z",
          "iopub.status.busy": "2024-12-31T20:05:55.096967Z",
          "iopub.status.idle": "2024-12-31T20:06:33.892277Z",
          "shell.execute_reply": "2024-12-31T20:06:33.891407Z",
          "shell.execute_reply.started": "2024-12-31T20:05:55.097325Z"
        },
        "id": "3dbcf993-4ffa-43d6-96ce-af367778cec8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# text = 'advent chimney elf family fireplace gingerbread mistletoe ornament reindeer scrooge'\n",
        "\n",
        "# woa = WhaleOptimization(\n",
        "#     text, score,\n",
        "#     n_whales=3, max_iter=1, debug=True,\n",
        "#     tabu_tenure=5, tabu_no_imp=1, tabu_max_iter=4\n",
        "# )\n",
        "# woa.optimize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29349879",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-31T20:06:33.893834Z",
          "iopub.status.busy": "2024-12-31T20:06:33.893250Z",
          "iopub.status.idle": "2024-12-31T20:06:33.897450Z",
          "shell.execute_reply": "2024-12-31T20:06:33.896618Z",
          "shell.execute_reply.started": "2024-12-31T20:06:33.893808Z"
        },
        "id": "29349879",
        "papermill": {
          "duration": 34077.046284,
          "end_time": "2024-12-30T03:24:02.474536",
          "exception": false,
          "start_time": "2024-12-29T17:56:05.428252",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "problem = pd.read_csv(\"/kaggle/input/santa-2024/sample_submission.csv\")\n",
        "\n",
        "res = []\n",
        "\n",
        "for idx, row in problem.iterrows():\n",
        "    print(\"----------------------------------------------\")\n",
        "    print(\"----------------------------------------------\")\n",
        "    print(\"----------------------------------------------\")\n",
        "    def logger(msg):\n",
        "        print(f\"[WOA {idx}] {msg}\")\n",
        "    woa = WhaleOptimization(\n",
        "        row.text, score,\n",
        "        n_whales=3, max_iter=10, debug=True, \n",
        "        tabu_tenure=10, tabu_no_imp=3, tabu_max_iter=20,\n",
        "        logger = logger\n",
        "    )\n",
        "    sol, p, sen = woa.optimize()\n",
        "    res.append({\n",
        "        \"id\": len(res),\n",
        "        \"or\": row.text,\n",
        "        \"sol\": sol,\n",
        "        \"text\": sen,\n",
        "        \"p\": p\n",
        "    })\n",
        "\n",
        "submission_ = pd.DataFrame(res)\n",
        "submission_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66c5acde",
      "metadata": {
        "id": "66c5acde"
      },
      "outputs": [],
      "source": [
        "submission = submission_[['id', 'text']]\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "submission"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "databundleVersionId": 10229277,
          "sourceId": 88046,
          "sourceType": "competition"
        },
        {
          "isSourceIdPinned": true,
          "modelId": 76277,
          "modelInstanceId": 72255,
          "sourceId": 104492,
          "sourceType": "modelInstanceVersion"
        }
      ],
      "dockerImageVersionId": 30823,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 34250.814184,
      "end_time": "2024-12-30T03:24:06.176525",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-12-29T17:53:15.362341",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
