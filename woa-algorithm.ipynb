{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d97bccb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T20:03:01.281597Z",
     "iopub.status.busy": "2024-12-31T20:03:01.281306Z",
     "iopub.status.idle": "2024-12-31T20:03:04.926601Z",
     "shell.execute_reply": "2024-12-31T20:03:04.925646Z",
     "shell.execute_reply.started": "2024-12-31T20:03:01.281572Z"
    },
    "papermill": {
     "duration": 4.184328,
     "end_time": "2024-12-29T17:53:21.724893",
     "exception": false,
     "start_time": "2024-12-29T17:53:17.540565",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "from math import exp\n",
    "from collections import Counter\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "from collections import deque\n",
    "from typing import Tuple, List\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50ebb67",
   "metadata": {},
   "source": [
    "## GEMMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05f83c54",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-31T20:03:04.928118Z",
     "iopub.status.busy": "2024-12-31T20:03:04.927686Z",
     "iopub.status.idle": "2024-12-31T20:03:05.013956Z",
     "shell.execute_reply": "2024-12-31T20:03:05.013034Z",
     "shell.execute_reply.started": "2024-12-31T20:03:04.928083Z"
    },
    "papermill": {
     "duration": 0.090415,
     "end_time": "2024-12-29T17:53:21.818503",
     "exception": false,
     "start_time": "2024-12-29T17:53:21.728088",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "PAD_TOKEN_LABEL_ID = torch.nn.CrossEntropyLoss().ignore_index\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "def score(\n",
    "    solution: pd.DataFrame,\n",
    "    submission: pd.DataFrame,\n",
    "    row_id_column_name: str,\n",
    "    model_path: str = '/kaggle/input/gemma-2/transformers/gemma-2-9b/2',\n",
    "    load_in_8bit: bool = False,\n",
    "    clear_mem: bool = False,\n",
    ") -> float:\n",
    "    # Check that each submitted string is a permutation of the solution string\n",
    "    sol_counts = solution.loc[:, 'text'].str.split().apply(Counter)\n",
    "    sub_counts = submission.loc[:, 'text'].str.split().apply(Counter)\n",
    "    invalid_mask = sol_counts != sub_counts\n",
    "    if invalid_mask.any():\n",
    "        raise ParticipantVisibleError(\n",
    "            'At least one submitted string is not a valid permutation of the solution string.'\n",
    "        )\n",
    "\n",
    "    # Calculate perplexity for the submitted strings\n",
    "    sub_strings = [\n",
    "        ' '.join(s.split()) for s in submission['text'].tolist()\n",
    "    ]  # Split and rejoin to normalize whitespace\n",
    "    scorer = PerplexityCalculator(\n",
    "        model_path=model_path,\n",
    "        load_in_8bit=load_in_8bit,\n",
    "    )  # Initialize the perplexity calculator with a pre-trained model\n",
    "    perplexities = scorer.get_perplexity(\n",
    "        sub_strings\n",
    "    )  # Calculate perplexity for each submitted string\n",
    "\n",
    "    if clear_mem:\n",
    "        # Just move on if it fails. Not essential if we have the score.\n",
    "        try:\n",
    "            scorer.clear_gpu_memory()\n",
    "        except:\n",
    "            print('GPU memory clearing failed.')\n",
    "\n",
    "    return float(np.mean(perplexities))\n",
    "\n",
    "class PerplexityCalculator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: str,\n",
    "        load_in_8bit: bool = False,\n",
    "        device_map: str = 'auto',\n",
    "    ):\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)\n",
    "        # Configure model loading based on quantization setting and device availability\n",
    "        if load_in_8bit:\n",
    "            if DEVICE.type != 'cuda':\n",
    "                raise ValueError('8-bit quantization requires CUDA device')\n",
    "            quantization_config = transformers.BitsAndBytesConfig(load_in_8bit=True)\n",
    "            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                quantization_config=quantization_config,\n",
    "                device_map=device_map,\n",
    "            )\n",
    "        else:\n",
    "            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                torch_dtype=torch.float16 if DEVICE.type == 'cuda' else torch.float32,\n",
    "                device_map=device_map,\n",
    "            )\n",
    "\n",
    "        self.loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "    def get_perplexity(\n",
    "        self, input_texts: Union[str, List[str]], debug=False\n",
    "    ) -> Union[float, List[float]]:\n",
    "        single_input = isinstance(input_texts, str)\n",
    "        input_texts = [input_texts] if single_input else input_texts\n",
    "\n",
    "        loss_list = []\n",
    "        with torch.no_grad():\n",
    "            # Process each sequence independently\n",
    "            for text in input_texts:\n",
    "                # Explicitly add sequence boundary tokens to the text\n",
    "                text_with_special = f\"{self.tokenizer.bos_token}{text}{self.tokenizer.eos_token}\"\n",
    "\n",
    "                # Tokenize\n",
    "                model_inputs = self.tokenizer(\n",
    "                    text_with_special,\n",
    "                    return_tensors='pt',\n",
    "                    add_special_tokens=False,\n",
    "                )\n",
    "\n",
    "                if 'token_type_ids' in model_inputs:\n",
    "                    model_inputs.pop('token_type_ids')\n",
    "\n",
    "                model_inputs = {k: v.to(DEVICE) for k, v in model_inputs.items()}\n",
    "\n",
    "                # Get model output\n",
    "                output = self.model(**model_inputs, use_cache=False)\n",
    "                logits = output['logits']\n",
    "\n",
    "                # Shift logits and labels for calculating loss\n",
    "                shift_logits = logits[..., :-1, :].contiguous()  # Drop last prediction\n",
    "                shift_labels = model_inputs['input_ids'][..., 1:].contiguous()  # Drop first input\n",
    "\n",
    "                # Calculate token-wise loss\n",
    "                loss = self.loss_fct(\n",
    "                    shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                    shift_labels.view(-1)\n",
    "                )\n",
    "\n",
    "                # Calculate average loss\n",
    "                sequence_loss = loss.sum() / len(loss)\n",
    "                loss_list.append(sequence_loss.cpu().item())\n",
    "\n",
    "                # Debug output\n",
    "                if debug:\n",
    "                    print(f\"\\nProcessing: '{text}'\")\n",
    "                    print(f\"With special tokens: '{text_with_special}'\")\n",
    "                    print(f\"Input tokens: {model_inputs['input_ids'][0].tolist()}\")\n",
    "                    print(f\"Target tokens: {shift_labels[0].tolist()}\")\n",
    "                    print(f\"Input decoded: {self.tokenizer.decode(model_inputs['input_ids'][0])}\")\n",
    "                    print(f\"Target decoded: {self.tokenizer.decode(shift_labels[0])}\")\n",
    "                    print(f\"Individual losses: {loss.tolist()}\")\n",
    "                    print(f\"Average loss: {sequence_loss.item():.4f}\")\n",
    "\n",
    "        ppl = [exp(i) for i in loss_list]\n",
    "\n",
    "        if debug:\n",
    "            print(\"\\nFinal perplexities:\")\n",
    "            for text, perp in zip(input_texts, ppl):\n",
    "                print(f\"Text: '{text}'\")\n",
    "                print(f\"Perplexity: {perp:.2f}\")\n",
    "\n",
    "        return ppl[0] if single_input else ppl\n",
    "\n",
    "    def clear_gpu_memory(self) -> None:\n",
    "        if not torch.cuda.is_available():\n",
    "            return\n",
    "\n",
    "        # Delete model and tokenizer if they exist\n",
    "        if hasattr(self, 'model'):\n",
    "            del self.model\n",
    "        if hasattr(self, 'tokenizer'):\n",
    "            del self.tokenizer\n",
    "\n",
    "        # Run garbage collection\n",
    "        gc.collect()\n",
    "\n",
    "        # Clear CUDA cache and reset memory stats\n",
    "        with DEVICE:\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "            torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001c4b31",
   "metadata": {},
   "source": [
    "---\n",
    "# CACHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df9165e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T20:03:05.015910Z",
     "iopub.status.busy": "2024-12-31T20:03:05.015656Z",
     "iopub.status.idle": "2024-12-31T20:03:05.029331Z",
     "shell.execute_reply": "2024-12-31T20:03:05.028513Z",
     "shell.execute_reply.started": "2024-12-31T20:03:05.015885Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Caching:\n",
    "    def __init__(self, max_size: int = 1000):\n",
    "        self.stores: List[dict] = []\n",
    "        self.idx = -1\n",
    "        self.max_size = max_size\n",
    "        self._state = {'len': 0, 'hit': 0, 'miss': 0, 'total': 0}\n",
    "\n",
    "    def create_store(self):\n",
    "        self.stores.append({})\n",
    "        self.idx += 1\n",
    "        self._state['len'] += 1\n",
    "\n",
    "    def add(self, key, value):\n",
    "        if self.idx == -1 or len(self.stores[self.idx]) >= self.max_size:\n",
    "            self.create_store()\n",
    "        self.stores[self.idx][key] = value\n",
    "\n",
    "    def get(self, key):\n",
    "        if self.idx == -1:\n",
    "            return None\n",
    "        self._state['total'] += 1\n",
    "        store_idx = random.choices(range(self.idx+1), k=self.idx+1) \n",
    "        for i in store_idx:\n",
    "            v = self.stores[i].get(key, None)\n",
    "            if v is not None:\n",
    "                self._state['hit'] += 1\n",
    "                return v\n",
    "        self._state['miss'] += 1\n",
    "        return v\n",
    "\n",
    "    def state(self):\n",
    "        return f\"total: {self._state['total']}, len: {self._state['len']}, stores: {len(self.stores)} \"\\\n",
    "              f\"hit: {self._state['hit']} [{self._state['hit']/self._state['total']*100}%], \"\\\n",
    "              f\"miss: {self._state['miss']} [{self._state['miss']/self._state['total']*100}%]\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49681d0",
   "metadata": {
    "papermill": {
     "duration": 0.002792,
     "end_time": "2024-12-29T17:56:05.356078",
     "exception": false,
     "start_time": "2024-12-29T17:56:05.353286",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "# TabuSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "808ef37d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T20:03:05.030630Z",
     "iopub.status.busy": "2024-12-31T20:03:05.030295Z",
     "iopub.status.idle": "2024-12-31T20:03:05.045410Z",
     "shell.execute_reply": "2024-12-31T20:03:05.044731Z",
     "shell.execute_reply.started": "2024-12-31T20:03:05.030605Z"
    },
    "papermill": {
     "duration": 0.016426,
     "end_time": "2024-12-29T17:56:05.375373",
     "exception": false,
     "start_time": "2024-12-29T17:56:05.358947",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TabuSearch:\n",
    "    '''\n",
    "    Tabu Search Algorithm\n",
    "\n",
    "    This class implements the Tabu Search optimization algorithm, which is used to solve\n",
    "    combinatorial optimization problems. It iteratively improves solutions by exploring\n",
    "    neighboring candidates and using a tabu list to avoid revisiting recently explored solutions.\n",
    "\n",
    "    Parameters:\n",
    "        words (list[str]): The sequence of words to be optimized.\n",
    "        calculate_perplexity (callable): A method to calculate the perplexity of a sentence.\n",
    "        tabu_tenure (int, optional): The size of the tabu list. Default is 5.\n",
    "        stop_on_non_improvement (int, optional): The number of iterations to allow without improvement before stopping. Default is 20.\n",
    "        max_iter (int, optional): The maximum number of iterations to execute. Default is 100.\n",
    "        max_candidates (int, optional): The maximum number of candidate solutions to generate per iteration. Default is 100.\n",
    "        debug (bool, optional): If True, enables debug logging. Default is False.\n",
    "        logger (callable, optional): A logging function. If None, prints to the console. Default is None.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, words: list, calculate_perplexity, tabu_tenure=5,\n",
    "                 stop_on_non_improvement=20, max_iter=100, \n",
    "                 max_condidates=100,\n",
    "                 debug=False, logger = None):\n",
    "        '''\n",
    "        Initializes the TabuSearch instance.\n",
    "\n",
    "        Parameters:\n",
    "            words (list[str]): The sequence of words to be optimized.\n",
    "            calculate_perplexity (callable): A method to calculate the perplexity of a sentence.\n",
    "            tabu_tenure (int, optional): The size of the tabu list. Default is 5.\n",
    "            stop_on_non_improvement (int, optional): The number of iterations to allow without improvement. Default is 20.\n",
    "            max_iter (int, optional): The maximum number of iterations. Default is 100.\n",
    "            max_condidates (int, optional): The maximum number of candidate solutions per iteration. Default is 100.\n",
    "            debug (bool, optional): If True, enables debug logging. Default is False.\n",
    "            logger (callable, optional): A logging function. Default is None.\n",
    "        '''\n",
    "        self.words = words\n",
    "        self.n = len(self.words)\n",
    "        self.tabu_tenure = tabu_tenure\n",
    "        self.stop_on_non_improvement = stop_on_non_improvement\n",
    "        self.max_iter = max_iter\n",
    "        self.calculate_perplexity = calculate_perplexity\n",
    "        self.tabu_list = deque(maxlen=self.tabu_tenure)\n",
    "        self.max_condidates = max_condidates\n",
    "        self.debug = debug\n",
    "        self.logger = logger\n",
    "\n",
    "    def log(self, msg):\n",
    "        '''\n",
    "        Logs a message if debugging is enabled.\n",
    "\n",
    "        Parameters:\n",
    "            msg (str): The message to log.\n",
    "        '''\n",
    "        if self.debug:\n",
    "            if self.logger is None:\n",
    "                print(f\"[LOCAL SEARCH] {msg}\")\n",
    "            else:\n",
    "                self.logger(f\"[LOCAL SEARCH] {msg}\")\n",
    "    \n",
    "    def is_move_used(self, move: Tuple[int, int]):\n",
    "        '''\n",
    "        Checks if a move is present in the tabu list.\n",
    "\n",
    "        Parameters:\n",
    "            move (Tuple[int, int]): The move to check.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the move is in the tabu list, False otherwise.\n",
    "        '''\n",
    "        return move in self.tabu_list\n",
    "    \n",
    "    def __to_sentence_text(self, solution):\n",
    "        '''\n",
    "        Converts a solution to a sentence by mapping indices to words.\n",
    "\n",
    "        Parameters:\n",
    "            solution (list[int]): The solution as a list of word indices.\n",
    "\n",
    "        Returns:\n",
    "            str: The sentence formed by the solution.\n",
    "        '''\n",
    "        return \" \".join([self.words[i] for i in solution])\n",
    "    \n",
    "    def to_tabulist(self, move):\n",
    "        '''\n",
    "        Adds a move to the tabu list.\n",
    "\n",
    "        Parameters:\n",
    "            move (Tuple[int, int]): The move to add to the tabu list.\n",
    "        '''\n",
    "        self.tabu_list.append(tuple(move))\n",
    "\n",
    "    def get_candidates(self, solution: np.ndarray) -> List[Tuple[list, Tuple[int, int]]]:\n",
    "        '''\n",
    "        Generates candidate solutions by randomly swapping two elements.\n",
    "\n",
    "        Parameters:\n",
    "            solution (np.ndarray): The current solution as an array of indices.\n",
    "\n",
    "        Returns:\n",
    "            list[Tuple[list, Tuple[int, int]]]: A list of candidate solutions and their corresponding moves.\n",
    "        '''\n",
    "        candidates = []\n",
    "\n",
    "        while len(candidates) < self.max_condidates:\n",
    "            i,j = random.choices(solution, k=2)\n",
    "            candidate = solution.copy()\n",
    "            candidate[i], candidate[j] = candidate[j], candidate[i]\n",
    "            candidates.append((candidate, (i, j)))\n",
    "        \n",
    "        return candidates\n",
    "\n",
    "    def optimize(self, current_sentence: np.ndarray) -> Tuple[np.ndarray, float]:\n",
    "        '''\n",
    "        Executes the Tabu Search algorithm.\n",
    "\n",
    "        Parameters:\n",
    "            current_sentence (np.ndarray): The initial solution as an array of indices.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[np.ndarray, float]: The best solution and its corresponding perplexity.\n",
    "        '''\n",
    "        self.log(f\"start {current_sentence}\")\n",
    "        \n",
    "        # Calculate the perplexity of the initial solution\n",
    "        current_perplexity = self.calculate_perplexity(current_sentence)\n",
    "        \n",
    "        # Initialize the best solution and its perplexity\n",
    "        best_sentence = current_sentence\n",
    "        best_perplexity = current_perplexity\n",
    "        \n",
    "        # Track the number of iterations without improvement and the total iterations\n",
    "        no_improvement = 0\n",
    "        itr = 1\n",
    "        \n",
    "        # Iterate until the stopping condition is met\n",
    "        while no_improvement < self.stop_on_non_improvement and itr <= self.max_iter:\n",
    "            self.log(f\"[ITER] [{itr}] --- {best_perplexity}\")\n",
    "            \n",
    "            # Generate candidate solutions\n",
    "            candidates = self.get_candidates(current_sentence)\n",
    "            \n",
    "            # Initialize variables for the best candidate solution\n",
    "            best_candidate = None\n",
    "            best_candidate_perplexity = None\n",
    "            used_move = None\n",
    "            \n",
    "            # Evaluate each candidate solution\n",
    "            for candidate, swap in candidates:\n",
    "                candidate_perplexity = self.calculate_perplexity(candidate)\n",
    "                # Update the best candidate if it is better and satisfies tabu conditions\n",
    "                if best_candidate_perplexity is None \\\n",
    "                    or (((not self.is_move_used(swap)) or (candidate_perplexity < best_perplexity)) \\\n",
    "                        and (candidate_perplexity < best_candidate_perplexity)):\n",
    "                        best_candidate = candidate\n",
    "                        best_candidate_perplexity = candidate_perplexity\n",
    "                        used_move = swap\n",
    "            \n",
    "            self.log(f\"[{best_candidate_perplexity}] {best_candidate}\")\n",
    "            \n",
    "            # Check if the best candidate is an improvement\n",
    "            if best_candidate_perplexity < best_perplexity:\n",
    "                # Update the best solution and reset the no-improvement counter\n",
    "                best_sentence = best_candidate\n",
    "                best_perplexity = best_candidate_perplexity\n",
    "                no_improvement = 0\n",
    "                self.log(f\"[IM] [{itr}/{self.max_iter}] [{best_perplexity}]\")\n",
    "            else:\n",
    "                # Increment the no-improvement counter\n",
    "                no_improvement += 1\n",
    "                self.log(f\"[NO IM] __{no_improvement}__\")\n",
    "            \n",
    "            # consider the best candidate as the current solution\n",
    "            current_sentence = best_candidate\n",
    "            current_perplexity = best_candidate_perplexity\n",
    "            \n",
    "            # add the move to the tabu list\n",
    "            self.to_tabulist(used_move)\n",
    "            itr+=1\n",
    "\n",
    "            # Break if the no-improvement limit is reached\n",
    "            if(no_improvement == self.stop_on_non_improvement):\n",
    "                self.log(f\"[NO IM] break; {best_sentence} | {best_perplexity}\")\n",
    "                break\n",
    "        \n",
    "        # Return the best solution and its perplexity\n",
    "        return best_sentence, best_perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15651c7f",
   "metadata": {
    "papermill": {
     "duration": 0.002429,
     "end_time": "2024-12-29T17:56:05.390506",
     "exception": false,
     "start_time": "2024-12-29T17:56:05.388077",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "# WAHLE OPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d437fd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T20:03:09.670684Z",
     "iopub.status.busy": "2024-12-31T20:03:09.670364Z",
     "iopub.status.idle": "2024-12-31T20:03:09.692037Z",
     "shell.execute_reply": "2024-12-31T20:03:09.691194Z",
     "shell.execute_reply.started": "2024-12-31T20:03:09.670654Z"
    },
    "papermill": {
     "duration": 0.022337,
     "end_time": "2024-12-29T17:56:05.415462",
     "exception": false,
     "start_time": "2024-12-29T17:56:05.393125",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import permutations\n",
    "\n",
    "class WhaleOptimization:\n",
    "    \"\"\"\n",
    "    Implementation of the Whale Optimization Algorithm (WOA) for optimizing word order \n",
    "    in a sentence based on perplexity scoring.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sentence: str, scorer,\n",
    "                 n_whales: int = 20, max_iter: int = 80,\n",
    "                 tabu_tenure=10, tabu_no_imp=10, tabu_max_iter=50,\n",
    "                 cache: Caching = Caching(), logger=None, debug=False):\n",
    "        \"\"\"\n",
    "        Initialize the Whale Optimization Algorithm.\n",
    "\n",
    "        Args:\n",
    "            sentence (str): Input sentence to optimize.\n",
    "            scorer (object): Scorer with a `get_perplexity` method to evaluate sentence quality.\n",
    "            n_whales (int, optional): Number of whales in the population. Defaults to 20.\n",
    "            max_iter (int, optional): Maximum iterations for WOA. Defaults to 80.\n",
    "            tabu_tenure (int, optional): Size of tabu list in Tabu Search. Defaults to 10.\n",
    "            tabu_no_imp (int, optional): No-improvement iterations to stop Tabu Search. Defaults to 10.\n",
    "            tabu_max_iter (int, optional): Maximum iterations in Tabu Search. Defaults to 50.\n",
    "            logger (callable, optional): Logging function. Defaults to None.\n",
    "            debug (bool, optional): Enable debug messages. Defaults to False.\n",
    "        \"\"\"\n",
    "        self.words = sentence.split(' ')\n",
    "        self.n = len(self.words)\n",
    "        self.scorer = scorer\n",
    "        self.n_whales = n_whales\n",
    "        self.max_iter = max_iter\n",
    "        self.logger = logger\n",
    "        self.debug = debug\n",
    "        \n",
    "        self.cwi = None  # Index of the current whale\n",
    "        self.itr = ''  # Current iteration\n",
    "        \n",
    "        # Initialize Tabu Search for local improvements\n",
    "        self.TabuSearch = TabuSearch(\n",
    "            self.words, \n",
    "            self.__calculate_perplexity,\n",
    "            tabu_tenure=tabu_tenure,\n",
    "            stop_on_non_improvement=tabu_no_imp,\n",
    "            max_iter=tabu_max_iter,\n",
    "            debug=debug,\n",
    "            logger=self.log\n",
    "        )\n",
    "\n",
    "        # Cache for perplexity calculations\n",
    "        self.cache = cache\n",
    "    \n",
    "    def __create_initial_sols(self) -> np.ndarray:\n",
    "        \"\"\"Generate a random initial solution (word permutation).\"\"\"\n",
    "        return np.random.permutation(self.n)\n",
    "    \n",
    "    def log(self, msg):\n",
    "        \"\"\"\n",
    "        Log messages for debugging and tracing execution.\n",
    "\n",
    "        Args:\n",
    "            msg (str): The message to log.\n",
    "        \"\"\"\n",
    "        if self.debug:\n",
    "            if self.logger is None:\n",
    "                if self.cwi is None:\n",
    "                    print(f\"[WOA] [{self.itr}] {msg}\")\n",
    "                else:\n",
    "                    print(f\"[WOA] [{self.itr}] [WHALE {self.cwi + 1}] {msg}\")\n",
    "            else:\n",
    "                if self.cwi is None:\n",
    "                    self.logger(f\"[{self.itr}] {msg}\")\n",
    "                else:\n",
    "                    self.logger(f\"[{self.itr}] [WHALE {self.cwi + 1}] {msg}\")\n",
    "    \n",
    "    def _compute_A(self, a: float):\n",
    "        \"\"\"\n",
    "        Compute the coefficient A for WOA movement equations.\n",
    "\n",
    "        Args:\n",
    "            a (float): Shrinking parameter.\n",
    "\n",
    "        Returns:\n",
    "            float: Coefficient A.\n",
    "        \"\"\"\n",
    "        r = np.random.uniform(0.0, 1.0, size=1)\n",
    "        return (2.0*np.multiply(a, r)) - a\n",
    "\n",
    "    def _compute_C(self):\n",
    "        \"\"\"\n",
    "        Compute the coefficient C for WOA movement equations.\n",
    "\n",
    "        Returns:\n",
    "            float: Coefficient C.\n",
    "        \"\"\"\n",
    "        r = np.random.uniform(0.0, 1.0, size=1)\n",
    "        return 2.0 * r\n",
    "    \n",
    "    def __to_sentence_text(self, sol):\n",
    "        \"\"\"\n",
    "        Convert a solution (word order) into a readable sentence.\n",
    "\n",
    "        Args:\n",
    "            sol (np.ndarray): Solution as indices.\n",
    "\n",
    "        Returns:\n",
    "            str: Reconstructed sentence.\n",
    "        \"\"\"\n",
    "        return \" \".join([self.words[i] for i in sol])\n",
    "    \n",
    "    def __get_from_cache(self, solution: tuple) -> float:\n",
    "        return self.cache.get(solution)\n",
    "    \n",
    "    def __calculate_perplexity(self, solution: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Compute the perplexity of a given solution, with caching.\n",
    "\n",
    "        Args:\n",
    "            solution (np.ndarray): Solution as indices.\n",
    "\n",
    "        Returns:\n",
    "            float: Perplexity score of the solution.\n",
    "        \"\"\"\n",
    "        solt = tuple(solution)\n",
    "        cached = self.__get_from_cache(solt)\n",
    "        if cached is not None:\n",
    "            return cached\n",
    "        \n",
    "        # Convert solution to sentence and calculate perplexity\n",
    "        sentence = self.__to_sentence_text(solution)\n",
    "        submission = pd.DataFrame({'id': [0], 'text': [sentence] })\n",
    "        perplexities = scorer.get_perplexity(submission[\"text\"].tolist()) # TODO use self to access the scorer\n",
    "        self.cache.add(solt, perplexities[0])\n",
    "        return perplexities[0]\n",
    "\n",
    "    def __caching_state(self):\n",
    "        self.log(self.cache.state())\n",
    "    \n",
    "    def __encircling_prey(self, current_pos: np.ndarray, best_pos: np.ndarray, A: float, C: float) -> np.ndarray:\n",
    "        D = abs(C * best_pos - current_pos)\n",
    "        new_pos = best_pos - A * D\n",
    "        return np.argsort(new_pos)\n",
    "    \n",
    "    def __search_for_prey(self, current_pos: np.ndarray, random_pos: np.ndarray, A: float, C: float) -> np.ndarray:\n",
    "        D = abs(C * random_pos - current_pos)\n",
    "        new_pos = random_pos - A * D\n",
    "        return np.argsort(new_pos) \n",
    "    \n",
    "    def __bubble_net_attack(self, current_pos: np.ndarray, best_pos: np.ndarray) -> np.ndarray:\n",
    "        D = abs(best_pos - current_pos)\n",
    "        b = 1\n",
    "        l = random.uniform(-1, 1)\n",
    "        new_pos = D * np.exp(l * b) * np.cos(2 * np.pi * l) + best_pos\n",
    "        return np.argsort(new_pos)\n",
    "    \n",
    "    def __amend_position(self, position: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Ensure position is a valid permutation.\"\"\"\n",
    "        return np.argsort(position)\n",
    "   \n",
    "    def __local_search(self, solution: np.ndarray) -> Tuple[np.ndarray, float]:\n",
    "        return self.TabuSearch.optimize(solution)\n",
    "    \n",
    "    def optimize(self) -> Tuple[np.ndarray, float, List[float], List[list]]:\n",
    "        \"\"\"\n",
    "        Execute the Whale Optimization Algorithm to find the best word order.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[np.ndarray, float]: The best solution and its perplexity score.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize population of whales with random solutions\n",
    "        population = []\n",
    "        perplexities_values = []\n",
    "        for w in range(self.n_whales):\n",
    "            self.cwi = w\n",
    "            solution = self.__create_initial_sols()\n",
    "            self.log(f\"sol {solution} --> to improve\")\n",
    "            improved_solution, improved_perplexity = self.__local_search(solution)\n",
    "            self.log(f\"{improved_solution} | {improved_perplexity}\")\n",
    "            population.append(improved_solution)\n",
    "            perplexities_values.append(improved_perplexity)\n",
    "        self.cwi=None\n",
    "        # population = [np.array([8, 6, 9, 1, 2, 5, 3, 0, 4, 7]), np.array([8, 6, 2, 9, 5, 3, 7, 0, 1, 4]), np.array([8, 6, 2, 9, 1, 4, 5, 7, 3, 0])]\n",
    "        # perplexities_values = [531.0074131628102, 530.3983005966822, 561.4057235990515]\n",
    "\n",
    "        self.log(\"------------------------------\")\n",
    "        self.log(f\"[WHALES]\")\n",
    "        self.log(population)\n",
    "        self.log(perplexities_values)\n",
    "        self.log(\"------------------------------\") # output\n",
    "        \n",
    "        # Identify the initial best whale\n",
    "        best_idx = np.argmin(perplexities_values)\n",
    "        prey = population[best_idx].copy()\n",
    "        best_perplexity = perplexities_values[best_idx]\n",
    "        \n",
    "        self.log(f\"[PREY] : {prey} | {best_perplexity}\")\n",
    "        \n",
    "        # Start main WOA loop\n",
    "        self.itr = 1\n",
    "        while self.itr <= self.max_iter:\n",
    "            self.log(f\"[START ITER] {prey} | {best_perplexity}\")\n",
    "            \n",
    "            # Shrinking encircling mechanism parameter\n",
    "            a = 2 - self.itr / self.max_iter * 2\n",
    "            \n",
    "            # Update each whale in the population\n",
    "            for self.cwi in range(self.n_whales):\n",
    "                cwhale = population[self.cwi]\n",
    "                self.log(f\"{cwhale} | {perplexities_values[self.cwi]}\")\n",
    "                A = self._compute_A(a)\n",
    "                C = self._compute_C()\n",
    "                p = random.random()\n",
    "                self.log(f\"a={a}, A={A}, C={C}, p={p}\")\n",
    "\n",
    "                if p < 0.5: # TODO: check if this is important for the problem, or it can be removed\n",
    "                    if abs(A) < 1:\n",
    "                        # Encircling prey\n",
    "                        new_pos = self.__encircling_prey(cwhale, prey, A, C)\n",
    "                        self.log(f\"[MOVE] encircling prey --> {new_pos}\")\n",
    "                    else:\n",
    "                        # Searching for prey\n",
    "                        rand_idx = random.randint(0, self.n_whales-1)\n",
    "                        random_pos = population[rand_idx]\n",
    "                        new_pos = self.__search_for_prey(cwhale, random_pos, A, C)\n",
    "                        self.log(f\"[MOVE] search for prey --> {new_pos}\")\n",
    "                else:\n",
    "                    new_pos = self.__bubble_net_attack(cwhale, prey)\n",
    "                    self.log(f\"[MOVE] bubble net attack --> {new_pos}\")\n",
    "                \n",
    "                # new_pos = self.__amend_position(new_pos)\n",
    "                # self.log(f\"amend position --> {new_pos}\")\n",
    "                # Apply local search to improve the new position\n",
    "                improved_pos, improved_perplexity = self.__local_search(new_pos)\n",
    "                self.log(f\"[IMPROVE POSITION] {new_pos}, [{improved_perplexity}]\")\n",
    "                \n",
    "                # Evaluate the new solution\n",
    "                # if improved_perplexity < perplexities_values[self.cwi]: # TODO: check if this will be useful or improve the solution\n",
    "                population[self.cwi] = improved_pos\n",
    "                perplexities_values[self.cwi] = improved_perplexity\n",
    "                \n",
    "                # Update the best whale if a better solution is found\n",
    "                # if improved_perplexity < best_perplexity:\n",
    "                #     prey = improved_pos.copy()\n",
    "                #     best_perplexity = improved_perplexity\n",
    "                #     self.log(f\"[BEST] {prey}, [{best_perplexity}]\")\n",
    "            \n",
    "            # Update the prey if a better solution is found\n",
    "            best_idx = np.argmin(perplexities_values)\n",
    "            if perplexities_values[best_idx] < best_perplexity:\n",
    "                prey = population[best_idx]\n",
    "                best_perplexity = perplexities_values[best_idx]\n",
    "                self.log(f\"[RES] [IMP] Prey: {prey} with perplexity {best_perplexity}\")\n",
    "            else:\n",
    "                self.log(f\"[RES] [NO IMP] Prey: {prey} with perplexity {best_perplexity}\")\n",
    "            \n",
    "            self.cwi=None\n",
    "            self.itr += 1\n",
    "    \n",
    "        self.__caching_state()\n",
    "        return prey, best_perplexity, self.__to_sentence_text(prey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8a95137-50d9-4c85-85ea-4019ee66934a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T20:03:09.693604Z",
     "iopub.status.busy": "2024-12-31T20:03:09.693371Z",
     "iopub.status.idle": "2024-12-31T20:05:55.095517Z",
     "shell.execute_reply": "2024-12-31T20:05:55.094716Z",
     "shell.execute_reply.started": "2024-12-31T20:03:09.693584Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b4a50ae8e39479e84fd859df5068849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n",
    "scorer = PerplexityCalculator(model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbcf993-4ffa-43d6-96ce-af367778cec8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T20:05:55.097349Z",
     "iopub.status.busy": "2024-12-31T20:05:55.096967Z",
     "iopub.status.idle": "2024-12-31T20:06:33.892277Z",
     "shell.execute_reply": "2024-12-31T20:06:33.891407Z",
     "shell.execute_reply.started": "2024-12-31T20:05:55.097325Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "text = 'advent chimney elf family fireplace gingerbread mistletoe ornament reindeer scrooge'\n",
    "\n",
    "woa = WhaleOptimization(\n",
    "    text, score,\n",
    "    n_whales=3, max_iter=1, debug=True, \n",
    "    tabu_tenure=5, tabu_no_imp=1, tabu_max_iter=4\n",
    ")\n",
    "woa.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29349879",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T20:06:33.893834Z",
     "iopub.status.busy": "2024-12-31T20:06:33.893250Z",
     "iopub.status.idle": "2024-12-31T20:06:33.897450Z",
     "shell.execute_reply": "2024-12-31T20:06:33.896618Z",
     "shell.execute_reply.started": "2024-12-31T20:06:33.893808Z"
    },
    "papermill": {
     "duration": 34077.046284,
     "end_time": "2024-12-30T03:24:02.474536",
     "exception": false,
     "start_time": "2024-12-29T17:56:05.428252",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# problem = pd.read_csv(\"/kaggle/input/santa-2024/sample_submission.csv\")\n",
    "\n",
    "# res = []\n",
    "\n",
    "# for idx, row in problem.iterrows():\n",
    "#     print(\"----------------------------------------------\")\n",
    "#     print(\"----------------------------------------------\")\n",
    "#     print(\"----------------------------------------------\")\n",
    "#     def logger(msg):\n",
    "#         print(f\"[WOA {idx}] {msg}\")\n",
    "#     woa = WhaleOptimization(\n",
    "#         row.text, score,\n",
    "#         n_whales=3, max_iter=10, debug=True, \n",
    "#         tabu_tenure=10, tabu_no_imp=3, tabu_max_iter=20,\n",
    "#         logger = logger\n",
    "#     )\n",
    "#     sol, p, sen = woa.optimize()\n",
    "#     res.append({\n",
    "#         \"id\": len(res),\n",
    "#         \"or\": row.text,\n",
    "#         \"sol\": sol,\n",
    "#         \"text\": sen,\n",
    "#         \"p\": p\n",
    "#     })\n",
    "\n",
    "# submission_ = pd.DataFrame(res)\n",
    "# submission_"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 10229277,
     "sourceId": 88046,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 76277,
     "modelInstanceId": 72255,
     "sourceId": 104492,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 34250.814184,
   "end_time": "2024-12-30T03:24:06.176525",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-29T17:53:15.362341",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "059039d466be481ca0e6c1ebf32a7bf1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2760a09b7dcc427f9ae13881a5f248b6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "6c28f617367c48099870d3a8cba07425": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a239bc75c3754dcbb849992358aa7e2d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a66db6c6e6284dbba7e9c9e4a35e970f",
       "placeholder": "​",
       "style": "IPY_MODEL_e7f03ab786014ebebe4f44b0164ae4d2",
       "tabbable": null,
       "tooltip": null,
       "value": " 8/8 [02:40&lt;00:00, 18.13s/it]"
      }
     },
     "a66db6c6e6284dbba7e9c9e4a35e970f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ac6d51f04382460db0fda8ef99a6e87d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_059039d466be481ca0e6c1ebf32a7bf1",
       "placeholder": "​",
       "style": "IPY_MODEL_6c28f617367c48099870d3a8cba07425",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "b91eaa8982ae4bf7807d0d8a0157f1e6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_faafe3ae5e75441abf38c1e58a7014a9",
       "max": 8,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2760a09b7dcc427f9ae13881a5f248b6",
       "tabbable": null,
       "tooltip": null,
       "value": 8
      }
     },
     "c8b74db2c20749bca286d3e4b372658d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ac6d51f04382460db0fda8ef99a6e87d",
        "IPY_MODEL_b91eaa8982ae4bf7807d0d8a0157f1e6",
        "IPY_MODEL_a239bc75c3754dcbb849992358aa7e2d"
       ],
       "layout": "IPY_MODEL_e76eac02051044f585864916c471ff5c",
       "tabbable": null,
       "tooltip": null
      }
     },
     "e76eac02051044f585864916c471ff5c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e7f03ab786014ebebe4f44b0164ae4d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "faafe3ae5e75441abf38c1e58a7014a9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
