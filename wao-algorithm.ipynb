{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":88046,"databundleVersionId":10229277,"sourceType":"competition"},{"sourceId":104492,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":72255,"modelId":76277}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gc\nimport os\nfrom math import exp\nfrom collections import Counter\nfrom typing import List, Optional, Union\n\nimport numpy as np\nimport pandas as pd\nimport transformers\nimport torch\n\nos.environ['OMP_NUM_THREADS'] = '1'\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\nPAD_TOKEN_LABEL_ID = torch.nn.CrossEntropyLoss().ignore_index\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\nclass ParticipantVisibleError(Exception):\n    pass\n\n\ndef score(\n    solution: pd.DataFrame,\n    submission: pd.DataFrame,\n    row_id_column_name: str,\n    model_path: str = '/kaggle/input/gemma-2/transformers/gemma-2-9b/2',\n    load_in_8bit: bool = False,\n    clear_mem: bool = False,\n) -> float:\n    # Check that each submitted string is a permutation of the solution string\n    sol_counts = solution.loc[:, 'text'].str.split().apply(Counter)\n    sub_counts = submission.loc[:, 'text'].str.split().apply(Counter)\n    invalid_mask = sol_counts != sub_counts\n    if invalid_mask.any():\n        raise ParticipantVisibleError(\n            'At least one submitted string is not a valid permutation of the solution string.'\n        )\n\n    # Calculate perplexity for the submitted strings\n    sub_strings = [\n        ' '.join(s.split()) for s in submission['text'].tolist()\n    ]  # Split and rejoin to normalize whitespace\n    scorer = PerplexityCalculator(\n        model_path=model_path,\n        load_in_8bit=load_in_8bit,\n    )  # Initialize the perplexity calculator with a pre-trained model\n    perplexities = scorer.get_perplexity(\n        sub_strings\n    )  # Calculate perplexity for each submitted string\n\n    if clear_mem:\n        # Just move on if it fails. Not essential if we have the score.\n        try:\n            scorer.clear_gpu_memory()\n        except:\n            print('GPU memory clearing failed.')\n\n    return float(np.mean(perplexities))\n\n\nclass PerplexityCalculator:\n    def __init__(\n        self,\n        model_path: str,\n        load_in_8bit: bool = False,\n        device_map: str = 'auto',\n    ):\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)\n        # Configure model loading based on quantization setting and device availability\n        if load_in_8bit:\n            if DEVICE.type != 'cuda':\n                raise ValueError('8-bit quantization requires CUDA device')\n            quantization_config = transformers.BitsAndBytesConfig(load_in_8bit=True)\n            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n                model_path,\n                quantization_config=quantization_config,\n                device_map=device_map,\n            )\n        else:\n            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n                model_path,\n                torch_dtype=torch.float16 if DEVICE.type == 'cuda' else torch.float32,\n                device_map=device_map,\n            )\n\n        self.loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n\n        self.model.eval()\n\n    def get_perplexity(\n        self, input_texts: Union[str, List[str]], debug=False\n    ) -> Union[float, List[float]]:\n        single_input = isinstance(input_texts, str)\n        input_texts = [input_texts] if single_input else input_texts\n\n        loss_list = []\n        with torch.no_grad():\n            # Process each sequence independently\n            for text in input_texts:\n                # Explicitly add sequence boundary tokens to the text\n                text_with_special = f\"{self.tokenizer.bos_token}{text}{self.tokenizer.eos_token}\"\n\n                # Tokenize\n                model_inputs = self.tokenizer(\n                    text_with_special,\n                    return_tensors='pt',\n                    add_special_tokens=False,\n                )\n\n                if 'token_type_ids' in model_inputs:\n                    model_inputs.pop('token_type_ids')\n\n                model_inputs = {k: v.to(DEVICE) for k, v in model_inputs.items()}\n\n                # Get model output\n                output = self.model(**model_inputs, use_cache=False)\n                logits = output['logits']\n\n                # Shift logits and labels for calculating loss\n                shift_logits = logits[..., :-1, :].contiguous()  # Drop last prediction\n                shift_labels = model_inputs['input_ids'][..., 1:].contiguous()  # Drop first input\n\n                # Calculate token-wise loss\n                loss = self.loss_fct(\n                    shift_logits.view(-1, shift_logits.size(-1)),\n                    shift_labels.view(-1)\n                )\n\n                # Calculate average loss\n                sequence_loss = loss.sum() / len(loss)\n                loss_list.append(sequence_loss.cpu().item())\n\n                # Debug output\n                if debug:\n                    print(f\"\\nProcessing: '{text}'\")\n                    print(f\"With special tokens: '{text_with_special}'\")\n                    print(f\"Input tokens: {model_inputs['input_ids'][0].tolist()}\")\n                    print(f\"Target tokens: {shift_labels[0].tolist()}\")\n                    print(f\"Input decoded: {self.tokenizer.decode(model_inputs['input_ids'][0])}\")\n                    print(f\"Target decoded: {self.tokenizer.decode(shift_labels[0])}\")\n                    print(f\"Individual losses: {loss.tolist()}\")\n                    print(f\"Average loss: {sequence_loss.item():.4f}\")\n\n        ppl = [exp(i) for i in loss_list]\n\n        if debug:\n            print(\"\\nFinal perplexities:\")\n            for text, perp in zip(input_texts, ppl):\n                print(f\"Text: '{text}'\")\n                print(f\"Perplexity: {perp:.2f}\")\n\n        return ppl[0] if single_input else ppl\n\n    def clear_gpu_memory(self) -> None:\n        if not torch.cuda.is_available():\n            return\n\n        # Delete model and tokenizer if they exist\n        if hasattr(self, 'model'):\n            del self.model\n        if hasattr(self, 'tokenizer'):\n            del self.tokenizer\n\n        # Run garbage collection\n        gc.collect()\n\n        # Clear CUDA cache and reset memory stats\n        with DEVICE:\n            torch.cuda.empty_cache()\n            torch.cuda.ipc_collect()\n            torch.cuda.reset_peak_memory_stats()","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-12-19T18:33:13.474925Z","iopub.execute_input":"2024-12-19T18:33:13.475255Z","iopub.status.idle":"2024-12-19T18:33:17.058755Z","shell.execute_reply.started":"2024-12-19T18:33:13.475215Z","shell.execute_reply":"2024-12-19T18:33:17.058058Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nmodel_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\nscorer = PerplexityCalculator(model_path=model_path)","metadata":{"execution":{"iopub.status.busy":"2024-12-19T18:33:17.846636Z","iopub.execute_input":"2024-12-19T18:33:17.847035Z","iopub.status.idle":"2024-12-19T18:35:55.485446Z","shell.execute_reply.started":"2024-12-19T18:33:17.847012Z","shell.execute_reply":"2024-12-19T18:35:55.484491Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6372014eead94368a5ed6a9935a27206"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"submission = pd.read_csv(\"/kaggle/input/santa-2024/sample_submission.csv\")\n#perplexities = scorer.get_perplexity(submission[\"text\"].tolist())\n#perplexities","metadata":{"execution":{"iopub.status.busy":"2024-12-19T18:35:55.486578Z","iopub.execute_input":"2024-12-19T18:35:55.487076Z","iopub.status.idle":"2024-12-19T18:35:55.507032Z","shell.execute_reply.started":"2024-12-19T18:35:55.487044Z","shell.execute_reply":"2024-12-19T18:35:55.506137Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import numpy as np\nfrom collections import deque\nfrom typing import Tuple, List\n\nclass TabuSearch:\n    def __init__(self, tabu_tenure=5, stop_on_non_improvement=20, max_ter=100):\n        self.tabu_tenure = tabu_tenure\n        self.stop_on_non_improvement = stop_on_non_improvement\n        self.max_ter = max_ter\n        # self.calculate_cost = calculate_cost\n        self.tabu_list = deque(maxlen=self.tabu_tenure)\n    \n    def calculate_perplexity(self, sentence) -> float:\n        sentence = \" \".join(sentence)\n        submission = pd.DataFrame({'id': [0], 'text': [sentence] })\n        perplexities = scorer.get_perplexity(submission[\"text\"].tolist())\n        # print(f\"{perplexities[0]} || {sentence}\")\n        return perplexities[0]\n    \n    def is_sentence_visited(self, sentence):\n        return self.to_text(sentence) in self.tabu_list\n    \n    def to_text(self, sentence):\n        return \" \".join(sentence) \n\n    def get_candidates(self, solution):\n        n = len(solution)\n        neighbors = []\n\n        for i in range(n):\n            for j in range(i + 1, n):\n                neighbor = solution.copy()\n                neighbor[i], neighbor[j] = neighbor[j], neighbor[i]\n                neighbors.append(neighbor)\n\n        return neighbors\n\n    def optimize(self, current_sentence) -> Tuple[str, float]:\n        current_perplexity = self.calculate_perplexity(current_sentence)\n\n        best_sentence = current_sentence\n        best_perplexity = current_perplexity\n\n        no_improvement = 0\n        itr = 0\n        \n        while True and itr < self.max_ter:\n            itr+=1\n            \n            candidates = self.get_candidates(current_sentence)\n\n            best_candidate = None\n            best_candidate_perplexity = None\n\n            for candidate in candidates:\n                candidate_perplexity = self.calculate_perplexity(candidate)\n                if self.is_sentence_visited(candidate) or best_candidate_perplexity is None or candidate_perplexity < best_candidate_perplexity:\n                        best_candidate = candidate\n                        best_candidate_perplexity = candidate_perplexity\n            print(f\"[{best_candidate_perplexity}] {self.to_text(best_candidate)}\")\n\n            if best_candidate_perplexity < best_perplexity:\n                best_sentence = best_candidate\n                best_perplexity = best_candidate_perplexity\n                no_improvement = 0\n                print(f\"[IM] [{best_perplexity}]\")\n            else:\n                no_improvement += 1\n                print(f\"[NO IM] __{no_improvement}__\")\n\n            if no_improvement == self.stop_on_non_improvement:\n                break\n\n            current_sentence = best_candidate\n            current_perplexity = best_candidate_perplexity\n\n            self.tabu_list.append(best_sentence)\n\n        return self.to_text(best_sentence), best_perplexity\n","metadata":{"execution":{"iopub.status.busy":"2024-12-19T18:53:51.477743Z","iopub.execute_input":"2024-12-19T18:53:51.478034Z","iopub.status.idle":"2024-12-19T18:53:51.487051Z","shell.execute_reply.started":"2024-12-19T18:53:51.478010Z","shell.execute_reply":"2024-12-19T18:53:51.486151Z"},"trusted":true},"outputs":[],"execution_count":25},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/santa-2024/sample_submission.csv\").sample(1)\n\nts = TabuSearch(tabu_tenure=20, stop_on_non_improvement=20)\n\nsentence = [word for sublist in data['text'].str.split(' ') for word in sublist]\nts.optimize(sentence)","metadata":{"execution":{"iopub.status.busy":"2024-12-19T18:53:56.568491Z","iopub.execute_input":"2024-12-19T18:53:56.568775Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[304.5799402664961] peppermint candle poinsettia snowglobe hohoho eggnog fruitcake chocolate candy puzzle game doll toy workshop wonder believe dream hope peace joy merry season greeting card wrapping paper bow fireplace night cookie milk star wish wreath angel the to of and in that have it not with as you from we kaggle\n[IM] [304.5799402664961]\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"----","metadata":{}},{"cell_type":"code","source":"class WhaleOptimizationQAP:\n    def __init__(self, words, n_whales: int = 20, max_iter: int = 80, tabu_tenure=10, stop_on_non_improvement=100) -> None:\n        self.words = words\n        self.n = len(words)\n        self.n_whales = n_whales\n        self.max_iter = max_iter\n        self.cache = {}\n        self.fromCache = 0\n        self.attempt = 0\n        self.updated = 0\n        self.calculted = 0\n        self.TabuSearch = TabuSearch(flow_matrix, distance_matrix, self.__calculate_fitness, tabu_tenure=tabu_tenure, stop_on_non_improvement=stop_on_non_improvement)\n    \n    def _compute_A(self, a: float):\n        r = np.random.uniform(0.0, 1.0, size=1)\n        return (2.0*np.multiply(a, r))- a\n\n    def _compute_C(self):\n        return 2.0 * np.random.uniform(0.0, 1.0, size=1)\n        \n    def __calculate_fitness(self, sentence: np.ndarray) -> float:\n        submission = pd.DataFrame({\n         'id': [0],\n         'text': [\" \".join(sentence)]\n        })\n        perplexities = scorer.get_perplexity(submission[\"text\"].tolist())\n        return perplexities[0]\n\n    def __create_initial_sols(self) -> np.ndarray:\n        \"\"\"Create a random permutation solution\"\"\"\n        return np.random.shufle(self.n)\n    \n    def __encircling_prey(self, current_pos: np.ndarray, best_pos: np.ndarray, A: float, C: float) -> np.ndarray:\n        D = abs(C * best_pos - current_pos)\n        new_pos = best_pos - A * D\n        return np.argsort(new_pos)\n    \n    def __search_for_prey(self, current_pos: np.ndarray, random_pos: np.ndarray, A: float, C: float) -> np.ndarray:\n        D = abs(C * random_pos - current_pos)\n        new_pos = random_pos - A * D\n        return np.argsort(new_pos) \n    \n    def __bubble_net_attack(self, current_pos: np.ndarray, best_pos: np.ndarray, l: float) -> np.ndarray:\n        D = abs(best_pos - current_pos)\n        b = 1\n        new_pos = D * np.exp(l * b) * np.cos(2 * np.pi * l) + best_pos\n        return np.argsort(new_pos)\n    \n    def __amend_position(self, position: np.ndarray) -> np.ndarray:\n        \"\"\"Ensure position is a valid permutation.\"\"\"\n        return np.argsort(position)\n   \n    def __local_search(self, solution: np.ndarray) -> Tuple[np.ndarray, float]:\n        \"\"\"2-opt local search improvement.\"\"\"\n        return self.TabuSearch.optimize(solution)\n    \n    def optimize_with_local_search(self) -> Tuple[np.ndarray, float, List[float], List[list]]:\n        # Initialize population with local search improvement\n        population = []\n        fitness_values = []\n        for _ in range(self.n_whales):\n            solution = self.__create_initial_sols()\n            improved_solution, improved_fitness = self.__local_search(solution)\n            population.append(improved_solution)\n            fitness_values.append(improved_fitness)\n            \n        best_idx = np.argmin(fitness_values)\n        best_pos = population[best_idx].copy()\n        best_fitness = fitness_values[best_idx]\n        \n        t = 0\n        while t < self.max_iter:\n            for i in range(self.n_whales):\n                a = 2 - t * (2 / self.max_iter)\n                r = random.random()\n                A = self._compute_A(a)  \n                C = self._compute_C() \n                l = random.uniform(-1, 1)\n                p = random.random()\n            \n                if p < 0.5:\n                    if abs(A) < 1:\n                        new_pos = self.__encircling_prey(population[i], best_pos, A, C)\n                    else:\n                        rand_idx = random.randint(0, self.n_whales-1)\n                        random_pos = population[rand_idx]\n                        new_pos = self.__search_for_prey(population[i], random_pos, A, C)\n                else:\n                    new_pos = self.__bubble_net_attack(population[i], best_pos, l)\n                \n                new_pos = self.__amend_position(new_pos)\n                \n                # Apply local search to improve the new position\n                improved_pos, improved_fitness = self.__local_search(new_pos)\n                population[i] = improved_pos\n                fitness_values[i] = improved_fitness\n                \n                if improved_fitness < best_fitness:\n                    best_pos = improved_pos.copy()\n                    best_fitness = improved_fitness\n            t += 1\n        \n        return best_pos, best_fitness","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}