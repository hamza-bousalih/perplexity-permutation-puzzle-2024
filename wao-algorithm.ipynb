{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T10:25:09.120127Z",
     "iopub.status.busy": "2024-12-28T10:25:09.119855Z",
     "iopub.status.idle": "2024-12-28T10:25:12.946216Z",
     "shell.execute_reply": "2024-12-28T10:25:12.945287Z",
     "shell.execute_reply.started": "2024-12-28T10:25:09.120106Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "from math import exp\n",
    "from collections import Counter\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "from collections import deque\n",
    "from typing import Tuple, List\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-28T10:25:12.948075Z",
     "iopub.status.busy": "2024-12-28T10:25:12.947584Z",
     "iopub.status.idle": "2024-12-28T10:25:13.042568Z",
     "shell.execute_reply": "2024-12-28T10:25:13.041666Z",
     "shell.execute_reply.started": "2024-12-28T10:25:12.948042Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "PAD_TOKEN_LABEL_ID = torch.nn.CrossEntropyLoss().ignore_index\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "def score(\n",
    "    solution: pd.DataFrame,\n",
    "    submission: pd.DataFrame,\n",
    "    row_id_column_name: str,\n",
    "    model_path: str = '/kaggle/input/gemma-2/transformers/gemma-2-9b/2',\n",
    "    load_in_8bit: bool = False,\n",
    "    clear_mem: bool = False,\n",
    ") -> float:\n",
    "    # Check that each submitted string is a permutation of the solution string\n",
    "    sol_counts = solution.loc[:, 'text'].str.split().apply(Counter)\n",
    "    sub_counts = submission.loc[:, 'text'].str.split().apply(Counter)\n",
    "    invalid_mask = sol_counts != sub_counts\n",
    "    if invalid_mask.any():\n",
    "        raise ParticipantVisibleError(\n",
    "            'At least one submitted string is not a valid permutation of the solution string.'\n",
    "        )\n",
    "\n",
    "    # Calculate perplexity for the submitted strings\n",
    "    sub_strings = [\n",
    "        ' '.join(s.split()) for s in submission['text'].tolist()\n",
    "    ]  # Split and rejoin to normalize whitespace\n",
    "    scorer = PerplexityCalculator(\n",
    "        model_path=model_path,\n",
    "        load_in_8bit=load_in_8bit,\n",
    "    )  # Initialize the perplexity calculator with a pre-trained model\n",
    "    perplexities = scorer.get_perplexity(\n",
    "        sub_strings\n",
    "    )  # Calculate perplexity for each submitted string\n",
    "\n",
    "    if clear_mem:\n",
    "        # Just move on if it fails. Not essential if we have the score.\n",
    "        try:\n",
    "            scorer.clear_gpu_memory()\n",
    "        except:\n",
    "            print('GPU memory clearing failed.')\n",
    "\n",
    "    return float(np.mean(perplexities))\n",
    "\n",
    "class PerplexityCalculator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: str,\n",
    "        load_in_8bit: bool = False,\n",
    "        device_map: str = 'auto',\n",
    "    ):\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)\n",
    "        # Configure model loading based on quantization setting and device availability\n",
    "        if load_in_8bit:\n",
    "            if DEVICE.type != 'cuda':\n",
    "                raise ValueError('8-bit quantization requires CUDA device')\n",
    "            quantization_config = transformers.BitsAndBytesConfig(load_in_8bit=True)\n",
    "            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                quantization_config=quantization_config,\n",
    "                device_map=device_map,\n",
    "            )\n",
    "        else:\n",
    "            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                torch_dtype=torch.float16 if DEVICE.type == 'cuda' else torch.float32,\n",
    "                device_map=device_map,\n",
    "            )\n",
    "\n",
    "        self.loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "    def get_perplexity(\n",
    "        self, input_texts: Union[str, List[str]], debug=False\n",
    "    ) -> Union[float, List[float]]:\n",
    "        single_input = isinstance(input_texts, str)\n",
    "        input_texts = [input_texts] if single_input else input_texts\n",
    "\n",
    "        loss_list = []\n",
    "        with torch.no_grad():\n",
    "            # Process each sequence independently\n",
    "            for text in input_texts:\n",
    "                # Explicitly add sequence boundary tokens to the text\n",
    "                text_with_special = f\"{self.tokenizer.bos_token}{text}{self.tokenizer.eos_token}\"\n",
    "\n",
    "                # Tokenize\n",
    "                model_inputs = self.tokenizer(\n",
    "                    text_with_special,\n",
    "                    return_tensors='pt',\n",
    "                    add_special_tokens=False,\n",
    "                )\n",
    "\n",
    "                if 'token_type_ids' in model_inputs:\n",
    "                    model_inputs.pop('token_type_ids')\n",
    "\n",
    "                model_inputs = {k: v.to(DEVICE) for k, v in model_inputs.items()}\n",
    "\n",
    "                # Get model output\n",
    "                output = self.model(**model_inputs, use_cache=False)\n",
    "                logits = output['logits']\n",
    "\n",
    "                # Shift logits and labels for calculating loss\n",
    "                shift_logits = logits[..., :-1, :].contiguous()  # Drop last prediction\n",
    "                shift_labels = model_inputs['input_ids'][..., 1:].contiguous()  # Drop first input\n",
    "\n",
    "                # Calculate token-wise loss\n",
    "                loss = self.loss_fct(\n",
    "                    shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                    shift_labels.view(-1)\n",
    "                )\n",
    "\n",
    "                # Calculate average loss\n",
    "                sequence_loss = loss.sum() / len(loss)\n",
    "                loss_list.append(sequence_loss.cpu().item())\n",
    "\n",
    "                # Debug output\n",
    "                if debug:\n",
    "                    print(f\"\\nProcessing: '{text}'\")\n",
    "                    print(f\"With special tokens: '{text_with_special}'\")\n",
    "                    print(f\"Input tokens: {model_inputs['input_ids'][0].tolist()}\")\n",
    "                    print(f\"Target tokens: {shift_labels[0].tolist()}\")\n",
    "                    print(f\"Input decoded: {self.tokenizer.decode(model_inputs['input_ids'][0])}\")\n",
    "                    print(f\"Target decoded: {self.tokenizer.decode(shift_labels[0])}\")\n",
    "                    print(f\"Individual losses: {loss.tolist()}\")\n",
    "                    print(f\"Average loss: {sequence_loss.item():.4f}\")\n",
    "\n",
    "        ppl = [exp(i) for i in loss_list]\n",
    "\n",
    "        if debug:\n",
    "            print(\"\\nFinal perplexities:\")\n",
    "            for text, perp in zip(input_texts, ppl):\n",
    "                print(f\"Text: '{text}'\")\n",
    "                print(f\"Perplexity: {perp:.2f}\")\n",
    "\n",
    "        return ppl[0] if single_input else ppl\n",
    "\n",
    "    def clear_gpu_memory(self) -> None:\n",
    "        if not torch.cuda.is_available():\n",
    "            return\n",
    "\n",
    "        # Delete model and tokenizer if they exist\n",
    "        if hasattr(self, 'model'):\n",
    "            del self.model\n",
    "        if hasattr(self, 'tokenizer'):\n",
    "            del self.tokenizer\n",
    "\n",
    "        # Run garbage collection\n",
    "        gc.collect()\n",
    "\n",
    "        # Clear CUDA cache and reset memory stats\n",
    "        with DEVICE:\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "            torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T10:30:08.364168Z",
     "iopub.status.busy": "2024-12-28T10:30:08.363833Z",
     "iopub.status.idle": "2024-12-28T10:32:44.447440Z",
     "shell.execute_reply": "2024-12-28T10:32:44.446752Z",
     "shell.execute_reply.started": "2024-12-28T10:30:08.364145Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a05ec5f9def40eba47e58a4bb658a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n",
    "scorer = PerplexityCalculator(model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T10:32:44.449063Z",
     "iopub.status.busy": "2024-12-28T10:32:44.448585Z",
     "iopub.status.idle": "2024-12-28T10:32:44.478979Z",
     "shell.execute_reply": "2024-12-28T10:32:44.478350Z",
     "shell.execute_reply.started": "2024-12-28T10:32:44.449021Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>advent chimney elf family fireplace gingerbrea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>advent chimney elf family fireplace gingerbrea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>yuletide decorations gifts cheer holiday carol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>yuletide decorations gifts cheer holiday carol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>hohoho candle poinsettia snowglobe peppermint ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>advent chimney elf family fireplace gingerbrea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text\n",
       "0   0  advent chimney elf family fireplace gingerbrea...\n",
       "1   1  advent chimney elf family fireplace gingerbrea...\n",
       "2   2  yuletide decorations gifts cheer holiday carol...\n",
       "3   3  yuletide decorations gifts cheer holiday carol...\n",
       "4   4  hohoho candle poinsettia snowglobe peppermint ...\n",
       "5   5  advent chimney elf family fireplace gingerbrea..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv(\"/kaggle/input/santa-2024/sample_submission.csv\")\n",
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# TabuSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T10:37:15.616908Z",
     "iopub.status.busy": "2024-12-28T10:37:15.616615Z",
     "iopub.status.idle": "2024-12-28T10:37:15.626952Z",
     "shell.execute_reply": "2024-12-28T10:37:15.625900Z",
     "shell.execute_reply.started": "2024-12-28T10:37:15.616886Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TabuSearch:\n",
    "    def __init__(self, words: list, calculate_perplexity, tabu_tenure=5,\n",
    "                 stop_on_non_improvement=20, max_iter=100, \n",
    "                 max_condidates=100,\n",
    "                 debug=False, logger = None):\n",
    "        self.words = words\n",
    "        self.n = len(self.words)\n",
    "        self.tabu_tenure = tabu_tenure\n",
    "        self.stop_on_non_improvement = stop_on_non_improvement\n",
    "        self.max_iter = max_iter\n",
    "        self.calculate_perplexity = calculate_perplexity\n",
    "        self.tabu_list = deque(maxlen=self.tabu_tenure)\n",
    "        self.max_condidates = max_condidates\n",
    "        self.debug = debug\n",
    "        self.logger = logger\n",
    "\n",
    "    def log(self, msg):\n",
    "        if self.debug:\n",
    "            if self.logger is None:\n",
    "                print(f\"[LOCAL SEARCH] {msg}\")\n",
    "            else:\n",
    "                self.logger(f\"[LOCAL SEARCH] {msg}\")\n",
    "    \n",
    "    def is_move_used(self, move: Tuple[int, int]):\n",
    "        return move in self.tabu_list\n",
    "    \n",
    "    def to_sentence_text(self, sentence):\n",
    "        return \" \".join([self.words[i] for i in sentence])\n",
    "    \n",
    "    def to_tabulist(self, move):\n",
    "        self.tabu_list.append(tuple(move))\n",
    "\n",
    "    def get_candidates(self, solution: np.ndarray) -> List[Tuple[list, Tuple[int, int]]]:\n",
    "        candidates = []\n",
    "\n",
    "        itr = 0\n",
    "        while itr < self.max_condidates:\n",
    "            i,j = random.choices(solution, k=2)\n",
    "            candidate = solution.copy()\n",
    "            candidate[i], candidate[j] = candidate[j], candidate[i]\n",
    "            candidates.append((candidate, (i, j)))\n",
    "            itr+=1\n",
    "        \n",
    "        return candidates\n",
    "\n",
    "    def optimize(self, current_sentence: np.ndarray) -> Tuple[np.ndarray, float]:\n",
    "        self.log(f\"start {current_sentence}\")\n",
    "        current_perplexity = self.calculate_perplexity(current_sentence)\n",
    "        \n",
    "        best_sentence = current_sentence\n",
    "        best_perplexity = current_perplexity\n",
    "        \n",
    "        no_improvement = 0\n",
    "        itr = 1\n",
    "        \n",
    "        while no_improvement < self.stop_on_non_improvement and itr <= self.max_iter:\n",
    "            self.log(f\"[ITER] [{itr}] --- {best_perplexity}\")\n",
    "            \n",
    "            candidates = self.get_candidates(current_sentence)\n",
    "            \n",
    "            best_candidate = None\n",
    "            best_candidate_perplexity = None\n",
    "            used_move = None\n",
    "            \n",
    "            for candidate, swap in candidates:\n",
    "                candidate_perplexity = self.calculate_perplexity(candidate)\n",
    "\n",
    "                if best_candidate_perplexity is None \\\n",
    "                    or ((not self.is_move_used(swap)) or (candidate_perplexity < best_perplexity)) \\\n",
    "                        and (candidate_perplexity < best_candidate_perplexity):\n",
    "                        best_candidate = candidate\n",
    "                        best_candidate_perplexity = candidate_perplexity\n",
    "                        used_move = swap\n",
    "            \n",
    "            self.log(f\"[{best_candidate_perplexity}] {best_candidate}\")\n",
    "            \n",
    "            if best_candidate_perplexity < best_perplexity:\n",
    "                best_sentence = best_candidate\n",
    "                best_perplexity = best_candidate_perplexity\n",
    "                no_improvement = 0\n",
    "                self.log(f\"[IM] [{itr}/{self.max_iter}] [{best_perplexity}]\")\n",
    "            else:\n",
    "                no_improvement += 1\n",
    "                self.log(f\"[NO IM] __{no_improvement}__\")\n",
    "            \n",
    "            current_sentence = best_candidate\n",
    "            current_perplexity = best_candidate_perplexity\n",
    "            \n",
    "            self.to_tabulist(used_move)\n",
    "            itr+=1\n",
    "\n",
    "            if(no_improvement == self.stop_on_non_improvement):\n",
    "                self.log(f\"[NO IM] break; {best_sentence} | {best_perplexity}\")\n",
    "            \n",
    "        return best_sentence, best_perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T10:37:15.994569Z",
     "iopub.status.busy": "2024-12-28T10:37:15.994269Z",
     "iopub.status.idle": "2024-12-28T10:44:32.030345Z",
     "shell.execute_reply": "2024-12-28T10:44:32.029425Z",
     "shell.execute_reply.started": "2024-12-28T10:37:15.994529Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1, 4, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# words = submission['text'][0].split()\n",
    "# print(words)\n",
    "\n",
    "# init = np.random.permutation(len(words)).tolist()\n",
    "# print(init)\n",
    "\n",
    "# def __calculate_fitness(solution) -> float:\n",
    "#     sentence = \" \".join([words[i] for i in solution])\n",
    "#     submission = pd.DataFrame({'id': [0], 'text': [sentence] })\n",
    "#     perplexities = scorer.get_perplexity(submission[\"text\"].tolist())\n",
    "#     # print(f\"{perplexities[0]} || {sentence}\")\n",
    "#     return perplexities[0]\n",
    "\n",
    "# tabuSearch = TabuSearch(words, __calculate_fitness, tabu_tenure=10, stop_on_non_improvement=50, debug=True)\n",
    "\n",
    "# tabuSearch.optimize(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T10:51:02.175798Z",
     "iopub.status.busy": "2024-12-28T10:51:02.175438Z",
     "iopub.status.idle": "2024-12-28T10:51:02.192122Z",
     "shell.execute_reply": "2024-12-28T10:51:02.191126Z",
     "shell.execute_reply.started": "2024-12-28T10:51:02.175771Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tuple' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m permutations\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mWhaleOptimization\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mn_whales\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtabu_tenure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtabu_no_imp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtabu_max_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msentence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 75\u001b[0m, in \u001b[0;36mWhaleOptimization\u001b[1;34m()\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Ensure position is a valid permutation.\"\"\"\u001b[39;00m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39margsort(position)\n\u001b[1;32m---> 75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__local_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, solution: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mTuple\u001b[49m[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTabuSearch\u001b[38;5;241m.\u001b[39moptimize(solution)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mfloat\u001b[39m, List[\u001b[38;5;28mfloat\u001b[39m], List[\u001b[38;5;28mlist\u001b[39m]]:\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;66;03m# Initialize population with local search improvement\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Tuple' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from itertools import permutations\n",
    "\n",
    "class WhaleOptimization:\n",
    "    def __init__(self, sentence: str, scorer,\n",
    "                 n_whales: int = 20, max_iter: int = 80,\n",
    "                 tabu_tenure=10, tabu_no_imp=10, tabu_max_iter=50,\n",
    "                 debug=False):\n",
    "        self.words = sentence.split(' ')\n",
    "        self.n = len(self.words)\n",
    "        self.scorer = scorer\n",
    "        self.n_whales = n_whales\n",
    "        self.max_iter = max_iter\n",
    "        self.debug = debug\n",
    "        self.cur_whale = None\n",
    "        self.TabuSearch = TabuSearch(\n",
    "            self.words, \n",
    "            self.__calculate_fitness,\n",
    "            tabu_tenure=tabu_tenure,\n",
    "            stop_on_non_improvement=tabu_no_imp,\n",
    "            max_iter=tabu_max_iter,\n",
    "            debug=debug,\n",
    "            logger=self.log\n",
    "        )\n",
    "    \n",
    "    def __create_initial_sols(self) -> np.ndarray:\n",
    "        \"\"\"Create a random permutation solution\"\"\"\n",
    "        return np.random.permutation(self.n)\n",
    "    \n",
    "    def log(self, msg):\n",
    "        if self.debug:\n",
    "            if self.cur_whale is None:\n",
    "                print(f\"[WOA] {msg}\")\n",
    "            else:\n",
    "                print(f\"[WOA] [WHALE {self.cur_whale + 1}] {msg}\")\n",
    "    \n",
    "    def _compute_A(self, a: float):\n",
    "        r = np.random.uniform(0.0, 1.0, size=1)\n",
    "        return (2.0*np.multiply(a, r)) - a\n",
    "\n",
    "    def _compute_C(self):\n",
    "        return 2.0 * np.random.uniform(0.0, 1.0, size=1)\n",
    "    \n",
    "    def __to_sentence_text(self, sol):\n",
    "        return \" \".join([self.words[i] for i in sol])\n",
    "    \n",
    "    def __calculate_fitness(self, solution: np.ndarray) -> float:\n",
    "        sentence = self.__to_sentence_text(solution)\n",
    "        submission = pd.DataFrame({'id': [0], 'text': [sentence] })\n",
    "        perplexities = scorer.get_perplexity(submission[\"text\"].tolist()) # TODO use self to access the scorer\n",
    "        # print(f\"{perplexities[0]} || {sentence}\")\n",
    "        return perplexities[0]\n",
    "    \n",
    "    def __encircling_prey(self, current_pos: np.ndarray, best_pos: np.ndarray, A: float, C: float) -> np.ndarray:\n",
    "        D = abs(C * best_pos - current_pos)\n",
    "        new_pos = best_pos - A * D\n",
    "        return np.argsort(new_pos)\n",
    "    \n",
    "    def __search_for_prey(self, current_pos: np.ndarray, random_pos: np.ndarray, A: float, C: float) -> np.ndarray:\n",
    "        D = abs(C * random_pos - current_pos)\n",
    "        new_pos = random_pos - A * D\n",
    "        return np.argsort(new_pos) \n",
    "    \n",
    "    def __bubble_net_attack(self, current_pos: np.ndarray, best_pos: np.ndarray) -> np.ndarray:\n",
    "        D = abs(best_pos - current_pos)\n",
    "        b = 1\n",
    "        l = random.uniform(-1, 1)\n",
    "        new_pos = D * np.exp(l * b) * np.cos(2 * np.pi * l) + best_pos\n",
    "        return np.argsort(new_pos)\n",
    "    \n",
    "    def __amend_position(self, position: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Ensure position is a valid permutation.\"\"\"\n",
    "        return np.argsort(position)\n",
    "   \n",
    "    def __local_search(self, solution: np.ndarray) -> Tuple[np.ndarray, float]:\n",
    "        return self.TabuSearch.optimize(solution)\n",
    "    \n",
    "    def optimize(self) -> Tuple[np.ndarray, float, List[float], List[list]]:\n",
    "        # Initialize population with local search improvement\n",
    "        population = []\n",
    "        fitness_values = []\n",
    "        # for w in range(self.n_whales):\n",
    "        #     self.cur_whale = w\n",
    "        #     solution = self.__create_initial_sols()\n",
    "        #     self.log(f\"sol {solution} --> to improve\")\n",
    "        #     improved_solution, improved_fitness = self.__local_search(solution)\n",
    "        #     self.log(f\"{improved_solution} | {improved_fitness}\")\n",
    "        #     population.append(improved_solution)\n",
    "        #     fitness_values.append(improved_fitness)\n",
    "        # self.cur_whale=None\n",
    "        population = [np.array([8, 6, 9, 1, 2, 5, 3, 0, 4, 7]), np.array([8, 6, 2, 9, 5, 3, 7, 0, 1, 4]), np.array([8, 6, 2, 9, 1, 4, 5, 7, 3, 0])]\n",
    "        fitness_values = [531.0074131628102, 530.3983005966822, 561.4057235990515]\n",
    "\n",
    "        self.log(\"------------------------------\")\n",
    "        self.log(f\"[WHALES]\")\n",
    "        self.log(population)\n",
    "        self.log(fitness_values)\n",
    "        self.log(\"------------------------------\")\n",
    "        \n",
    "        best_idx = np.argmin(fitness_values)\n",
    "        best_pos = population[best_idx].copy()\n",
    "        best_fitness = fitness_values[best_idx]\n",
    "        \n",
    "        t = 0\n",
    "        while t < self.max_iter:\n",
    "            self.log(f\"[ITER] {t+1}\")\n",
    "            for w in range(self.n_whales):\n",
    "                self.cur_whale = w\n",
    "                whale = population[w]\n",
    "                self.log(f\"{whale} | {fitness_values[w]}\")\n",
    "                a = 2 - t * (2 / self.max_iter)\n",
    "                A = self._compute_A(a)\n",
    "                C = self._compute_C()\n",
    "                p = random.random()\n",
    "                self.log(f\"a={a}, A={A}, C={C}, p={p}\")\n",
    "\n",
    "                if p < 0.5:\n",
    "                    if abs(A) < 1:\n",
    "                        new_pos = self.__encircling_prey(whale, best_pos, A, C)\n",
    "                        self.log(f\"[MOVE] encircling prey --> {new_pos}\")\n",
    "                    else:\n",
    "                        rand_idx = random.randint(0, self.n_whales-1)\n",
    "                        random_pos = population[rand_idx]\n",
    "                        new_pos = self.__search_for_prey(whale, random_pos, A, C)\n",
    "                        self.log(f\"[MOVE] search for prey --> {new_pos}\")\n",
    "                else:\n",
    "                    new_pos = self.__bubble_net_attack(whale, best_pos)\n",
    "                    self.log(f\"[MOVE] bubble net attack --> {new_pos}\")\n",
    "                \n",
    "                # new_pos = self.__amend_position(new_pos)\n",
    "                # self.log(f\"amend position --> {new_pos}\")\n",
    "                # Apply local search to improve the new position\n",
    "                improved_pos, improved_fitness = self.__local_search(new_pos)\n",
    "                self.log(f\"[IMPROVE POSITION] {new_pos}, [{improved_fitness}]\")\n",
    "                \n",
    "                population[w] = improved_pos\n",
    "                fitness_values[w] = improved_fitness\n",
    "                \n",
    "                if improved_fitness < best_fitness:\n",
    "                    best_pos = improved_pos.copy()\n",
    "                    best_fitness = improved_fitness\n",
    "                    self.log(f\"[BEST] {best_pos}, [{best_fitness}]\")\n",
    "            self.cur_whale=None\n",
    "            t += 1\n",
    "        \n",
    "        return best_pos, best_fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T10:51:03.816678Z",
     "iopub.status.busy": "2024-12-28T10:51:03.816354Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# text = submission['text'][0]\n",
    "\n",
    "text = 'advent chimney elf family fireplace gingerbread mistletoe ornament reindeer scrooge'\n",
    "\n",
    "woa = WhaleOptimization(\n",
    "    text, score, \n",
    "    n_whales=3, max_iter=50, debug=True, \n",
    "    tabu_tenure=5, tabu_no_imp=5, tabu_max_iter=50\n",
    ")\n",
    "woa.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([0, 2, 1], 0.2312), ([0, 2, 1], 0.2312), ([0, 2, 1], 0.2312)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': [0,1,2],\n",
    "    'text': ['a b c', 'd g e', 'a f w']\n",
    "})\n",
    "\n",
    "res = []\n",
    "for s in submission['text']:\n",
    "    # woa = WhaleOptimization(\n",
    "    #     text, score, \n",
    "    #     n_whales=3, max_iter=20, debug=True, \n",
    "    #     tabu_tenure=5, tabu_no_imp=5, tabu_max_iter=50\n",
    "    # )\n",
    "    best_pos, best_fitness = [0,2,1], 0.2312\n",
    "    res.append((best_pos, best_fitness))\n",
    "    \n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 10229277,
     "sourceId": 88046,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 76277,
     "modelInstanceId": 72255,
     "sourceId": 104492,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
