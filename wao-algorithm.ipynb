{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "from math import exp\n",
    "from collections import Counter\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "from collections import deque\n",
    "from typing import Tuple, List\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-19T18:33:13.475255Z",
     "iopub.status.busy": "2024-12-19T18:33:13.474925Z",
     "iopub.status.idle": "2024-12-19T18:33:17.058755Z",
     "shell.execute_reply": "2024-12-19T18:33:17.058058Z",
     "shell.execute_reply.started": "2024-12-19T18:33:13.475215Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "PAD_TOKEN_LABEL_ID = torch.nn.CrossEntropyLoss().ignore_index\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "def score(\n",
    "    solution: pd.DataFrame,\n",
    "    submission: pd.DataFrame,\n",
    "    row_id_column_name: str,\n",
    "    model_path: str = '/kaggle/input/gemma-2/transformers/gemma-2-9b/2',\n",
    "    load_in_8bit: bool = False,\n",
    "    clear_mem: bool = False,\n",
    ") -> float:\n",
    "    # Check that each submitted string is a permutation of the solution string\n",
    "    sol_counts = solution.loc[:, 'text'].str.split().apply(Counter)\n",
    "    sub_counts = submission.loc[:, 'text'].str.split().apply(Counter)\n",
    "    invalid_mask = sol_counts != sub_counts\n",
    "    if invalid_mask.any():\n",
    "        raise ParticipantVisibleError(\n",
    "            'At least one submitted string is not a valid permutation of the solution string.'\n",
    "        )\n",
    "\n",
    "    # Calculate perplexity for the submitted strings\n",
    "    sub_strings = [\n",
    "        ' '.join(s.split()) for s in submission['text'].tolist()\n",
    "    ]  # Split and rejoin to normalize whitespace\n",
    "    scorer = PerplexityCalculator(\n",
    "        model_path=model_path,\n",
    "        load_in_8bit=load_in_8bit,\n",
    "    )  # Initialize the perplexity calculator with a pre-trained model\n",
    "    perplexities = scorer.get_perplexity(\n",
    "        sub_strings\n",
    "    )  # Calculate perplexity for each submitted string\n",
    "\n",
    "    if clear_mem:\n",
    "        # Just move on if it fails. Not essential if we have the score.\n",
    "        try:\n",
    "            scorer.clear_gpu_memory()\n",
    "        except:\n",
    "            print('GPU memory clearing failed.')\n",
    "\n",
    "    return float(np.mean(perplexities))\n",
    "\n",
    "class PerplexityCalculator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: str,\n",
    "        load_in_8bit: bool = False,\n",
    "        device_map: str = 'auto',\n",
    "    ):\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)\n",
    "        # Configure model loading based on quantization setting and device availability\n",
    "        if load_in_8bit:\n",
    "            if DEVICE.type != 'cuda':\n",
    "                raise ValueError('8-bit quantization requires CUDA device')\n",
    "            quantization_config = transformers.BitsAndBytesConfig(load_in_8bit=True)\n",
    "            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                quantization_config=quantization_config,\n",
    "                device_map=device_map,\n",
    "            )\n",
    "        else:\n",
    "            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                torch_dtype=torch.float16 if DEVICE.type == 'cuda' else torch.float32,\n",
    "                device_map=device_map,\n",
    "            )\n",
    "\n",
    "        self.loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "    def get_perplexity(\n",
    "        self, input_texts: Union[str, List[str]], debug=False\n",
    "    ) -> Union[float, List[float]]:\n",
    "        single_input = isinstance(input_texts, str)\n",
    "        input_texts = [input_texts] if single_input else input_texts\n",
    "\n",
    "        loss_list = []\n",
    "        with torch.no_grad():\n",
    "            # Process each sequence independently\n",
    "            for text in input_texts:\n",
    "                # Explicitly add sequence boundary tokens to the text\n",
    "                text_with_special = f\"{self.tokenizer.bos_token}{text}{self.tokenizer.eos_token}\"\n",
    "\n",
    "                # Tokenize\n",
    "                model_inputs = self.tokenizer(\n",
    "                    text_with_special,\n",
    "                    return_tensors='pt',\n",
    "                    add_special_tokens=False,\n",
    "                )\n",
    "\n",
    "                if 'token_type_ids' in model_inputs:\n",
    "                    model_inputs.pop('token_type_ids')\n",
    "\n",
    "                model_inputs = {k: v.to(DEVICE) for k, v in model_inputs.items()}\n",
    "\n",
    "                # Get model output\n",
    "                output = self.model(**model_inputs, use_cache=False)\n",
    "                logits = output['logits']\n",
    "\n",
    "                # Shift logits and labels for calculating loss\n",
    "                shift_logits = logits[..., :-1, :].contiguous()  # Drop last prediction\n",
    "                shift_labels = model_inputs['input_ids'][..., 1:].contiguous()  # Drop first input\n",
    "\n",
    "                # Calculate token-wise loss\n",
    "                loss = self.loss_fct(\n",
    "                    shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                    shift_labels.view(-1)\n",
    "                )\n",
    "\n",
    "                # Calculate average loss\n",
    "                sequence_loss = loss.sum() / len(loss)\n",
    "                loss_list.append(sequence_loss.cpu().item())\n",
    "\n",
    "                # Debug output\n",
    "                if debug:\n",
    "                    print(f\"\\nProcessing: '{text}'\")\n",
    "                    print(f\"With special tokens: '{text_with_special}'\")\n",
    "                    print(f\"Input tokens: {model_inputs['input_ids'][0].tolist()}\")\n",
    "                    print(f\"Target tokens: {shift_labels[0].tolist()}\")\n",
    "                    print(f\"Input decoded: {self.tokenizer.decode(model_inputs['input_ids'][0])}\")\n",
    "                    print(f\"Target decoded: {self.tokenizer.decode(shift_labels[0])}\")\n",
    "                    print(f\"Individual losses: {loss.tolist()}\")\n",
    "                    print(f\"Average loss: {sequence_loss.item():.4f}\")\n",
    "\n",
    "        ppl = [exp(i) for i in loss_list]\n",
    "\n",
    "        if debug:\n",
    "            print(\"\\nFinal perplexities:\")\n",
    "            for text, perp in zip(input_texts, ppl):\n",
    "                print(f\"Text: '{text}'\")\n",
    "                print(f\"Perplexity: {perp:.2f}\")\n",
    "\n",
    "        return ppl[0] if single_input else ppl\n",
    "\n",
    "    def clear_gpu_memory(self) -> None:\n",
    "        if not torch.cuda.is_available():\n",
    "            return\n",
    "\n",
    "        # Delete model and tokenizer if they exist\n",
    "        if hasattr(self, 'model'):\n",
    "            del self.model\n",
    "        if hasattr(self, 'tokenizer'):\n",
    "            del self.tokenizer\n",
    "\n",
    "        # Run garbage collection\n",
    "        gc.collect()\n",
    "\n",
    "        # Clear CUDA cache and reset memory stats\n",
    "        with DEVICE:\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "            torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n",
    "scorer = PerplexityCalculator(model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T18:35:55.487076Z",
     "iopub.status.busy": "2024-12-19T18:35:55.486578Z",
     "iopub.status.idle": "2024-12-19T18:35:55.507032Z",
     "shell.execute_reply": "2024-12-19T18:35:55.506137Z",
     "shell.execute_reply.started": "2024-12-19T18:35:55.487044Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"/kaggle/input/santa-2024/sample_submission.csv\")\n",
    "#perplexities = scorer.get_perplexity(submission[\"text\"].tolist())\n",
    "#perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T18:53:51.478034Z",
     "iopub.status.busy": "2024-12-19T18:53:51.477743Z",
     "iopub.status.idle": "2024-12-19T18:53:51.487051Z",
     "shell.execute_reply": "2024-12-19T18:53:51.486151Z",
     "shell.execute_reply.started": "2024-12-19T18:53:51.478010Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TabuSearch:\n",
    "    def __init__(self, calculate_perplexity, tabu_tenure=5, stop_on_non_improvement=20, max_iter=100):\n",
    "        self.tabu_tenure = tabu_tenure\n",
    "        self.stop_on_non_improvement = stop_on_non_improvement\n",
    "        self.max_iter = max_iter\n",
    "        self.calculate_perplexity = calculate_perplexity\n",
    "        self.tabu_list = deque(maxlen=self.tabu_tenure)\n",
    "    \n",
    "    def is_sentence_visited(self, sentence):\n",
    "        return self.to_text(sentence) in self.tabu_list\n",
    "    \n",
    "    def to_text(self, sentence):\n",
    "        return \" \".join(sentence) \n",
    "\n",
    "    def get_candidates(self, sentence): # two-opt swap\n",
    "        neighbors = []\n",
    "        \n",
    "        for _ in range(len(sentence)):\n",
    "            node1 = 0\n",
    "            node2 = 0\n",
    "            \n",
    "            while node1 == node2:\n",
    "                node1 = random.randint(1, len(sentence)-1)\n",
    "                node2 = random.randint(1, len(sentence)-1)\n",
    "                \n",
    "            if node1 > node2:\n",
    "                swap = node1\n",
    "                node1 = node2\n",
    "                node2 = swap\n",
    "                \n",
    "            tmp = sentence[node1:node2]\n",
    "            tmp_route = sentence[:node1] + tmp[::-1] +sentence[node2:]\n",
    "            neighbors.append(tmp_route)\n",
    "            \n",
    "        return neighbors\n",
    "\n",
    "    def optimize(self, current_sentence) -> Tuple[str, float]:\n",
    "        current_perplexity = self.calculate_perplexity(current_sentence)\n",
    "\n",
    "        best_sentence = current_sentence\n",
    "        best_perplexity = current_perplexity\n",
    "\n",
    "        no_improvement = 0\n",
    "        itr = 1\n",
    "        \n",
    "        while True and itr <= self.max_iter:\n",
    "            itr+=1\n",
    "            print(f\"[ITER] [{itr}] --- {best_perplexity}\")\n",
    "            \n",
    "            candidates = self.get_candidates(current_sentence)\n",
    "\n",
    "            best_candidate = None\n",
    "            best_candidate_perplexity = None\n",
    "\n",
    "            for candidate in candidates:\n",
    "                candidate_perplexity = self.calculate_perplexity(candidate)\n",
    "                if self.is_sentence_visited(candidate) or best_candidate_perplexity is None or candidate_perplexity < best_candidate_perplexity:\n",
    "                        best_candidate = candidate\n",
    "                        best_candidate_perplexity = candidate_perplexity\n",
    "            print(f\"[{best_candidate_perplexity}] {self.to_text(best_candidate)}\")\n",
    "\n",
    "            if best_candidate_perplexity < best_perplexity:\n",
    "                best_sentence = best_candidate\n",
    "                best_perplexity = best_candidate_perplexity\n",
    "                no_improvement = 0\n",
    "                print(f\"[IM] [{best_perplexity}]\")\n",
    "            else:\n",
    "                no_improvement += 1\n",
    "                print(f\"[NO IM] __{no_improvement}__\")\n",
    "\n",
    "            if no_improvement == self.stop_on_non_improvement:\n",
    "                break\n",
    "\n",
    "            current_sentence = best_candidate\n",
    "            current_perplexity = best_candidate_perplexity\n",
    "\n",
    "            self.tabu_list.append(best_sentence)\n",
    "\n",
    "        return best_sentence, best_perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import permutations\n",
    "\n",
    "\n",
    "class WhaleOptimizationForPerplexity:\n",
    "    def __init__(self, words, pop_size=30, max_iter=100):\n",
    "        # TODO pop_size < n!\n",
    "        self.words = words\n",
    "        self.n = len(words)  # Number of words in the sequence\n",
    "        self.pop_size = pop_size\n",
    "        self.max_iter = max_iter\n",
    "        self.population = None\n",
    "        self.best_solution = None\n",
    "        self.best_fitness = float('inf')\n",
    "    \n",
    "    def calculate_perplexity(self, sentence) -> float:\n",
    "        sentence = \" \".join(sentence)\n",
    "        submission = pd.DataFrame({'id': [0], 'text': [sentence] })\n",
    "        perplexities = scorer.get_perplexity(submission[\"text\"].tolist())\n",
    "        # print(f\"{perplexities[0]} || {sentence}\")\n",
    "        return perplexities[0]\n",
    "\n",
    "    def initialize_population(self):\n",
    "        \"\"\"Randomly initialize the population with valid permutations of the sequence.\"\"\"\n",
    "        perms = list(permutations(self.words))\n",
    "        random_indices = np.random.choice(len(perms), self.pop_size, replace=False)\n",
    "        self.population = [list(perms[idx]) for idx in random_indices]\n",
    "\n",
    "    def evaluate_population(self):\n",
    "        \"\"\"Evaluate the perplexity of each sequence in the population.\"\"\"\n",
    "        return np.array([self.calculate_perplexity(seq) for seq in self.population])\n",
    "\n",
    "    def optimize(self):\n",
    "        \"\"\"Perform the optimization using WOA.\"\"\"\n",
    "        # Initialize population and evaluate fitness\n",
    "        self.initialize_population()\n",
    "        fitness = self.evaluate_population()\n",
    "\n",
    "        # Track the best solution\n",
    "        best_idx = np.argmin(fitness)\n",
    "        self.best_solution = self.population[best_idx]\n",
    "        self.best_fitness = fitness[best_idx]\n",
    "\n",
    "        # Optimization loop\n",
    "        for itr in range(self.max_iter):\n",
    "            a = 2 - itr * (2 / self.max_iter)  # Linearly decreasing parameter\n",
    "\n",
    "            for i in range(self.pop_size):\n",
    "                r = np.random.random(self.n)  # Random vector\n",
    "                A = 2 * a * r - a  # Encircling prey\n",
    "                C = 2 * r  # Attraction parameter\n",
    "                p = np.random.random()  # Probability for exploitation vs exploration\n",
    "\n",
    "                if p < 0.5:\n",
    "                    if np.linalg.norm(A) < 1:  # Exploitation: Move closer to the best solution\n",
    "                        new_position = self.population[i].copy()\n",
    "                        for j in range(self.n):\n",
    "                            if np.random.random() < np.abs(A[j]):  # Update based on closeness to the best\n",
    "                                new_position[j] = self.best_solution[j]\n",
    "                    else:  # Exploration: Move closer to a random whale\n",
    "                        random_whale = self.population[np.random.randint(0, self.pop_size)]\n",
    "                        new_position = self.population[i].copy()\n",
    "                        for j in range(self.n):\n",
    "                            if np.random.random() < np.abs(A[j]):  # Update based on closeness to the random whale\n",
    "                                new_position[j] = random_whale[j]\n",
    "                else:  # Spiral updating (bubble-net hunting)\n",
    "                    l = np.random.uniform(-1, 1)\n",
    "                    new_position = self.population[i].copy()\n",
    "                    for j in range(self.n):\n",
    "                        distance = self.best_solution[j] != new_position[j]\n",
    "                        if np.random.random() < np.exp(-distance * l):  # Spiral towards the best\n",
    "                            new_position[j] = self.best_solution[j]\n",
    "\n",
    "                # Ensure new position is a valid permutation\n",
    "                new_position = list(np.random.permutation(new_position))\n",
    "\n",
    "                # Update population\n",
    "                self.population[i] = new_position\n",
    "\n",
    "            # Evaluate fitness after update\n",
    "            fitness = self.evaluate_population()\n",
    "            current_best_idx = np.argmin(fitness)\n",
    "            current_best_fitness = fitness[current_best_idx]\n",
    "\n",
    "            # Update global best if needed\n",
    "            if current_best_fitness < self.best_fitness:\n",
    "                self.best_fitness = current_best_fitness\n",
    "                self.best_solution = self.population[current_best_idx]\n",
    "\n",
    "            # Logging (optional)\n",
    "            print(f\"Iteration {itr+1}/{self.max_iter}, Best Perplexity: {self.best_fitness}\")\n",
    "\n",
    "        return self.best_solution, self.best_fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = [word for sublist in submission['text'].str.split(' ') for word in sublist]\n",
    "\n",
    "woa = WhaleOptimizationForPerplexity(sequence, pop_size=10, max_iter=50)\n",
    "woa.optimize()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 10229277,
     "sourceId": 88046,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 76277,
     "modelInstanceId": 72255,
     "sourceId": 104492,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
