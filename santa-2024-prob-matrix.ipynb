{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":88046,"databundleVersionId":10229277,"sourceType":"competition"},{"sourceId":104492,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":72255,"modelId":76277}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"Evaluation metric for Santa 2024.\"\"\"\n\nimport gc\nimport os\nfrom math import exp\nfrom collections import Counter\nfrom typing import List, Optional, Union\n\nimport numpy as np\nimport pandas as pd\nimport transformers\nimport torch\nimport torch.nn.functional as F\n\n\nos.environ['OMP_NUM_THREADS'] = '1'\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\nPAD_TOKEN_LABEL_ID = torch.nn.CrossEntropyLoss().ignore_index\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\nclass ParticipantVisibleError(Exception):\n    pass\n\n\ndef score(\n    solution: pd.DataFrame,\n    submission: pd.DataFrame,\n    row_id_column_name: str,\n    model_path: str = '/kaggle/input/gemma-2/transformers/gemma-2-9b/2',\n    load_in_8bit: bool = False,\n    clear_mem: bool = False,\n) -> float:\n    \"\"\"\n    Calculates the mean perplexity of submitted text permutations compared to an original text.\n\n    Parameters\n    ----------\n    solution : DataFrame\n        DataFrame containing the original text in a column named 'text'.\n        Includes a row ID column specified by `row_id_column_name`.\n\n    submission : DataFrame\n        DataFrame containing the permuted text in a column named 'text'.\n        Must have the same row IDs as the solution.\n        Includes a row ID column specified by `row_id_column_name`.\n\n    row_id_column_name : str\n        Name of the column containing row IDs.\n        Ensures aligned comparison between solution and submission.\n\n    model_path : str, default='/kaggle/input/gemma-2/transformers/gemma-2-9b/2'\n        Path to the serialized LLM.\n\n    load_in_8bit : bool, default=False\n        Use 8-bit quantization for the model. Requires CUDA.\n\n    clear_mem : bool, default=False\n        Clear GPU memory after scoring by clearing the CUDA cache.\n        Useful for testing.\n\n    Returns\n    -------\n    float\n        The mean perplexity score. Lower is better.\n\n    Raises\n    ------\n    ParticipantVisibleError\n        If the submission format is invalid or submitted strings are not valid permutations.\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n    >>> solution = pd.DataFrame({\n    ...     'id': [0, 1],\n    ...     'text': [\"this is a normal english sentence\", \"the quick brown fox jumps over the lazy dog\"]\n    ... })\n    >>> submission = pd.DataFrame({\n    ...     'id': [0, 1],\n    ...     'text': [\"sentence english normal a is this\", \"lazy the over jumps fox brown quick the dog\"]\n    ... })\n    >>> score(solution, submission, 'id', model_path=model_path, clear_mem=True) > 0\n    True\n    \"\"\"\n    # Check that each submitted string is a permutation of the solution string\n    sol_counts = solution.loc[:, 'text'].str.split().apply(Counter)\n    sub_counts = submission.loc[:, 'text'].str.split().apply(Counter)\n    invalid_mask = sol_counts != sub_counts\n    if invalid_mask.any():\n        raise ParticipantVisibleError(\n            'At least one submitted string is not a valid permutation of the solution string.'\n        )\n\n    # Calculate perplexity for the submitted strings\n    sub_strings = [\n        ' '.join(s.split()) for s in submission['text'].tolist()\n    ]  # Split and rejoin to normalize whitespace\n    scorer = PerplexityCalculator(\n        model_path=model_path,\n        load_in_8bit=load_in_8bit,\n    )  # Initialize the perplexity calculator with a pre-trained model\n    perplexities = scorer.get_perplexity(\n        sub_strings\n    )  # Calculate perplexity for each submitted string\n\n    if clear_mem:\n        # Just move on if it fails. Not essential if we have the score.\n        try:\n            scorer.clear_gpu_memory()\n        except:\n            print('GPU memory clearing failed.')\n\n    return float(np.mean(perplexities))\n\n\nclass PerplexityCalculator:\n    \"\"\"\n    Calculates perplexity of text using a pre-trained language model.\n\n    Adapted from https://github.com/asahi417/lmppl/blob/main/lmppl/ppl_recurrent_lm.py\n\n    Parameters\n    ----------\n    model_path : str\n        Path to the pre-trained language model\n\n    load_in_8bit : bool, default=False\n        Use 8-bit quantization for the model. Requires CUDA.\n\n    device_map : str, default=\"auto\"\n        Device mapping for the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_path: str,\n        load_in_8bit: bool = False,\n        device_map: str = 'auto',\n    ):\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)\n        # Configure model loading based on quantization setting and device availability\n        if load_in_8bit:\n            if DEVICE.type != 'cuda':\n                raise ValueError('8-bit quantization requires CUDA device')\n            quantization_config = transformers.BitsAndBytesConfig(load_in_8bit=True)\n            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n                model_path,\n                quantization_config=quantization_config,\n                device_map=device_map,\n            )\n        else:\n            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n                model_path,\n                torch_dtype=torch.float16 if DEVICE.type == 'cuda' else torch.float32,\n                device_map=device_map,\n            )\n\n        self.loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n\n        self.model.eval()\n\n    def get_perplexity(\n        self, input_texts: Union[str, List[str]], debug=False\n    ) -> Union[float, List[float]]:\n        \"\"\"\n        Calculates the perplexity of given texts.\n\n        Parameters\n        ----------\n        input_texts : str or list of str\n            A single string or a list of strings.\n\n        batch_size : int, default=None\n            Batch size for processing. Defaults to the number of input texts.\n\n        debug : bool, default=False\n            Print debugging information.\n\n        Returns\n        -------\n        float or list of float\n            A single perplexity value if input is a single string,\n            or a list of perplexity values if input is a list of strings.\n\n        Examples\n        --------\n        >>> import pandas as pd\n        >>> model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n        >>> scorer = PerplexityCalculator(model_path=model_path)\n\n        >>> submission = pd.DataFrame({\n        ...     'id': [0, 1, 2],\n        ...     'text': [\"this is a normal english sentence\", \"thsi is a slihgtly misspelled zr4g sentense\", \"the quick brown fox jumps over the lazy dog\"]\n        ... })\n        >>> perplexities = scorer.get_perplexity(submission[\"text\"].tolist())\n        >>> perplexities[0] < perplexities[1]\n        True\n        >>> perplexities[2] < perplexities[0]\n        True\n\n        >>> perplexities = scorer.get_perplexity([\"this is a sentence\", \"another sentence\"])\n        >>> all(p > 0 for p in perplexities)\n        True\n\n        >>> scorer.clear_gpu_memory()\n        \"\"\"\n        single_input = isinstance(input_texts, str)\n        input_texts = [input_texts] if single_input else input_texts\n\n        loss_list = []\n        with torch.no_grad():\n            # Process each sequence independently\n            for text in input_texts:\n                # Explicitly add sequence boundary tokens to the text\n                text_with_special = f\"{self.tokenizer.bos_token}{text}{self.tokenizer.eos_token}\"\n\n                # Tokenize\n                model_inputs = self.tokenizer(\n                    text_with_special,\n                    return_tensors='pt',\n                    add_special_tokens=False,\n                )\n\n                if 'token_type_ids' in model_inputs:\n                    model_inputs.pop('token_type_ids')\n\n                model_inputs = {k: v.to(DEVICE) for k, v in model_inputs.items()}\n\n                # Get model output\n                output = self.model(**model_inputs, use_cache=False)\n                logits = output['logits']\n\n                # Shift logits and labels for calculating loss\n                shift_logits = logits[..., :-1, :].contiguous()  # Drop last prediction\n                shift_labels = model_inputs['input_ids'][..., 1:].contiguous()  # Drop first input\n\n                # Calculate token-wise loss\n                loss = self.loss_fct(\n                    shift_logits.view(-1, shift_logits.size(-1)),\n                    shift_labels.view(-1)\n                )\n\n                # Calculate average loss\n                sequence_loss = loss.sum() / len(loss)\n                loss_list.append(sequence_loss.cpu().item())\n\n                # Debug output\n                if debug:\n                    print(f\"\\nProcessing: '{text}'\")\n                    print(f\"With special tokens: '{text_with_special}'\")\n                    print(f\"Input tokens: {model_inputs['input_ids'][0].tolist()}\")\n                    print(f\"Target tokens: {shift_labels[0].tolist()}\")\n                    print(f\"Input decoded: {self.tokenizer.decode(model_inputs['input_ids'][0])}\")\n                    print(f\"Target decoded: {self.tokenizer.decode(shift_labels[0])}\")\n                    print(f\"Individual losses: {loss.tolist()}\")\n                    print(f\"Average loss: {sequence_loss.item():.4f}\")\n\n        ppl = [exp(i) for i in loss_list]\n\n        if debug:\n            print(\"\\nFinal perplexities:\")\n            for text, perp in zip(input_texts, ppl):\n                print(f\"Text: '{text}'\")\n                print(f\"Perplexity: {perp:.2f}\")\n\n        return ppl[0] if single_input else ppl\n\n    def clear_gpu_memory(self) -> None:\n        \"\"\"Clears GPU memory by deleting references and emptying caches.\"\"\"\n        if not torch.cuda.is_available():\n            return\n\n        # Delete model and tokenizer if they exist\n        if hasattr(self, 'model'):\n            del self.model\n        if hasattr(self, 'tokenizer'):\n            del self.tokenizer\n\n        # Run garbage collection\n        gc.collect()\n\n        # Clear CUDA cache and reset memory stats\n        with DEVICE:\n            torch.cuda.empty_cache()\n            torch.cuda.ipc_collect()\n            torch.cuda.reset_peak_memory_stats()\n    \n    def get_word_transition_matrix(self, sentence: str, debug=False):\n        \"\"\"\n        Calculate the transition probability matrix for words in a sentence.\n    \n        Parameters\n        ----------\n        sentence : str\n            The input sentence for which the transition probabilities are calculated.\n    \n        debug : bool, default=False\n            Print debugging information.\n    \n        Returns\n        -------\n        pd.DataFrame\n            A DataFrame containing the transition probabilities between words.\n        \"\"\"\n        # Add special tokens to the input sentence\n        text_with_special = f\"{self.tokenizer.bos_token}{sentence}{self.tokenizer.eos_token}\"\n    \n        if debug:\n            print(f\"Input sentence: {sentence}\")\n            print(f\"Sentence with special tokens: {text_with_special}\")\n    \n        # Tokenize the sentence\n        model_inputs = self.tokenizer(\n            text_with_special,\n            return_tensors='pt',\n            add_special_tokens=False,\n        )\n    \n        if 'token_type_ids' in model_inputs:\n            model_inputs.pop('token_type_ids')\n    \n        model_inputs = {k: v.to(DEVICE) for k, v in model_inputs.items()}\n    \n        if debug:\n            print(f\"Tokenized input IDs: {model_inputs['input_ids'][0].tolist()}\")\n            print(f\"Decoded tokens: {self.tokenizer.convert_ids_to_tokens(model_inputs['input_ids'][0].tolist())}\")\n    \n        with torch.no_grad():\n            # Get model output\n            output = self.model(**model_inputs, use_cache=False)\n            logits = output['logits']  # Shape: (batch_size, seq_len, vocab_size)\n    \n        if debug:\n            print(f\"Logits shape: {logits.shape}\")\n    \n        # Convert logits to probabilities using softmax\n        probabilities = F.softmax(logits, dim=-1).squeeze(0)  # Shape: (seq_len, vocab_size)\n    \n        if debug:\n            print(f\"Softmax probabilities shape: {probabilities.shape}\")\n    \n        # Map token IDs to words\n        tokens = model_inputs['input_ids'][0].tolist()\n        words = self.tokenizer.convert_ids_to_tokens(tokens)\n    \n        if debug:\n            print(f\"Tokens: {tokens}\")\n            print(f\"Words: {words}\")\n    \n        # Build transition probability matrix\n        transition_matrix = []\n        for i, word in enumerate(words[:-1]):  # Skip the last token as it has no next word\n            next_probs = probabilities[i].cpu().numpy()  # Probabilities for the next token\n            transition_matrix.append((word, next_probs))\n    \n            if debug:\n                print(f\"Word: {word}\")\n                print(f\"Next token probabilities: {next_probs}\")\n    \n        # Convert to DataFrame for easier readability\n        vocab_words = self.tokenizer.convert_ids_to_tokens(range(len(next_probs)))\n        df = pd.DataFrame(\n            {word: probs for word, probs in transition_matrix},\n            index=vocab_words\n        ).T\n    \n        if debug:\n            print(f\"Transition matrix DataFrame:\\n{df}\")\n    \n        return df.loc[words[1:-1], words[1:-1]]\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T11:49:40.807355Z","iopub.execute_input":"2024-12-15T11:49:40.807804Z","iopub.status.idle":"2024-12-15T11:49:45.187726Z","shell.execute_reply.started":"2024-12-15T11:49:40.807766Z","shell.execute_reply":"2024-12-15T11:49:45.186682Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"scorer = PerplexityCalculator(model_path=\"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\")\n\nsentence = \"the quick brown fox jumps over the lazy dog\"\ntransition_matrix = scorer.get_word_transition_matrix(sentence, True)\n\nprint(transition_matrix)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T11:49:45.189691Z","iopub.execute_input":"2024-12-15T11:49:45.190168Z","iopub.status.idle":"2024-12-15T11:53:59.756078Z","shell.execute_reply.started":"2024-12-15T11:49:45.190135Z","shell.execute_reply":"2024-12-15T11:53:59.752617Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54be69e74e8746e397e26f0207727909"}},"metadata":{}},{"name":"stdout","text":"Input sentence: the quick brown fox jumps over the lazy dog\nSentence with special tokens: <bos>the quick brown fox jumps over the lazy dog<eos>\nTokenized input IDs: [2, 1175, 4320, 8426, 25341, 36271, 1163, 573, 27894, 5929, 1]\nDecoded tokens: ['<bos>', 'the', '▁quick', '▁brown', '▁fox', '▁jumps', '▁over', '▁the', '▁lazy', '▁dog', '<eos>']\n","output_type":"stream"},{"name":"stderr","text":"\nKeyboardInterrupt\n\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"transition_matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T11:53:59.757426Z","iopub.status.idle":"2024-12-15T11:53:59.757901Z","shell.execute_reply.started":"2024-12-15T11:53:59.757696Z","shell.execute_reply":"2024-12-15T11:53:59.757732Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"code","source":"model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\nload_in_8bit = False\ndevice_map: str = 'auto'\nsentence = \"the quick brown fox jumps over the lazy dog\"\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_path)\n\nif load_in_8bit:\n    if DEVICE.type != 'cuda':\n        raise ValueError('8-bit quantization requires CUDA device')\n    quantization_config = transformers.BitsAndBytesConfig(load_in_8bit=True)\n    model = transformers.AutoModelForCausalLM.from_pretrained(\n        model_path,\n        quantization_config=quantization_config,\n        device_map=device_map,\n    )\nelse:\n    model = transformers.AutoModelForCausalLM.from_pretrained(\n        model_path,\n        torch_dtype=torch.float16 if DEVICE.type == 'cuda' else torch.float32,\n        device_map=device_map,\n    )\n\nloss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T11:53:59.759183Z","iopub.status.idle":"2024-12-15T11:53:59.760418Z","shell.execute_reply.started":"2024-12-15T11:53:59.760179Z","shell.execute_reply":"2024-12-15T11:53:59.760213Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text_with_special = f\"{tokenizer.bos_token}{sentence}{tokenizer.eos_token}\"\n    \nprint(f\"Input sentence: {sentence}\")\nprint(f\"Sentence with special tokens: {text_with_special}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T11:53:59.762295Z","iopub.status.idle":"2024-12-15T11:53:59.762856Z","shell.execute_reply.started":"2024-12-15T11:53:59.762585Z","shell.execute_reply":"2024-12-15T11:53:59.762612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Tokenize the sentence\nmodel_inputs = tokenizer(\n    text_with_special,\n    return_tensors='pt',\n    add_special_tokens=False,\n)\n\nmodel_inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T11:53:59.764356Z","iopub.status.idle":"2024-12-15T11:53:59.765538Z","shell.execute_reply.started":"2024-12-15T11:53:59.765159Z","shell.execute_reply":"2024-12-15T11:53:59.765196Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if 'token_type_ids' in model_inputs:\n    model_inputs.pop('token_type_ids')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T11:53:59.767426Z","iopub.status.idle":"2024-12-15T11:53:59.767983Z","shell.execute_reply.started":"2024-12-15T11:53:59.767714Z","shell.execute_reply":"2024-12-15T11:53:59.767740Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nmodel_inputs = {k: v.to(DEVICE) for k, v in model_inputs.items()}\n\nprint(f\"Tokenized input IDs: {model_inputs['input_ids'][0].tolist()}\")\nprint(f\"Decoded tokens: {tokenizer.convert_ids_to_tokens(model_inputs['input_ids'][0].tolist())}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T11:53:59.769075Z","iopub.status.idle":"2024-12-15T11:53:59.769699Z","shell.execute_reply.started":"2024-12-15T11:53:59.769311Z","shell.execute_reply":"2024-12-15T11:53:59.769344Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with torch.no_grad():\n    # Get model output\n    output = model(**model_inputs, use_cache=False)\n    logits = output['logits']  # Shape: (batch_size, seq_len, vocab_size)\n\nprint(f\"Logits shape: {logits.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T11:53:59.773484Z","iopub.status.idle":"2024-12-15T11:53:59.774807Z","shell.execute_reply.started":"2024-12-15T11:53:59.774559Z","shell.execute_reply":"2024-12-15T11:53:59.774589Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert logits to probabilities using softmax\nprobabilities = F.softmax(logits, dim=-1).squeeze(0)  # Shape: (seq_len, vocab_size)\n\nprint(f\"Softmax probabilities shape: {probabilities.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T11:53:59.775870Z","iopub.status.idle":"2024-12-15T11:53:59.776401Z","shell.execute_reply.started":"2024-12-15T11:53:59.776132Z","shell.execute_reply":"2024-12-15T11:53:59.776159Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"probabilities[0][1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T11:53:59.778712Z","iopub.status.idle":"2024-12-15T11:53:59.779234Z","shell.execute_reply.started":"2024-12-15T11:53:59.778983Z","shell.execute_reply":"2024-12-15T11:53:59.779012Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Map token IDs to words\ntokens = model_inputs['input_ids'][0].tolist()\nwords = tokenizer.convert_ids_to_tokens(tokens)\n\nprint(f\"Tokens: {tokens}\")\nprint(f\"Words: {words}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T11:53:59.781011Z","iopub.status.idle":"2024-12-15T11:53:59.781529Z","shell.execute_reply.started":"2024-12-15T11:53:59.781268Z","shell.execute_reply":"2024-12-15T11:53:59.781294Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Build transition probability matrix\ntransition_matrix = []\nfor i, word in enumerate(words[:-1]):  # Skip the last token as it has no next word\n    next_probs = probabilities[i].cpu().numpy()  # Probabilities for the next token\n    transition_matrix.append((word, next_probs))\n\nprint(f\"Word: {word}\")\nprint(f\"Next token probabilities: {next_probs}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T11:53:59.783245Z","iopub.status.idle":"2024-12-15T11:53:59.783760Z","shell.execute_reply.started":"2024-12-15T11:53:59.783519Z","shell.execute_reply":"2024-12-15T11:53:59.783544Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transition_matrix[0][1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T11:53:59.785359Z","iopub.status.idle":"2024-12-15T11:53:59.785902Z","shell.execute_reply.started":"2024-12-15T11:53:59.785643Z","shell.execute_reply":"2024-12-15T11:53:59.785677Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert to DataFrame for easier readability\nvocab_words = tokenizer.convert_ids_to_tokens(range(len(next_probs)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T11:53:59.787323Z","iopub.status.idle":"2024-12-15T11:53:59.788284Z","shell.execute_reply.started":"2024-12-15T11:53:59.788092Z","shell.execute_reply":"2024-12-15T11:53:59.788114Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.DataFrame(\n    {word: probs for word, probs in transition_matrix},\n    index=vocab_words\n).T\n\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T11:53:59.789750Z","iopub.status.idle":"2024-12-15T11:53:59.790316Z","shell.execute_reply.started":"2024-12-15T11:53:59.789952Z","shell.execute_reply":"2024-12-15T11:53:59.789971Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"words[1:-1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T11:53:59.792065Z","iopub.status.idle":"2024-12-15T11:53:59.792438Z","shell.execute_reply.started":"2024-12-15T11:53:59.792264Z","shell.execute_reply":"2024-12-15T11:53:59.792283Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.loc[words[1:-1], words[1:-1]]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T11:53:59.793747Z","iopub.status.idle":"2024-12-15T11:53:59.794133Z","shell.execute_reply.started":"2024-12-15T11:53:59.793959Z","shell.execute_reply":"2024-12-15T11:53:59.793978Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}