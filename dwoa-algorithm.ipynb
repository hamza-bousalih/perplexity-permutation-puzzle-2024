{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d97bccb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T12:04:32.446413Z",
     "iopub.status.busy": "2025-01-01T12:04:32.446231Z",
     "iopub.status.idle": "2025-01-01T12:04:36.359222Z",
     "shell.execute_reply": "2025-01-01T12:04:36.358556Z",
     "shell.execute_reply.started": "2025-01-01T12:04:32.446394Z"
    },
    "papermill": {
     "duration": 4.184328,
     "end_time": "2024-12-29T17:53:21.724893",
     "exception": false,
     "start_time": "2024-12-29T17:53:17.540565",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "from math import exp\n",
    "from collections import Counter\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "from collections import deque\n",
    "from typing import Tuple, List\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50ebb67",
   "metadata": {},
   "source": [
    "## GEMMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05f83c54",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-01T12:04:36.360810Z",
     "iopub.status.busy": "2025-01-01T12:04:36.360310Z",
     "iopub.status.idle": "2025-01-01T12:04:36.446038Z",
     "shell.execute_reply": "2025-01-01T12:04:36.445127Z",
     "shell.execute_reply.started": "2025-01-01T12:04:36.360771Z"
    },
    "papermill": {
     "duration": 0.090415,
     "end_time": "2024-12-29T17:53:21.818503",
     "exception": false,
     "start_time": "2024-12-29T17:53:21.728088",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "PAD_TOKEN_LABEL_ID = torch.nn.CrossEntropyLoss().ignore_index\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "def score(\n",
    "    solution: pd.DataFrame,\n",
    "    submission: pd.DataFrame,\n",
    "    row_id_column_name: str,\n",
    "    model_path: str = '/kaggle/input/gemma-2/transformers/gemma-2-9b/2',\n",
    "    load_in_8bit: bool = False,\n",
    "    clear_mem: bool = False,\n",
    ") -> float:\n",
    "    # Check that each submitted string is a permutation of the solution string\n",
    "    sol_counts = solution.loc[:, 'text'].str.split().apply(Counter)\n",
    "    sub_counts = submission.loc[:, 'text'].str.split().apply(Counter)\n",
    "    invalid_mask = sol_counts != sub_counts\n",
    "    if invalid_mask.any():\n",
    "        raise ParticipantVisibleError(\n",
    "            'At least one submitted string is not a valid permutation of the solution string.'\n",
    "        )\n",
    "\n",
    "    # Calculate perplexity for the submitted strings\n",
    "    sub_strings = [\n",
    "        ' '.join(s.split()) for s in submission['text'].tolist()\n",
    "    ]  # Split and rejoin to normalize whitespace\n",
    "    scorer = PerplexityCalculator(\n",
    "        model_path=model_path,\n",
    "        load_in_8bit=load_in_8bit,\n",
    "    )  # Initialize the perplexity calculator with a pre-trained model\n",
    "    perplexities = scorer.get_perplexity(\n",
    "        sub_strings\n",
    "    )  # Calculate perplexity for each submitted string\n",
    "\n",
    "    if clear_mem:\n",
    "        # Just move on if it fails. Not essential if we have the score.\n",
    "        try:\n",
    "            scorer.clear_gpu_memory()\n",
    "        except:\n",
    "            print('GPU memory clearing failed.')\n",
    "\n",
    "    return float(np.mean(perplexities))\n",
    "\n",
    "class PerplexityCalculator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: str,\n",
    "        load_in_8bit: bool = False,\n",
    "        device_map: str = 'auto',\n",
    "    ):\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)\n",
    "        # Configure model loading based on quantization setting and device availability\n",
    "        if load_in_8bit:\n",
    "            if DEVICE.type != 'cuda':\n",
    "                raise ValueError('8-bit quantization requires CUDA device')\n",
    "            quantization_config = transformers.BitsAndBytesConfig(load_in_8bit=True)\n",
    "            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                quantization_config=quantization_config,\n",
    "                device_map=device_map,\n",
    "            )\n",
    "        else:\n",
    "            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                torch_dtype=torch.float16 if DEVICE.type == 'cuda' else torch.float32,\n",
    "                device_map=device_map,\n",
    "            )\n",
    "\n",
    "        self.loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "    def get_perplexity(\n",
    "        self, input_texts: Union[str, List[str]], debug=False\n",
    "    ) -> Union[float, List[float]]:\n",
    "        single_input = isinstance(input_texts, str)\n",
    "        input_texts = [input_texts] if single_input else input_texts\n",
    "\n",
    "        loss_list = []\n",
    "        with torch.no_grad():\n",
    "            # Process each sequence independently\n",
    "            for text in input_texts:\n",
    "                # Explicitly add sequence boundary tokens to the text\n",
    "                text_with_special = f\"{self.tokenizer.bos_token}{text}{self.tokenizer.eos_token}\"\n",
    "\n",
    "                # Tokenize\n",
    "                model_inputs = self.tokenizer(\n",
    "                    text_with_special,\n",
    "                    return_tensors='pt',\n",
    "                    add_special_tokens=False,\n",
    "                )\n",
    "\n",
    "                if 'token_type_ids' in model_inputs:\n",
    "                    model_inputs.pop('token_type_ids')\n",
    "\n",
    "                model_inputs = {k: v.to(DEVICE) for k, v in model_inputs.items()}\n",
    "\n",
    "                # Get model output\n",
    "                output = self.model(**model_inputs, use_cache=False)\n",
    "                logits = output['logits']\n",
    "\n",
    "                # Shift logits and labels for calculating loss\n",
    "                shift_logits = logits[..., :-1, :].contiguous()  # Drop last prediction\n",
    "                shift_labels = model_inputs['input_ids'][..., 1:].contiguous()  # Drop first input\n",
    "\n",
    "                # Calculate token-wise loss\n",
    "                loss = self.loss_fct(\n",
    "                    shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                    shift_labels.view(-1)\n",
    "                )\n",
    "\n",
    "                # Calculate average loss\n",
    "                sequence_loss = loss.sum() / len(loss)\n",
    "                loss_list.append(sequence_loss.cpu().item())\n",
    "\n",
    "                # Debug output\n",
    "                if debug:\n",
    "                    print(f\"\\nProcessing: '{text}'\")\n",
    "                    print(f\"With special tokens: '{text_with_special}'\")\n",
    "                    print(f\"Input tokens: {model_inputs['input_ids'][0].tolist()}\")\n",
    "                    print(f\"Target tokens: {shift_labels[0].tolist()}\")\n",
    "                    print(f\"Input decoded: {self.tokenizer.decode(model_inputs['input_ids'][0])}\")\n",
    "                    print(f\"Target decoded: {self.tokenizer.decode(shift_labels[0])}\")\n",
    "                    print(f\"Individual losses: {loss.tolist()}\")\n",
    "                    print(f\"Average loss: {sequence_loss.item():.4f}\")\n",
    "\n",
    "        ppl = [exp(i) for i in loss_list]\n",
    "\n",
    "        if debug:\n",
    "            print(\"\\nFinal perplexities:\")\n",
    "            for text, perp in zip(input_texts, ppl):\n",
    "                print(f\"Text: '{text}'\")\n",
    "                print(f\"Perplexity: {perp:.2f}\")\n",
    "\n",
    "        return ppl[0] if single_input else ppl\n",
    "\n",
    "    def clear_gpu_memory(self) -> None:\n",
    "        if not torch.cuda.is_available():\n",
    "            return\n",
    "\n",
    "        # Delete model and tokenizer if they exist\n",
    "        if hasattr(self, 'model'):\n",
    "            del self.model\n",
    "        if hasattr(self, 'tokenizer'):\n",
    "            del self.tokenizer\n",
    "\n",
    "        # Run garbage collection\n",
    "        gc.collect()\n",
    "\n",
    "        # Clear CUDA cache and reset memory stats\n",
    "        with DEVICE:\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "            torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001c4b31",
   "metadata": {},
   "source": [
    "---\n",
    "# CACHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df9165e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T12:04:36.447576Z",
     "iopub.status.busy": "2025-01-01T12:04:36.447268Z",
     "iopub.status.idle": "2025-01-01T12:04:36.464793Z",
     "shell.execute_reply": "2025-01-01T12:04:36.463897Z",
     "shell.execute_reply.started": "2025-01-01T12:04:36.447547Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Caching:\n",
    "    def __init__(self, max_size: int = 1000):\n",
    "        self.stores: List[dict] = []\n",
    "        self.idx = -1\n",
    "        self.max_size = max_size\n",
    "        self._state = {'len': 0, 'hit': 0, 'miss': 0, 'total': 0}\n",
    "\n",
    "    def create_store(self):\n",
    "        self.stores.append({})\n",
    "        self.idx += 1\n",
    "        self._state['len'] += 1\n",
    "\n",
    "    def add(self, key, value):\n",
    "        if self.idx == -1 or len(self.stores[self.idx]) >= self.max_size:\n",
    "            self.create_store()\n",
    "        self.stores[self.idx][key] = value\n",
    "\n",
    "    def get(self, key):\n",
    "        if self.idx == -1:\n",
    "            return None\n",
    "        self._state['total'] += 1\n",
    "        v = self.stores[self.idx].get(key, None)\n",
    "        if v is None:\n",
    "            self._state['miss'] += 1\n",
    "        else:\n",
    "            self._state['hit'] += 1\n",
    "        return v\n",
    "\n",
    "    def state(self):\n",
    "        return f\"total: {self._state['total']}, len: {self._state['len']}, stores: {len(self.stores)} \"\\\n",
    "              f\"hit: {self._state['hit']} [{self._state['hit']/self._state['total']*100}%], \"\\\n",
    "              f\"miss: {self._state['miss']} [{self._state['miss']/self._state['total']*100}%]\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49681d0",
   "metadata": {
    "papermill": {
     "duration": 0.002792,
     "end_time": "2024-12-29T17:56:05.356078",
     "exception": false,
     "start_time": "2024-12-29T17:56:05.353286",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "# TabuSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "808ef37d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T12:04:36.466203Z",
     "iopub.status.busy": "2025-01-01T12:04:36.465878Z",
     "iopub.status.idle": "2025-01-01T12:04:36.485882Z",
     "shell.execute_reply": "2025-01-01T12:04:36.485177Z",
     "shell.execute_reply.started": "2025-01-01T12:04:36.466171Z"
    },
    "papermill": {
     "duration": 0.016426,
     "end_time": "2024-12-29T17:56:05.375373",
     "exception": false,
     "start_time": "2024-12-29T17:56:05.358947",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TabuSearch:\n",
    "    '''\n",
    "    Tabu Search Algorithm\n",
    "\n",
    "    This class implements the Tabu Search optimization algorithm, which is used to solve\n",
    "    combinatorial optimization problems. It iteratively improves solutions by exploring\n",
    "    neighboring candidates and using a tabu list to avoid revisiting recently explored solutions.\n",
    "\n",
    "    Parameters:\n",
    "        words (list[str]): The sequence of words to be optimized.\n",
    "        calculate_perplexity (callable): A method to calculate the perplexity of a sentence.\n",
    "        tabu_tenure (int, optional): The size of the tabu list. Default is 5.\n",
    "        max_iter (int, optional): The maximum number of iterations to execute. Default is 100.\n",
    "        stop_on_non_improvement (int, optional): The number of iterations to allow without improvement before stopping. Default is 20.\n",
    "        update_if_no_improvement (int, optional): The number of iterations to allow without improvement before updating the tabu list. Default is 20.\n",
    "        max_candidates (int, optional): The maximum number of candidate solutions to generate per iteration. Default is 100.\n",
    "        debug (bool, optional): If True, enables debug logging. Default is False.\n",
    "        logger (callable, optional): A logging function. If None, prints to the console. Default is None.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, words: list, calculate_perplexity, tabu_tenure=5,\n",
    "                 max_iter=300, stop_on_non_improvement=80, max_candidates=100,\n",
    "                 debug=False, logger = None):\n",
    "        '''\n",
    "        Initializes the TabuSearch instance.\n",
    "\n",
    "        Parameters:\n",
    "            words (list[str]): The sequence of words to be optimized.\n",
    "            calculate_perplexity (callable): A method to calculate the perplexity of a sentence.\n",
    "            tabu_tenure (int, optional): The size of the tabu list. Default is 5.\n",
    "            max_iter (int, optional): The maximum number of iterations. Default is 100.\n",
    "            stop_on_non_improvement (int, optional): The number of iterations to allow without improvement. Default is 20.\n",
    "            max_candidates (int, optional): The maximum number of candidate solutions per iteration. Default is 100.\n",
    "            debug (bool, optional): If True, enables debug logging. Default is False.\n",
    "            logger (callable, optional): A logging function. Default is None.\n",
    "        '''\n",
    "        self.words = words\n",
    "        self.n = len(self.words)\n",
    "        self.tabu_tenure = tabu_tenure\n",
    "        self.stop_on_non_improvement = stop_on_non_improvement\n",
    "        self.max_iter = max_iter\n",
    "        self.itr = None\n",
    "        self.calculate_perplexity = calculate_perplexity\n",
    "        self.tabu_list = deque(maxlen=self.tabu_tenure)\n",
    "        self.max_candidates = max_candidates\n",
    "        self.debug = debug\n",
    "        self.logger = logger\n",
    "        self.condidate_level = 2\n",
    "        \n",
    "        self.log(f\"TABU(tabu_tenure={self.tabu_tenure}, stop_on_non_improvement={self.stop_on_non_improvement}, max_iter={self.max_iter})\")\n",
    "\n",
    "    def log(self, msg):\n",
    "        if self.debug:\n",
    "            itr = '' if self.itr == None else f'{self.itr}/{self.max_iter}'\n",
    "            if self.logger is None:\n",
    "                print(f\"[TABU SEARCH] [{itr}] {msg}\")\n",
    "            else:\n",
    "                self.logger(f\"[TABU SEARCH] [{itr}] {msg}\")\n",
    "    \n",
    "    def is_move_used(self, move: Tuple[int, int]):\n",
    "        '''\n",
    "        Checks if a move is present in the tabu list.\n",
    "\n",
    "        Parameters:\n",
    "            move (Tuple[int, int]): The move to check.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the move is in the tabu list, False otherwise.\n",
    "        '''\n",
    "        return move in self.tabu_list\n",
    "    \n",
    "    def to_tabulist(self, move):\n",
    "        '''\n",
    "        Adds a move to the tabu list.\n",
    "\n",
    "        Parameters:\n",
    "            move (Tuple[int, int]): The move to add to the tabu list.\n",
    "        '''\n",
    "        self.tabu_list.append(tuple(move))\n",
    "\n",
    "    def get_candidates(self, solution: np.ndarray) -> List[Tuple[list, Tuple[int, ...]]]:\n",
    "        '''\n",
    "        Generates candidate solutions by randomly swapping k elements.\n",
    "\n",
    "        Parameters:\n",
    "            solution (np.ndarray): The current solution as an array of indices.\n",
    "            k (int): The number of elements to swap. Default is 2.\n",
    "\n",
    "        Returns:\n",
    "            list[Tuple[list, Tuple[int, ...]]]: A list of candidate solutions and their corresponding moves.\n",
    "        '''\n",
    "        candidates = []\n",
    "\n",
    "        while len(candidates) < self.max_candidates:\n",
    "            # Randomly select k distinct indices\n",
    "            indices = random.sample(range(len(solution)), self.condidate_level)\n",
    "            # Create a candidate by swapping the elements at the selected indices\n",
    "            candidate = solution.copy()\n",
    "            # Perform a cyclic swap: candidate[indices[0]] -> candidate[indices[1]] -> ... -> candidate[indices[-1]] -> candidate[indices[0]]\n",
    "            temp = candidate[indices[0]]\n",
    "            for i in range(self.condidate_level - 1):\n",
    "                candidate[indices[i]] = candidate[indices[i + 1]]\n",
    "            candidate[indices[-1]] = temp\n",
    "            # Add the candidate and the move (indices) to the list\n",
    "            candidates.append((candidate, tuple(indices)))\n",
    "        \n",
    "        return candidates\n",
    "\n",
    "    def optimize(self, current_sentence: np.ndarray) -> Tuple[np.ndarray, float]:\n",
    "        '''\n",
    "        Executes the Tabu Search algorithm.\n",
    "\n",
    "        Parameters:\n",
    "            current_sentence (np.ndarray): The initial solution as an array of indices.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[np.ndarray, float]: The best solution and its corresponding perplexity.\n",
    "        '''\n",
    "        self.log(f\"[START] {current_sentence}\")\n",
    "        \n",
    "        # Calculate the perplexity of the initial solution\n",
    "        current_perplexity = self.calculate_perplexity(current_sentence)\n",
    "        \n",
    "        # Initialize the best solution and its perplexity\n",
    "        best_sentence = current_sentence\n",
    "        best_perplexity = current_perplexity\n",
    "        \n",
    "        # Track the number of iterations without improvement and the total iterations\n",
    "        no_improvement = 0\n",
    "        self.itr = 1\n",
    "        \n",
    "        # Iterate until the stopping condition is met\n",
    "        while self.itr <= self.max_iter:\n",
    "            self.log(f\"{best_sentence} | {best_perplexity}\")\n",
    "            \n",
    "            # Generate candidate solutions\n",
    "            candidates = self.get_candidates(current_sentence)\n",
    "            \n",
    "            # Initialize variables for the best candidate solution\n",
    "            best_candidate = None\n",
    "            best_candidate_perplexity = None\n",
    "            used_move = None\n",
    "            \n",
    "            # Evaluate each candidate solution\n",
    "            for candidate, swap in candidates:\n",
    "                candidate_perplexity = self.calculate_perplexity(candidate)\n",
    "                \n",
    "                # Check if the candidate is better than the current best candidate\n",
    "                is_better_candidate = (\n",
    "                    (best_candidate_perplexity is None) or\n",
    "                    (candidate_perplexity < best_candidate_perplexity)\n",
    "                )\n",
    "                \n",
    "                # Check if the move is allowed (not in tabu list or better than the best solution)\n",
    "                is_move_allowed = (\n",
    "                    (not self.is_move_used(swap)) or\n",
    "                    (candidate_perplexity < best_perplexity)\n",
    "                )\n",
    "                \n",
    "                # Update the best candidate if it is better and the move is allowed\n",
    "                if is_better_candidate and is_move_allowed:\n",
    "                    best_candidate = candidate\n",
    "                    best_candidate_perplexity = candidate_perplexity\n",
    "                    used_move = swap\n",
    "\n",
    "            # self.log(f\"[COND] {best_candidate} | [{best_candidate_perplexity}]\")\n",
    "            \n",
    "            # Check if the best candidate is an improvement\n",
    "            if best_candidate_perplexity < best_perplexity:\n",
    "                # Update the best solution and reset the no-improvement counter\n",
    "                best_sentence = best_candidate\n",
    "                best_perplexity = best_candidate_perplexity\n",
    "                no_improvement = 0\n",
    "                # self.log(f\"[IMPR] [{itr}/{self.max_iter}] [{best_perplexity}]\")\n",
    "            else:\n",
    "                # Increment the no-improvement counter\n",
    "                no_improvement += 1\n",
    "                self.log(f\"[!IMPR] __{no_improvement}__\")\n",
    "            \n",
    "            # consider the best candidate as the current solution\n",
    "            current_sentence = best_candidate\n",
    "            current_perplexity = best_candidate_perplexity\n",
    "            \n",
    "            # add the move to the tabu list\n",
    "            self.to_tabulist(used_move)\n",
    "            self.itr+=1\n",
    "\n",
    "            # Stop if the no-improvement limit is reached\n",
    "            if(no_improvement == self.stop_on_non_improvement):\n",
    "                self.log(f\"[BREAK] {best_sentence} | {best_perplexity}\")\n",
    "                break\n",
    "        \n",
    "        # Return the best solution and its perplexity\n",
    "        self.log(f\"[END] {best_sentence} | {best_perplexity}\")\n",
    "        return best_sentence, best_perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15651c7f",
   "metadata": {
    "papermill": {
     "duration": 0.002429,
     "end_time": "2024-12-29T17:56:05.390506",
     "exception": false,
     "start_time": "2024-12-29T17:56:05.388077",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "# WAHLE OPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d437fd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T12:04:36.487886Z",
     "iopub.status.busy": "2025-01-01T12:04:36.487630Z",
     "iopub.status.idle": "2025-01-01T12:04:36.515184Z",
     "shell.execute_reply": "2025-01-01T12:04:36.514519Z",
     "shell.execute_reply.started": "2025-01-01T12:04:36.487867Z"
    },
    "papermill": {
     "duration": 0.022337,
     "end_time": "2024-12-29T17:56:05.415462",
     "exception": false,
     "start_time": "2024-12-29T17:56:05.393125",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import permutations\n",
    "\n",
    "class WhaleOptimization:\n",
    "    \"\"\"\n",
    "    Implementation of the Whale Optimization Algorithm (WOA) for optimizing word order \n",
    "    in a sentence based on perplexity scoring.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sentence: str, scorer,\n",
    "                 \n",
    "                 n_whales: int = 20, max_iter: int = 80,\n",
    "                 \n",
    "                 tabu_tenure=10, tabu_no_imp=10, tabu_max_iter=100, tabu_max_candidates=100,\n",
    "                 \n",
    "                 cache: Caching = Caching(), logger=None, debug=False):\n",
    "        \"\"\"\n",
    "        Initialize the Whale Optimization Algorithm.\n",
    "\n",
    "        Args:\n",
    "            sentence (str): Input sentence to optimize.\n",
    "            scorer (object): Scorer with a `get_perplexity` method to evaluate sentence quality.\n",
    "            n_whales (int, optional): Number of whales in the population. Defaults to 20.\n",
    "            max_iter (int, optional): Maximum iterations for WOA. Defaults to 80.\n",
    "            tabu_tenure (int, optional): Size of tabu list in Tabu Search. Defaults to 10.\n",
    "            tabu_no_imp (int, optional): No-improvement iterations to stop Tabu Search. Defaults to 10.\n",
    "            tabu_max_iter (int, optional): Maximum iterations in Tabu Search. Defaults to 50.\n",
    "            logger (callable, optional): Logging function. Defaults to None.\n",
    "            debug (bool, optional): Enable debug messages. Defaults to False.\n",
    "        \"\"\"\n",
    "        self.words = sentence.split(' ')\n",
    "        self.n = len(self.words)\n",
    "        self.scorer = scorer\n",
    "        self.n_whales = n_whales\n",
    "        self.max_iter = max_iter\n",
    "        self.logger = logger\n",
    "        self.debug = debug\n",
    "        \n",
    "        self.cwi = None  # Index of the current whale\n",
    "        self.itr = 0  # Current iteration\n",
    "        \n",
    "        # Initialize Tabu Search for local improvements\n",
    "        self.TabuSearch = TabuSearch(\n",
    "            self.words, \n",
    "            self.__calculate_perplexity,\n",
    "            tabu_tenure=tabu_tenure,\n",
    "            stop_on_non_improvement=tabu_no_imp,\n",
    "            max_iter=tabu_max_iter,\n",
    "            max_candidates=tabu_max_candidates,\n",
    "            debug=debug, logger=self.log\n",
    "        )\n",
    "        \n",
    "        # Cache for perplexity calculations\n",
    "        self.cache = cache\n",
    "        \n",
    "        self.log(f\"WOA(n_whales={n_whales}, max_iter={max_iter}, n={self.n})\")\n",
    "    \n",
    "    def __create_initial_sols(self) -> np.ndarray:\n",
    "        \"\"\"Generate a random initial solution (word permutation).\"\"\"\n",
    "        return np.random.permutation(self.n)\n",
    "    \n",
    "    def log(self, msg):\n",
    "        \"\"\"\n",
    "        Log messages for debugging and tracing execution.\n",
    "\n",
    "        Args:\n",
    "            msg (str): The message to log.\n",
    "        \"\"\"\n",
    "        if self.debug:\n",
    "            itr = f\"{self.itr}/{self.max_iter}\"\n",
    "            if self.logger is None:\n",
    "                if self.cwi is None:\n",
    "                    print(f\"[WOA] [{itr}] {msg}\")\n",
    "                else:\n",
    "                    print(f\"[WOA] [{itr}] [WHALE {self.cwi + 1}] {msg}\")\n",
    "            else:\n",
    "                if self.cwi is None:\n",
    "                    self.logger(f\"[{itr}] {msg}\")\n",
    "                else:\n",
    "                    self.logger(f\"[{itr}] [WHALE {self.cwi + 1}] {msg}\")\n",
    "    \n",
    "    def _compute_A(self, a: float):\n",
    "        \"\"\"\n",
    "        Compute the coefficient A for WOA movement equations.\n",
    "\n",
    "        Args:\n",
    "            a (float): Shrinking parameter.\n",
    "\n",
    "        Returns:\n",
    "            float: Coefficient A.\n",
    "        \"\"\"\n",
    "        r = np.random.uniform(0.0, 1.0, size=1)\n",
    "        return (2.0*np.multiply(a, r)) - a\n",
    "\n",
    "    def _compute_C(self):\n",
    "        \"\"\"\n",
    "        Compute the coefficient C for WOA movement equations.\n",
    "\n",
    "        Returns:\n",
    "            float: Coefficient C.\n",
    "        \"\"\"\n",
    "        r = np.random.uniform(0.0, 1.0, size=1)\n",
    "        return 2.0 * r\n",
    "    \n",
    "    def __to_sentence_text(self, sol):\n",
    "        \"\"\"\n",
    "        Convert a solution (word order) into a readable sentence.\n",
    "\n",
    "        Args:\n",
    "            sol (np.ndarray): Solution as indices.\n",
    "\n",
    "        Returns:\n",
    "            str: Reconstructed sentence.\n",
    "        \"\"\"\n",
    "        return \" \".join([self.words[i] for i in sol])\n",
    "    \n",
    "    def __get_from_cache(self, solution: tuple) -> float:\n",
    "        return self.cache.get(solution)\n",
    "    \n",
    "    def __calculate_perplexity(self, solution: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Compute the perplexity of a given solution, with caching.\n",
    "\n",
    "        Args:\n",
    "            solution (np.ndarray): Solution as indices.\n",
    "\n",
    "        Returns:\n",
    "            float: Perplexity score of the solution.\n",
    "        \"\"\"\n",
    "        solt = tuple(solution)\n",
    "        cached = self.__get_from_cache(solt)\n",
    "        if cached is not None:\n",
    "            return cached\n",
    "        \n",
    "        # Convert solution to sentence and calculate perplexity\n",
    "        sentence = self.__to_sentence_text(solution)\n",
    "        submission = pd.DataFrame({'id': [0], 'text': [sentence] })\n",
    "        perplexities = scorer.get_perplexity(submission[\"text\"].tolist()) # TODO use self to access the scorer\n",
    "        self.cache.add(solt, perplexities[0])\n",
    "        return perplexities[0]\n",
    "\n",
    "    def __caching_state(self):\n",
    "        self.log(self.cache.state())\n",
    "    \n",
    "    def __search_for_prey(self, current_pos: np.ndarray, random_pos: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Simulates the \"searching for prey\" behavior using Word-Based Crossover (WBX).\n",
    "\n",
    "        This method updates the current solution by combining it with a randomly selected solution\n",
    "        from the population. The Word-Based Crossover (WBX) operator is used to generate two offspring\n",
    "        solutions, and the better one is selected based on the perplexity score.\n",
    "\n",
    "        Args:\n",
    "            current_pos (np.ndarray): The current solution (e.g., a permutation of indices).\n",
    "            random_pos (np.ndarray): A randomly selected solution from the population.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The better offspring solution generated using Word-Based Crossover.\n",
    "        \"\"\"\n",
    "        n = len(current_pos)\n",
    "        \n",
    "        # Step 1: Randomly generate a word permutation S, then randomly choose several words from S\n",
    "        # and put into set S1, the others put into set S2.\n",
    "        S = np.random.permutation(n)  # Random word permutation\n",
    "        split_point = random.randint(1, n - 1)  # Randomly choose a split point\n",
    "        S1 = set(S[:split_point])  # First set of words\n",
    "        S2 = set(S[split_point:])  # Second set of words\n",
    "        \n",
    "        # Step 2: Generate offspring C1\n",
    "        C1 = np.full(n, -1, dtype=int)  # Initialize C1\n",
    "        idx = 0\n",
    "        for i in range(n):\n",
    "            if current_pos[i] in S1:\n",
    "                C1[idx] = current_pos[i]\n",
    "                idx += 1\n",
    "        for i in range(n):\n",
    "            if random_pos[i] in S2:\n",
    "                C1[idx] = random_pos[i]\n",
    "                idx += 1\n",
    "        \n",
    "        # Step 3: Generate offspring C2\n",
    "        C2 = np.full(n, -1, dtype=int)  # Initialize C2\n",
    "        idx = 0\n",
    "        for i in range(n):\n",
    "            if random_pos[i] in S1:\n",
    "                C2[idx] = random_pos[i]\n",
    "                idx += 1\n",
    "        for i in range(n):\n",
    "            if current_pos[i] in S2:\n",
    "                C2[idx] = current_pos[i]\n",
    "                idx += 1\n",
    "        \n",
    "        # Step 4: Choose the better between C1 and C2 according to the perplexity criterion\n",
    "        perplexity_C1 = self.__calculate_perplexity(C1)\n",
    "        perplexity_C2 = self.__calculate_perplexity(C2)\n",
    "        \n",
    "        if perplexity_C1 < perplexity_C2:\n",
    "            return C1\n",
    "        else:\n",
    "            return C2\n",
    "    \n",
    "    def __encircling_prey(self, current_pos: np.ndarray, best_pos: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Simulates the \"encircling prey\" behavior using Two-Points Crossover (TPX).\n",
    "\n",
    "        This method combines the current solution and the best solution by selecting a segment\n",
    "        from the best solution and inserting it into the current solution. The rest of the elements\n",
    "        are filled while preserving the permutation property.\n",
    "\n",
    "        Args:\n",
    "            current_pos (np.ndarray): The current solution (e.g., a permutation of indices).\n",
    "            best_pos (np.ndarray): The best solution found so far.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: A new solution generated using Two-Points Crossover.\n",
    "        \"\"\"\n",
    "        n = len(current_pos)\n",
    "        # Select two random crossover points\n",
    "        r1, r2 = sorted(random.sample(range(1, n - 1), k=2))\n",
    "        \n",
    "        # Initialize the new solution\n",
    "        new_pos = np.full(n, -1, dtype=int)\n",
    "        \n",
    "        # Copy the segment between r1 and r2 from the best solution\n",
    "        new_pos[r1:r2] = best_pos[r1:r2]\n",
    "        \n",
    "        # Fill the remaining positions from the current solution, preserving the permutation\n",
    "        current_idx = 0\n",
    "        for i in range(n):\n",
    "            if new_pos[i] == -1:\n",
    "                while current_pos[current_idx] in new_pos:\n",
    "                    current_idx += 1\n",
    "                new_pos[i] = current_pos[current_idx]\n",
    "        \n",
    "        return new_pos\n",
    "    \n",
    "    def __bubble_net_attack(self, current_pos: np.ndarray, best_pos: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Simulates the \"bubble-net attacking\" behavior using Multi-Points Crossover (MPX).\n",
    "\n",
    "        This method combines the current solution and the best solution by selecting multiple\n",
    "        segments from the best solution and inserting them into the current solution. The rest\n",
    "        of the elements are filled while preserving the permutation property.\n",
    "\n",
    "        Args:\n",
    "            current_pos (np.ndarray): The current solution (e.g., a permutation of indices).\n",
    "            best_pos (np.ndarray): The best solution found so far.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: A new solution generated using Multi-Points Crossover.\n",
    "        \"\"\"\n",
    "        n = len(current_pos)\n",
    "        # Select multiple random crossover points\n",
    "        crossover_points = sorted(random.sample(range(1, n), k=random.randint(2, n // 2)))\n",
    "        \n",
    "        # Initialize the new solution\n",
    "        new_pos = np.full(n, -1, dtype=int)\n",
    "        \n",
    "        # Copy segments from the best solution based on crossover points\n",
    "        for i in range(0, len(crossover_points), 2):\n",
    "            if i + 1 < len(crossover_points):\n",
    "                start, end = crossover_points[i], crossover_points[i + 1]\n",
    "                new_pos[start:end] = best_pos[start:end]\n",
    "        \n",
    "        # Fill the remaining positions from the current solution, preserving the permutation\n",
    "        current_idx = 0\n",
    "        for i in range(n):\n",
    "            if new_pos[i] == -1:\n",
    "                while current_pos[current_idx] in new_pos:\n",
    "                    current_idx += 1\n",
    "                new_pos[i] = current_pos[current_idx]\n",
    "        \n",
    "        return new_pos\n",
    "    \n",
    "    def __local_search(self, solution: np.ndarray) -> Tuple[np.ndarray, float]:\n",
    "        return self.TabuSearch.optimize(solution)\n",
    "    \n",
    "    def optimize(self) -> Tuple[np.ndarray, float, List[float], List[list]]:\n",
    "        \"\"\"\n",
    "        Execute the Whale Optimization Algorithm to find the best word order.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[np.ndarray, float]: The best solution and its perplexity score.\n",
    "        \"\"\"\n",
    "        # Initialize population of whales with random solutions\n",
    "        population = []\n",
    "        perplexities_values = []\n",
    "        for w in range(self.n_whales):\n",
    "            self.cwi = w\n",
    "            solution = self.__create_initial_sols()\n",
    "            self.log(f\"sol {solution} --> to improve\")\n",
    "            # improved_solution, improved_perplexity = self.__local_search(solution)\n",
    "            # self.log(f\"{improved_solution} | {improved_perplexity}\")\n",
    "            # population.append(improved_solution)\n",
    "            population.append(solution)\n",
    "            perplexities_values.append(improved_perplexity)\n",
    "        self.cwi=None\n",
    "        # population = [np.array([8, 6, 9, 1, 2, 5, 3, 0, 4, 7]), np.array([8, 6, 2, 9, 5, 3, 7, 0, 1, 4]), np.array([8, 6, 2, 9, 1, 4, 5, 7, 3, 0])]\n",
    "        # perplexities_values = [531.0074131628102, 530.3983005966822, 561.4057235990515]\n",
    "\n",
    "        self.log(\"------------------------------\")\n",
    "        self.log(f\"[WHALES]\")\n",
    "        self.log(population)\n",
    "        self.log(perplexities_values)\n",
    "        self.log(\"------------------------------\")\n",
    "        \n",
    "        # Identify the initial best whale\n",
    "        best_idx = np.argmin(perplexities_values)\n",
    "        prey = population[best_idx].copy()\n",
    "        prey_perplexity = perplexities_values[best_idx]\n",
    "        \n",
    "        self.log(f\"[PREY] : {prey} | {prey_perplexity}\")\n",
    "        \n",
    "        # Start main WOA loop\n",
    "        self.itr = 0\n",
    "        while self.itr <= self.max_iter:\n",
    "            self.itr += 1\n",
    "            self.log(f\"[START ITER] {prey} | {prey_perplexity}\")\n",
    "            \n",
    "            # Shrinking encircling mechanism parameter\n",
    "            a = 2 - self.itr / self.max_iter * 2\n",
    "            \n",
    "            # Update each whale in the population\n",
    "            for self.cwi in range(self.n_whales):\n",
    "                cwhale = population[self.cwi]\n",
    "                self.log(f\"{cwhale} | {perplexities_values[self.cwi]}\")\n",
    "                A = self._compute_A(a)\n",
    "                C = self._compute_C()\n",
    "                p = random.random()\n",
    "                self.log(f\"a={a}, A={A}, C={C}, p={p}\")\n",
    "\n",
    "                if p < 0.5: # TODO: check if this is important for the problem, or it can be removed\n",
    "                    if abs(A) < 1:\n",
    "                        # Encircling prey\n",
    "                        new_pos = self.__encircling_prey(cwhale, prey)\n",
    "                        self.log(f\"[MOVE] encircling prey --> {new_pos}\")\n",
    "                    else:\n",
    "                        # Searching for prey\n",
    "                        rand_idx = random.randint(0, self.n_whales-1)\n",
    "                        random_pos = population[rand_idx]\n",
    "                        new_pos = self.__search_for_prey(cwhale, random_pos)\n",
    "                        self.log(f\"[MOVE] search for prey --> {new_pos}\")\n",
    "                else:\n",
    "                    new_pos = self.__bubble_net_attack(cwhale, prey)\n",
    "                    self.log(f\"[MOVE] bubble net attack --> {new_pos}\")\n",
    "                \n",
    "                # Apply local search to improve the new position\n",
    "                improved_pos, improved_perplexity = self.__local_search(new_pos)\n",
    "                self.log(f\"[IMPROVE POSITION] {new_pos}, [{improved_perplexity}]\")\n",
    "                \n",
    "                # Evaluate the new solution\n",
    "                # if improved_perplexity < perplexities_values[self.cwi]: # TODO: accept only the improving moves\n",
    "                population[self.cwi] = improved_pos\n",
    "                perplexities_values[self.cwi] = improved_perplexity\n",
    "            \n",
    "            # Update the prey if a better solution is found\n",
    "            best_idx = np.argmin(perplexities_values)\n",
    "            if perplexities_values[best_idx] < prey_perplexity:\n",
    "                prey = population[best_idx]\n",
    "                prey_perplexity = perplexities_values[best_idx]\n",
    "                self.log(f\"[IMPR] {prey} | {prey_perplexity}\")\n",
    "            else:\n",
    "                self.log(f\"[!IMPR] {prey} | {prey_perplexity}\")\n",
    "            \n",
    "            self.cwi=None\n",
    "        \n",
    "        self.__caching_state()\n",
    "        self.log(f\"[END] [PREY] {prey} | {prey_perplexity}\")\n",
    "        return prey, prey_perplexity, self.__to_sentence_text(prey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a95137-50d9-4c85-85ea-4019ee66934a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T12:04:36.516474Z",
     "iopub.status.busy": "2025-01-01T12:04:36.516229Z",
     "iopub.status.idle": "2025-01-01T12:07:14.266562Z",
     "shell.execute_reply": "2025-01-01T12:07:14.265634Z",
     "shell.execute_reply.started": "2025-01-01T12:04:36.516454Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n",
    "scorer = PerplexityCalculator(model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbcf993-4ffa-43d6-96ce-af367778cec8",
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-01T12:12:20.403Z",
     "iopub.execute_input": "2025-01-01T12:07:14.267980Z",
     "iopub.status.busy": "2025-01-01T12:07:14.267530Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# text = 'advent chimney elf family fireplace gingerbread mistletoe ornament reindeer scrooge'\n",
    "\n",
    "# woa = WhaleOptimization(\n",
    "#     text, score,\n",
    "    \n",
    "#     n_whales=3, max_iter=5,\n",
    "    \n",
    "#     tabu_tenure=10, tabu_no_imp=15, tabu_max_iter=20, tabu_max_candidates=100,\n",
    "    \n",
    "#     debug=True,\n",
    "# )\n",
    "# opt = woa.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29349879",
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-01T12:12:20.404Z"
    },
    "papermill": {
     "duration": 34077.046284,
     "end_time": "2024-12-30T03:24:02.474536",
     "exception": false,
     "start_time": "2024-12-29T17:56:05.428252",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "problem = pd.read_csv(\"/kaggle/input/santa-2024/sample_submission.csv\")\n",
    "\n",
    "res = []\n",
    "\n",
    "for idx, row in problem.iterrows():\n",
    "    print(\"----------------------------------------------\")\n",
    "    print(\"----------------------------------------------\")\n",
    "    print(\"----------------------------------------------\")\n",
    "    def logger(msg):\n",
    "        print(f\"[WOA {idx}] {msg}\")\n",
    "    woa = WhaleOptimization(\n",
    "        row.text, score,\n",
    "        n_whales=5, max_iter=20, debug=True, \n",
    "        tabu_tenure=10, tabu_no_imp=50, tabu_max_iter=100,\n",
    "        logger = logger\n",
    "    )\n",
    "    _s = time.time()\n",
    "    sol, p, sen = woa.optimize()\n",
    "    _e = time.time()\n",
    "    res.append({\n",
    "        \"id\": len(res),\n",
    "        \"or\": row.text,\n",
    "        \"sol\": sol,\n",
    "        \"text\": sen,\n",
    "        \"p\": p,\n",
    "        \"t\": _e - _s\n",
    "    })\n",
    "\n",
    "submission_ = pd.DataFrame(res)\n",
    "submission_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e181bbb5-168d-4d76-9677-9424068de362",
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-01T12:12:20.405Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# submission = submission_[['id', 'text']]\n",
    "# submission.to_csv(\"submission.csv\", index=False)\n",
    "# submission"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 10229277,
     "sourceId": 88046,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 76277,
     "modelInstanceId": 72255,
     "sourceId": 104492,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 34250.814184,
   "end_time": "2024-12-30T03:24:06.176525",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-29T17:53:15.362341",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "059039d466be481ca0e6c1ebf32a7bf1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2760a09b7dcc427f9ae13881a5f248b6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "6c28f617367c48099870d3a8cba07425": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a239bc75c3754dcbb849992358aa7e2d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a66db6c6e6284dbba7e9c9e4a35e970f",
       "placeholder": "​",
       "style": "IPY_MODEL_e7f03ab786014ebebe4f44b0164ae4d2",
       "tabbable": null,
       "tooltip": null,
       "value": " 8/8 [02:40&lt;00:00, 18.13s/it]"
      }
     },
     "a66db6c6e6284dbba7e9c9e4a35e970f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ac6d51f04382460db0fda8ef99a6e87d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_059039d466be481ca0e6c1ebf32a7bf1",
       "placeholder": "​",
       "style": "IPY_MODEL_6c28f617367c48099870d3a8cba07425",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "b91eaa8982ae4bf7807d0d8a0157f1e6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_faafe3ae5e75441abf38c1e58a7014a9",
       "max": 8,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2760a09b7dcc427f9ae13881a5f248b6",
       "tabbable": null,
       "tooltip": null,
       "value": 8
      }
     },
     "c8b74db2c20749bca286d3e4b372658d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ac6d51f04382460db0fda8ef99a6e87d",
        "IPY_MODEL_b91eaa8982ae4bf7807d0d8a0157f1e6",
        "IPY_MODEL_a239bc75c3754dcbb849992358aa7e2d"
       ],
       "layout": "IPY_MODEL_e76eac02051044f585864916c471ff5c",
       "tabbable": null,
       "tooltip": null
      }
     },
     "e76eac02051044f585864916c471ff5c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e7f03ab786014ebebe4f44b0164ae4d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "faafe3ae5e75441abf38c1e58a7014a9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
